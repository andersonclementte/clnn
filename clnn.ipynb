{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93226949",
   "metadata": {},
   "source": [
    "Imports Completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9c05c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configura√ß√£o inicial\n",
      "Device: cuda\n",
      "Arquivo: humob_all_cities_dpsk.parquet\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seus m√≥dulos locais\n",
    "from external_information import ExternalInformationFusionDTPC, ExternalInformationDense\n",
    "from partial_information import CoordLSTM\n",
    "\n",
    "# Configura√ß√µes\n",
    "parquet_file = \"humob_all_cities_dpsk.parquet\"\n",
    "n_users_by_city = {\"A\": 100_000, \"B\": 25_000, \"C\": 20_000, \"D\": 6_000}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"üöÄ Configura√ß√£o inicial\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Arquivo: {parquet_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de00896",
   "metadata": {},
   "source": [
    " C√©lula 2: Centros Est√°veis (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81924d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stable_centers(\n",
    "    parquet_path: str,\n",
    "    cities: list[str] = [\"A\"],\n",
    "    day_threshold: int = 60,\n",
    "    n_clusters: int = 1024,\n",
    "    sample_size: int = 200_000,\n",
    "    save_path: str = \"centers_stable.npy\",\n",
    "    chunk_size: int = 50_000\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calcula centros est√°veis usando K-Means.\n",
    "    Mais robusto que HDBSCAN: sempre produz exatamente n_clusters centros.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se j√° existe, carrega\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"üìÇ Carregando centros existentes: {save_path}\")\n",
    "        centers = np.load(save_path)\n",
    "        return torch.from_numpy(centers.astype(np.float32))\n",
    "    \n",
    "    print(f\"üîÑ Calculando {n_clusters} centros para cidades {cities}...\")\n",
    "    \n",
    "    # 1) Coleta coordenadas\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    coords_list = []\n",
    "    \n",
    "    for batch in pf.iter_batches(batch_size=chunk_size):\n",
    "        tbl = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "        \n",
    "        # Filtra cidades e dias\n",
    "        city_mask = pc.is_in(tbl.column(\"city\"), pa.array(cities))\n",
    "        day_mask = pc.less(tbl.column(\"d\"), day_threshold)\n",
    "        mask = pc.and_(city_mask, day_mask)\n",
    "        \n",
    "        tbl = tbl.filter(mask)\n",
    "        if tbl.num_rows == 0:\n",
    "            continue\n",
    "            \n",
    "        xs = tbl.column(\"x\").to_numpy()\n",
    "        ys = tbl.column(\"y\").to_numpy()\n",
    "        coords_list.append(np.stack([xs, ys], axis=1))\n",
    "    \n",
    "    coords = np.vstack(coords_list)\n",
    "    print(f\"üìä Coletadas {len(coords):,} coordenadas\")\n",
    "    \n",
    "    # 2) Amostra se muito grande\n",
    "    if len(coords) > sample_size:\n",
    "        idx = np.random.choice(len(coords), sample_size, replace=False)\n",
    "        coords = coords[idx]\n",
    "        print(f\"üé≤ Amostradas {sample_size:,} coordenadas\")\n",
    "    \n",
    "    # 3) K-Means\n",
    "    print(\"‚öôÔ∏è Executando K-Means...\")\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters, \n",
    "        n_init='auto', \n",
    "        random_state=42,\n",
    "        max_iter=300\n",
    "    ).fit(coords)\n",
    "    \n",
    "    centers = kmeans.cluster_centers_.astype(np.float32)\n",
    "    \n",
    "    # 4) Salva para reutilizar\n",
    "    np.save(save_path, centers)\n",
    "    print(f\"üíæ Centros salvos em: {save_path}\")\n",
    "    print(f\"üìè Shape dos centros: {centers.shape}\")\n",
    "    \n",
    "    return torch.from_numpy(centers)\n",
    "\n",
    "\n",
    "def load_or_compute_centers(\n",
    "    parquet_path: str,\n",
    "    cities: list[str] = [\"A\"],\n",
    "    n_clusters: int = 1024,\n",
    "    save_path: str = \"centers_stable.npy\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Fun√ß√£o conveniente que carrega se existe, sen√£o calcula.\"\"\"\n",
    "    return compute_stable_centers(\n",
    "        parquet_path=parquet_path,\n",
    "        cities=cities,\n",
    "        n_clusters=n_clusters,\n",
    "        save_path=save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a231eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityAPretrainDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Dataset otimizado para pr√©-treino em cidade A.\n",
    "    Faz amostragem estratificada por usu√°rio e divide train/val por dias.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        parquet_path: str,\n",
    "        mode: str = \"train\",  # \"train\" ou \"val\"\n",
    "        sequence_length: int = 48,\n",
    "        prediction_steps: int = 1,\n",
    "        chunk_size: int = 10_000,\n",
    "        max_sequences_per_user: int = 50,\n",
    "        train_days: tuple = (1, 55),\n",
    "        val_days: tuple = (56, 60)\n",
    "    ):\n",
    "        self.parquet_path = parquet_path\n",
    "        self.mode = mode\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_sequences_per_user = max_sequences_per_user\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.day_range = train_days\n",
    "        else:\n",
    "            self.day_range = val_days\n",
    "    \n",
    "    def _sample_user_sequences(self, user_group, max_seqs: int):\n",
    "        \"\"\"Amostra sequ√™ncias de um usu√°rio de forma estratificada.\"\"\"\n",
    "        sequences = self._build_sequences_for_user(user_group)\n",
    "        \n",
    "        if len(sequences) <= max_seqs:\n",
    "            return sequences\n",
    "        \n",
    "        # Amostragem estratificada por hora do dia\n",
    "        sequences_by_hour = {}\n",
    "        for seq in sequences:\n",
    "            hour = seq['t'] // 2  # agrupa slots em horas (48 slots / 24h)\n",
    "            if hour not in sequences_by_hour:\n",
    "                sequences_by_hour[hour] = []\n",
    "            sequences_by_hour[hour].append(seq)\n",
    "        \n",
    "        # Amostra proporcionalmente de cada hora\n",
    "        sampled = []\n",
    "        for hour_seqs in sequences_by_hour.values():\n",
    "            n_sample = max(1, min(len(hour_seqs), max_seqs // len(sequences_by_hour)))\n",
    "            sampled.extend(np.random.choice(hour_seqs, n_sample, replace=False))\n",
    "        \n",
    "        return sampled[:max_seqs]\n",
    "    \n",
    "    def _build_sequences_for_user(self, user_data):\n",
    "        \"\"\"Constr√≥i sequ√™ncias temporais para um usu√°rio espec√≠fico.\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(len(user_data) - self.sequence_length - self.prediction_steps + 1):\n",
    "            seq_start = i\n",
    "            seq_end = i + self.sequence_length\n",
    "            target_start = seq_end\n",
    "            target_end = target_start + self.prediction_steps\n",
    "            \n",
    "            coords_seq = user_data.iloc[seq_start:seq_end][['x', 'y']].values\n",
    "            current_info = user_data.iloc[seq_end - 1]\n",
    "            target_coords = user_data.iloc[target_start:target_end][['x', 'y']].values\n",
    "            \n",
    "            sequences.append({\n",
    "                'uid': current_info['uid'],\n",
    "                'd': current_info['d'], \n",
    "                't': current_info['t'],\n",
    "                'city_idx': 0,  # A √© sempre 0\n",
    "                'poi': current_info['POI'],\n",
    "                'coords_seq': coords_seq.astype(np.float32),\n",
    "                'target_coords': target_coords.astype(np.float32)\n",
    "            })\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        pf = pq.ParquetFile(self.parquet_path)\n",
    "        \n",
    "        for batch in pf.iter_batches(batch_size=self.chunk_size):\n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "\n",
    "            # Filtra cidade A e range de dias\n",
    "            city_mask = pc.equal(table.column(\"city\"), \"A\")\n",
    "            day_mask = pc.and_(\n",
    "                pc.greater_equal(table.column(\"d\"), self.day_range[0]),\n",
    "                pc.less_equal(table.column(\"d\"), self.day_range[1])\n",
    "            )\n",
    "            mask = pc.and_(city_mask, day_mask)\n",
    "            \n",
    "            table = table.filter(mask)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            # Converte e normaliza POIs\n",
    "            df = table.to_pandas()\n",
    "\n",
    "            # Normaliza POIs (VERS√ÉO CORRIGIDA)\n",
    "            poi_cols = [col for col in df.columns if 'POI' in col or col == 'POI']\n",
    "            if poi_cols:\n",
    "                for col in poi_cols:\n",
    "                    if col in df.columns:\n",
    "                        # Verifica o tipo da coluna POI\n",
    "                        sample_val = df[col].iloc[0] if len(df) > 0 else None\n",
    "                        \n",
    "                        if sample_val is not None:\n",
    "                            # Se POI √© uma lista/array (vetor 85-D)\n",
    "                            if hasattr(sample_val, '__len__') and not isinstance(sample_val, str):\n",
    "                                # df[col] = df[col].apply(lambda x: np.log1p(np.array(x)) if x is not None else x)\n",
    "                                df[col] = df[col].apply(lambda x: np.log1p(np.array(x, dtype=np.float32)) if x is not None else np.zeros(85, dtype=np.float32))\n",
    "                            # Se POI √© um valor escalar\n",
    "                            else:\n",
    "                                df[col] = np.log1p(df[col].fillna(0))\n",
    "            \n",
    "            df = df.sort_values(['uid', 'd', 't'])\n",
    "            \n",
    "            # Processa cada usu√°rio com amostragem estratificada\n",
    "            for uid, user_group in df.groupby('uid'):\n",
    "                if len(user_group) < self.sequence_length + self.prediction_steps:\n",
    "                    continue\n",
    "                \n",
    "                sequences = self._sample_user_sequences(\n",
    "                    user_group, \n",
    "                    self.max_sequences_per_user\n",
    "                )\n",
    "                \n",
    "                for seq in sequences:\n",
    "                    yield (\n",
    "                        torch.tensor(seq['uid'], dtype=torch.long),\n",
    "                        torch.tensor(seq['d'], dtype=torch.long),\n",
    "                        torch.tensor(seq['t'], dtype=torch.long),\n",
    "                        torch.tensor(seq['city_idx'], dtype=torch.long),\n",
    "                        torch.from_numpy(seq['poi']),\n",
    "                        torch.from_numpy(seq['coords_seq']),\n",
    "                        torch.from_numpy(seq['target_coords'])\n",
    "                    )\n",
    "\n",
    "\n",
    "def create_pretrain_loaders(\n",
    "    parquet_path: str,\n",
    "    batch_size: int = 32,\n",
    "    sequence_length: int = 48,\n",
    "    max_sequences_per_user: int = 50,\n",
    "    num_workers: int = 0\n",
    "):\n",
    "    \"\"\"Cria loaders de treino e valida√ß√£o para cidade A.\"\"\"\n",
    "    \n",
    "    train_ds = CityAPretrainDataset(\n",
    "        parquet_path=parquet_path,\n",
    "        mode=\"train\",\n",
    "        sequence_length=sequence_length,\n",
    "        max_sequences_per_user=max_sequences_per_user\n",
    "    )\n",
    "    \n",
    "    val_ds = CityAPretrainDataset(\n",
    "        parquet_path=parquet_path,\n",
    "        mode=\"val\", \n",
    "        sequence_length=sequence_length,\n",
    "        max_sequences_per_user=max_sequences_per_user // 2\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7358b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetCityDDataset(IterableDataset):\n",
    "    \"\"\"Dataset para avalia√ß√£o em cidade D.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        parquet_path: str,\n",
    "        city_list: list[str],\n",
    "        chunk_size: int = 10_000,\n",
    "        sequence_length: int = 48,\n",
    "        prediction_steps: int = 1\n",
    "    ):\n",
    "        self.parquet_path = parquet_path\n",
    "        self.city_list = city_list\n",
    "        self.city_set = set(city_list)\n",
    "        self.city_to_idx = {c: i for i, c in enumerate(city_list)}\n",
    "        self.chunk_size = chunk_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "\n",
    "    def _build_sequences_for_user(self, user_data):\n",
    "        \"\"\"Constr√≥i sequ√™ncias temporais para um usu√°rio espec√≠fico.\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(len(user_data) - self.sequence_length - self.prediction_steps + 1):\n",
    "            seq_start = i\n",
    "            seq_end = i + self.sequence_length\n",
    "            target_start = seq_end\n",
    "            target_end = target_start + self.prediction_steps\n",
    "            \n",
    "            coords_seq = user_data.iloc[seq_start:seq_end][['x', 'y']].values\n",
    "            current_info = user_data.iloc[seq_end - 1]\n",
    "            target_coords = user_data.iloc[target_start:target_end][['x', 'y']].values\n",
    "            \n",
    "            sequences.append({\n",
    "                'uid': current_info['uid'],\n",
    "                'd': current_info['d'], \n",
    "                't': current_info['t'],\n",
    "                'city_idx': self.city_to_idx[current_info['city']],\n",
    "                'poi': current_info['POI'],\n",
    "                'coords_seq': coords_seq.astype(np.float32),\n",
    "                'target_coords': target_coords.astype(np.float32)\n",
    "            })\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        pf = pq.ParquetFile(self.parquet_path)\n",
    "        \n",
    "        for batch in pf.iter_batches(batch_size=self.chunk_size):\n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "\n",
    "            # Filtra cidades\n",
    "            mask = pc.is_in(table.column(\"city\"), pa.array(list(self.city_set)))\n",
    "            table = table.filter(mask)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            # Converte para pandas\n",
    "            df = table.to_pandas()\n",
    "            \n",
    "\n",
    "            # Normaliza POIs (VERS√ÉO CORRIGIDA)\n",
    "            poi_cols = [col for col in df.columns if 'POI' in col or col == 'POI']\n",
    "            if poi_cols:\n",
    "                for col in poi_cols:\n",
    "                    if col in df.columns:\n",
    "                        # Verifica o tipo da coluna POI\n",
    "                        sample_val = df[col].iloc[0] if len(df) > 0 else None\n",
    "                        \n",
    "                        if sample_val is not None:\n",
    "                            # Se POI √© uma lista/array (vetor 85-D)\n",
    "                            if hasattr(sample_val, '__len__') and not isinstance(sample_val, str):\n",
    "                                # df[col] = df[col].apply(lambda x: np.log1p(np.array(x)).tolist() if x is not None else x)\n",
    "                                df[col] = df[col].apply(lambda x: np.log1p(np.array(x, dtype=np.float32)) if x is not None else np.zeros(85, dtype=np.float32))\n",
    "                            # Se POI √© um valor escalar\n",
    "                            else:\n",
    "                                df[col] = np.log1p(df[col].fillna(0))\n",
    "            \n",
    "            df = df.sort_values(['uid', 'd', 't'])\n",
    "            \n",
    "            for uid, user_group in df.groupby('uid'):\n",
    "                if len(user_group) < self.sequence_length + self.prediction_steps:\n",
    "                    continue\n",
    "                    \n",
    "                sequences = self._build_sequences_for_user(user_group)\n",
    "                \n",
    "                for seq in sequences:\n",
    "                    yield (\n",
    "                        torch.tensor(seq['uid'], dtype=torch.long),\n",
    "                        torch.tensor(seq['d'], dtype=torch.long),\n",
    "                        torch.tensor(seq['t'], dtype=torch.long),\n",
    "                        torch.tensor(seq['city_idx'], dtype=torch.long),\n",
    "                        torch.from_numpy(seq['poi']),\n",
    "                        torch.from_numpy(seq['coords_seq']),\n",
    "                        torch.from_numpy(seq['target_coords'])\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e3bbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Funde dois vetores de mesmo tamanho por uma soma ponderada aprend√≠vel.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 20, init_w_r: float = 0.5, init_w_e: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.w_r = nn.Parameter(torch.tensor(init_w_r, dtype=torch.float32))\n",
    "        self.w_e = nn.Parameter(torch.tensor(init_w_e, dtype=torch.float32))\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, static_red: torch.Tensor, dyn_emb: torch.Tensor) -> torch.Tensor:\n",
    "        assert static_red.shape == dyn_emb.shape and static_red.size(1) == self.dim\n",
    "        fused = self.w_r * static_red + self.w_e * dyn_emb\n",
    "        return fused\n",
    "\n",
    "\n",
    "class MLP500(nn.Module):\n",
    "    \"\"\"MLP simples com 1 hidden layer de 500 ReLUs.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, n_clusters: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, n_clusters)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class DestinationHead(nn.Module):\n",
    "    \"\"\"Combina MLP + softmax + weighted sum pelos cluster centers.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, cluster_centers: torch.Tensor):\n",
    "        super().__init__()\n",
    "        C, coord_dim = cluster_centers.shape\n",
    "        assert coord_dim == 2\n",
    "        self.mlp500 = MLP500(in_dim, hidden_dim, C)\n",
    "        self.register_buffer(\"centers\", cluster_centers)\n",
    "\n",
    "    def forward(self, fused: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.mlp500(fused)\n",
    "        P = F.softmax(logits, dim=1)\n",
    "        coords = P @ self.centers\n",
    "        return coords\n",
    "\n",
    "\n",
    "def discretize_coordinates(coords_pred: torch.Tensor, grid_size: int = 200):\n",
    "    \"\"\"Converte coordenadas cont√≠nuas para grid discreto [0, grid_size-1].\"\"\"\n",
    "    coords_discrete = torch.round(coords_pred).long()\n",
    "    coords_discrete = torch.clamp(coords_discrete, 0, grid_size - 1)\n",
    "    return coords_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b997e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuMobModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo completo que combina todas as partes e faz rollout para m√∫ltiplos passos.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users: int,\n",
    "        n_days: int,\n",
    "        n_slots: int,\n",
    "        n_cities: int,\n",
    "        cluster_centers: torch.Tensor,\n",
    "        emb_dim: int = 10,\n",
    "        poi_in_dim: int = 85,\n",
    "        poi_out_dim: int = 10,\n",
    "        lstm_hidden: int = 10,\n",
    "        fusion_dim: int = 20,\n",
    "        sequence_length: int = 48,\n",
    "        prediction_steps: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Salva configura√ß√µes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "        \n",
    "        # Componentes\n",
    "        self.fusion = ExternalInformationFusionDTPC(\n",
    "            n_users=n_users,\n",
    "            n_days=n_days,\n",
    "            n_slots=n_slots,\n",
    "            n_cities=n_cities,\n",
    "            emb_dim=emb_dim,\n",
    "            poi_in_dim=poi_in_dim,\n",
    "            poi_out_dim=poi_out_dim\n",
    "        )\n",
    "        self.dense = ExternalInformationDense(\n",
    "            in_dim=self.fusion.out_dim, \n",
    "            out_dim=fusion_dim\n",
    "        )\n",
    "        self.lstm = CoordLSTM(\n",
    "            input_size=2, \n",
    "            hidden_size=lstm_hidden, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.weighted_fusion = WeightedFusion(dim=fusion_dim)\n",
    "        self.destination_head = DestinationHead(\n",
    "            in_dim=fusion_dim,\n",
    "            hidden_dim=500,\n",
    "            cluster_centers=cluster_centers\n",
    "        )\n",
    "        \n",
    "    def forward_single_step(self, uid, d, t, city, poi, coords_seq):\n",
    "        \"\"\"Faz uma predi√ß√£o para um √∫nico passo.\"\"\"\n",
    "        # Informa√ß√£o est√°tica (contexto)\n",
    "        static_emb = self.fusion(uid, d, t, city, poi)\n",
    "        static_red = self.dense(static_emb)\n",
    "        \n",
    "        # Informa√ß√£o din√¢mica (padr√£o de movimento)\n",
    "        dyn_emb = self.lstm(coords_seq)\n",
    "        \n",
    "        # Fus√£o inteligente\n",
    "        fused = self.weighted_fusion(static_red, dyn_emb)\n",
    "        \n",
    "        # Predi√ß√£o final\n",
    "        pred_coords = self.destination_head(fused)\n",
    "        \n",
    "        return pred_coords\n",
    "    \n",
    "    def rollout_predictions(\n",
    "        self, \n",
    "        uid, d, t, city, poi, coords_seq, \n",
    "        n_steps: int,\n",
    "        use_predictions: bool = True\n",
    "    ):\n",
    "        \"\"\"Faz predi√ß√µes para m√∫ltiplos passos futuros.\"\"\"\n",
    "        predictions = []\n",
    "        current_seq = coords_seq.clone()\n",
    "        current_t = t.clone()\n",
    "        current_d = d.clone()\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Prediz pr√≥ximo passo\n",
    "            pred = self.forward_single_step(uid, current_d, current_t, city, poi, current_seq)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            if use_predictions and step < n_steps - 1:\n",
    "                # Atualiza sequ√™ncia: remove primeiro ponto, adiciona predi√ß√£o\n",
    "                new_point = pred.unsqueeze(1)  # (batch, 1, 2)\n",
    "                current_seq = torch.cat([current_seq[:, 1:, :], new_point], dim=1)\n",
    "            \n",
    "            # Incrementa tempo corretamente (wrap em 48 slots)\n",
    "            current_t = (current_t + 1) % 48\n",
    "            # Se voltou para slot 0, incrementa dia\n",
    "            mask_new_day = (current_t == 0)\n",
    "            current_d = current_d + mask_new_day.long()\n",
    "            \n",
    "        return torch.stack(predictions, dim=1)  # (batch, n_steps, 2)\n",
    "    \n",
    "    def forward(self, uid, d, t, city, poi, coords_seq, n_steps=1):\n",
    "        \"\"\"Forward principal - pode ser usado tanto para treino quanto infer√™ncia.\"\"\"\n",
    "        if n_steps == 1:\n",
    "            return self.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "        else:\n",
    "            return self.rollout_predictions(uid, d, t, city, poi, coords_seq, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "adb9adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_on_city_A(\n",
    "    parquet_path: str,\n",
    "    centers: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    n_epochs: int = 6,\n",
    "    learning_rate: float = 2e-3,\n",
    "    batch_size: int = 32,\n",
    "    sequence_length: int = 48,\n",
    "    save_path: str = \"ckpt_A_warmup.pt\"\n",
    "):\n",
    "    \"\"\"Pr√©-treina o modelo na cidade A para validar o pipeline.\"\"\"\n",
    "    print(\"üèãÔ∏è Iniciando pr√©-treino na cidade A...\")\n",
    "    \n",
    "    # 1. Cria loaders\n",
    "    train_loader, val_loader = create_pretrain_loaders(\n",
    "        parquet_path=parquet_path,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # 2. Instancia modelo\n",
    "    model = HuMobModel(\n",
    "        n_users=100_000,  # cidade A\n",
    "        n_days=75,\n",
    "        n_slots=48,\n",
    "        n_cities=4,\n",
    "        cluster_centers=centers,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_steps=1  # Next-step apenas\n",
    "    ).to(device)\n",
    "    \n",
    "    # 3. Setup de treino (MSE apenas para come√ßar)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Par√¢metros trein√°veis: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # === TREINO ===\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        train_count = 0\n",
    "        \n",
    "        print(f\"\\nüîÑ √âpoca {epoch+1}/{n_epochs}\")\n",
    "        train_pbar = tqdm(train_loader, desc=f'Treino A')\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward (apenas next-step)\n",
    "            pred = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "            target = target_coords.squeeze(1)  # Remove dimens√£o de step\n",
    "            \n",
    "            # Loss MSE simples\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # M√©tricas\n",
    "            train_loss_epoch += loss.item() * target.size(0)\n",
    "            train_count += target.size(0)\n",
    "            \n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "        \n",
    "        # === VALIDA√á√ÉO ===\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        val_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc='Val A')\n",
    "            for batch in val_pbar:\n",
    "                uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "                \n",
    "                pred = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "                target = target_coords.squeeze(1)\n",
    "                \n",
    "                loss = criterion(pred, target)\n",
    "                \n",
    "                val_loss_epoch += loss.item() * target.size(0)\n",
    "                val_count += target.size(0)\n",
    "                \n",
    "                val_pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # M√©dias\n",
    "        avg_train_loss = train_loss_epoch / max(train_count, 1)\n",
    "        avg_val_loss = val_loss_epoch / max(val_count, 1)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Treino: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}\")\n",
    "        print(f\"Fusion weights: w_r={model.weighted_fusion.w_r.item():.3f}, w_e={model.weighted_fusion.w_e.item():.3f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Salva melhor modelo\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"üíæ Novo melhor modelo! Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            torch.save({\n",
    "                'state_dict': model.state_dict(),\n",
    "                'centers': centers.cpu().numpy(),\n",
    "                'config': {\n",
    "                    'sequence_length': sequence_length,\n",
    "                    'prediction_steps': 1,\n",
    "                    'n_clusters': centers.shape[0]\n",
    "                },\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pr√©-treino conclu√≠do! Modelo salvo em: {save_path}\")\n",
    "    print(f\"Melhor loss de valida√ß√£o: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_zero_shot_on_D(\n",
    "    parquet_path: str,\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    n_samples: int = 5000,\n",
    "    sequence_length: int = 48\n",
    "):\n",
    "    \"\"\"Avalia o modelo pr√©-treinado em A na cidade D (zero-shot).\"\"\"\n",
    "    print(\"üéØ Avalia√ß√£o zero-shot em cidade D...\")\n",
    "    \n",
    "    # 1. Carrega checkpoint\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    centers = torch.from_numpy(ckpt['centers']).to(device)\n",
    "    \n",
    "    # 2. Instancia modelo\n",
    "    model = HuMobModel(\n",
    "        n_users=6_000,  # cidade D\n",
    "        n_days=75,\n",
    "        n_slots=48,\n",
    "        n_cities=4,\n",
    "        cluster_centers=centers,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_steps=1\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 3. Cria dataset de avalia√ß√£o em D\n",
    "    eval_ds = ParquetCityDDataset(\n",
    "        parquet_path=parquet_path,\n",
    "        city_list=[\"D\"],\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_steps=1,\n",
    "        chunk_size=2500\n",
    "    )\n",
    "    \n",
    "    # 4. Avalia em amostra\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    coords_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader = DataLoader(eval_ds, batch_size=32, num_workers=2)\n",
    "        pbar = tqdm(eval_loader, desc='Eval D')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            if total_samples >= n_samples:\n",
    "                break\n",
    "                \n",
    "            uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Filtra apenas dados ‚â§ dia 60 (observados)\n",
    "            mask = d <= 60\n",
    "            if not mask.any():\n",
    "                continue\n",
    "                \n",
    "            uid, d, t, city, poi = uid[mask], d[mask], t[mask], city[mask], poi[mask]\n",
    "            coords_seq, target_coords = coords_seq[mask], target_coords[mask]\n",
    "            \n",
    "            pred = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "            target = target_coords.squeeze(1)\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            total_samples += target.size(0)\n",
    "            \n",
    "            # Calcula erro em c√©lulas (ap√≥s discretiza√ß√£o)\n",
    "            pred_discrete = discretize_coordinates(pred)\n",
    "            target_discrete = discretize_coordinates(target)\n",
    "            cell_error = torch.abs(pred_discrete - target_discrete).float().mean(dim=1)\n",
    "            coords_errors.extend(cell_error.cpu().tolist())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'MSE': f'{loss.item():.4f}',\n",
    "                'Samples': total_samples\n",
    "            })\n",
    "    \n",
    "    avg_mse = total_loss / max(total_samples, 1)\n",
    "    avg_cell_error = np.mean(coords_errors)\n",
    "    \n",
    "    print(f\"\\nüìä Resultados zero-shot em D:\")\n",
    "    print(f\"  MSE: {avg_mse:.4f}\")\n",
    "    print(f\"  Erro m√©dio em c√©lulas: {avg_cell_error:.2f}\")\n",
    "    print(f\"  Amostras avaliadas: {total_samples:,}\")\n",
    "    \n",
    "    return avg_mse, avg_cell_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eb2e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretrain_pipeline(\n",
    "    parquet_path: str = \"humob_all_cities_dpsk.parquet\",\n",
    "    device: torch.device = None,\n",
    "    n_clusters: int = 1024,\n",
    "    pretrain_epochs: int = 6,\n",
    "    sequence_length: int = 48,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 2e-3\n",
    "):\n",
    "    \"\"\"Pipeline completo de pr√©-treino.\"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(\"üöÄ Iniciando pipeline de pr√©-treino HuMob\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Clusters: {n_clusters}\")\n",
    "    print(f\"Epochs: {pretrain_epochs}\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. CENTROS EST√ÅVEIS\n",
    "    print(\"\\nüìç ETAPA 1: Calculando centros est√°veis...\")\n",
    "    centers = load_or_compute_centers(\n",
    "        parquet_path=parquet_path,\n",
    "        cities=[\"A\"],\n",
    "        n_clusters=n_clusters,\n",
    "        save_path=f\"centers_A_{n_clusters}.npy\"\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Centros prontos: {centers.shape}\")\n",
    "    \n",
    "    # 2. PR√â-TREINO EM A\n",
    "    print(\"\\nüèãÔ∏è ETAPA 2: Pr√©-treino na cidade A...\")\n",
    "    model, train_losses, val_losses = pretrain_on_city_A(\n",
    "        parquet_path=parquet_path,\n",
    "        centers=centers,\n",
    "        device=device,\n",
    "        n_epochs=pretrain_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=sequence_length,\n",
    "        save_path=\"ckpt_A_warmup.pt\"\n",
    "    )\n",
    "    \n",
    "    # Plot das curvas\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Val')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Pr√©-treino em A')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot([model.weighted_fusion.w_r.item()], [model.weighted_fusion.w_e.item()], 'ro', markersize=10)\n",
    "    plt.xlabel('w_r (est√°tico)')\n",
    "    plt.ylabel('w_e (din√¢mico)')\n",
    "    plt.title('Pesos da Fus√£o')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pretrain_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. AVALIA√á√ÉO ZERO-SHOT EM D\n",
    "    print(\"\\nüéØ ETAPA 3: Avalia√ß√£o zero-shot na cidade D...\")\n",
    "    mse_d, cell_error_d = evaluate_zero_shot_on_D(\n",
    "        parquet_path=parquet_path,\n",
    "        checkpoint_path=\"ckpt_A_warmup.pt\",\n",
    "        device=device,\n",
    "        n_samples=5000,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # 4. RELAT√ìRIO FINAL\n",
    "    print(\"\\nüìä RELAT√ìRIO FINAL\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"‚úÖ Pr√©-treino conclu√≠do em {pretrain_epochs} √©pocas\")\n",
    "    print(f\"üìà Loss final de treino: {train_losses[-1]:.4f}\")\n",
    "    print(f\"üìâ Loss final de valida√ß√£o: {val_losses[-1]:.4f}\")\n",
    "    print(f\"üéØ MSE zero-shot em D: {mse_d:.4f}\")\n",
    "    print(f\"üìç Erro m√©dio em c√©lulas: {cell_error_d:.2f}\")\n",
    "    print(f\"‚öñÔ∏è Pesos da fus√£o: w_r={model.weighted_fusion.w_r.item():.3f}, w_e={model.weighted_fusion.w_e.item():.3f}\")\n",
    "    print(f\"üíæ Checkpoint salvo: ckpt_A_warmup.pt\")\n",
    "    print(f\"üé® Gr√°ficos salvos: pretrain_results.png\")\n",
    "    \n",
    "    # Verifica se est√° tudo OK\n",
    "    print(\"\\nüîç VERIFICA√á√ïES:\")\n",
    "    converged = val_losses[-1] < val_losses[0] * 0.8\n",
    "    reasonable_mse = mse_d < 1000\n",
    "    reasonable_error = cell_error_d < 20\n",
    "    \n",
    "    print(f\"  Convergiu? {'‚úÖ' if converged else '‚ùå'} (loss diminuiu 20%+)\")\n",
    "    print(f\"  MSE razo√°vel? {'‚úÖ' if reasonable_mse else '‚ùå'} (< 1000)\")\n",
    "    print(f\"  Erro de c√©lulas OK? {'‚úÖ' if reasonable_error else '‚ùå'} (< 20 c√©lulas)\")\n",
    "    \n",
    "    all_good = converged and reasonable_mse and reasonable_error\n",
    "    \n",
    "    if all_good:\n",
    "        print(\"\\nüéâ PIPELINE EXECUTADO COM SUCESSO!\")\n",
    "        print(\"   Pronto para pr√≥ximas etapas:\")\n",
    "        print(\"   - Fine-tune em D (opcional)\")\n",
    "        print(\"   - Rollout para submiss√£o (dias 61-75)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ALGUNS PROBLEMAS DETECTADOS\")\n",
    "        print(\"   Recomenda√ß√µes:\")\n",
    "        if not converged:\n",
    "            print(\"   - Aumentar n√∫mero de √©pocas ou ajustar LR\")\n",
    "        if not reasonable_mse:\n",
    "            print(\"   - Verificar normaliza√ß√£o dos dados\")\n",
    "        if not reasonable_error:\n",
    "            print(\"   - Revisar c√°lculo de centros ou head\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'centers': centers,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'mse_zero_shot': mse_d,\n",
    "        'cell_error': cell_error_d,\n",
    "        'checkpoint_path': 'ckpt_A_warmup.pt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26036ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Executando estrat√©gia de pr√©-treino sugerida...\n",
      "Device: cuda\n",
      "Estrat√©gia: A (pr√©-treino) ‚Üí D (zero-shot) ‚Üí D (fine-tune opcional)\n",
      "============================================================\n",
      "üöÄ Iniciando pipeline de pr√©-treino HuMob\n",
      "Device: cuda\n",
      "Clusters: 64\n",
      "Epochs: 2\n",
      "Sequence length: 6\n",
      "==================================================\n",
      "\n",
      "üìç ETAPA 1: Calculando centros est√°veis...\n",
      "üîÑ Calculando 64 centros para cidades ['A']...\n",
      "üìä Coletadas 88,405,298 coordenadas\n",
      "üé≤ Amostradas 200,000 coordenadas\n",
      "‚öôÔ∏è Executando K-Means...\n",
      "üíæ Centros salvos em: centers_A_64.npy\n",
      "üìè Shape dos centros: (64, 2)\n",
      "‚úÖ Centros prontos: torch.Size([64, 2])\n",
      "\n",
      "üèãÔ∏è ETAPA 2: Pr√©-treino na cidade A...\n",
      "üèãÔ∏è Iniciando pr√©-treino na cidade A...\n",
      "Par√¢metros trein√°veis: 1,046,836\n",
      "\n",
      "üîÑ √âpoca 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treino A: 94it [00:27,  3.44it/s, Loss=nan, LR=0.001000]      Traceback (most recent call last):\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/queues.py\", line 270, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/connection.py\", line 427, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/connection.py\", line 384, in _send\n",
      "    n = write(self._handle, buf)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/home/andersonc/anaconda3/envs/orion_ct/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Treino A: 568it [02:34,  3.69it/s, Loss=nan, LR=0.001000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# # EXECUTA O PIPELINE PRINCIPAL\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# results = run_pretrain_pipeline(\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     parquet_path=parquet_file,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#     learning_rate=2e-3\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m results = \u001b[43mrun_pretrain_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# ‚¨áÔ∏è Era 1024 ‚Üí 256 (4x menos centros)\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# ‚¨áÔ∏è Era 6 ‚Üí 2 (3x menos √©pocas)  \u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# ‚¨áÔ∏è Era 48 ‚Üí 12 (4x menos hist√≥rico)\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m     \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Verifica se o pr√©-treino foi bem-sucedido\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mmse_zero_shot\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m1000\u001b[39m:  \u001b[38;5;66;03m# Limite razo√°vel\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mrun_pretrain_pipeline\u001b[39m\u001b[34m(parquet_path, device, n_clusters, pretrain_epochs, sequence_length, batch_size, learning_rate)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 2. PR√â-TREINO EM A\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müèãÔ∏è ETAPA 2: Pr√©-treino na cidade A...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m model, train_losses, val_losses = \u001b[43mpretrain_on_city_A\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mckpt_A_warmup.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Plot das curvas\u001b[39;00m\n\u001b[32m     47\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mpretrain_on_city_A\u001b[39m\u001b[34m(parquet_path, centers, device, n_epochs, learning_rate, batch_size, sequence_length, save_path)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÑ √âpoca \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m train_pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mTreino A\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_pbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_coords\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         data.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     35\u001b[39m         \u001b[38;5;28mself\u001b[39m.ended = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 123\u001b[39m, in \u001b[36mCityAPretrainDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(user_group) < \u001b[38;5;28mself\u001b[39m.sequence_length + \u001b[38;5;28mself\u001b[39m.prediction_steps:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m sequences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_user_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_sequences_per_user\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m (\n\u001b[32m    130\u001b[39m         torch.tensor(seq[\u001b[33m'\u001b[39m\u001b[33muid\u001b[39m\u001b[33m'\u001b[39m], dtype=torch.long),\n\u001b[32m    131\u001b[39m         torch.tensor(seq[\u001b[33m'\u001b[39m\u001b[33md\u001b[39m\u001b[33m'\u001b[39m], dtype=torch.long),\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m         torch.from_numpy(seq[\u001b[33m'\u001b[39m\u001b[33mtarget_coords\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    137\u001b[39m     )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mCityAPretrainDataset._sample_user_sequences\u001b[39m\u001b[34m(self, user_group, max_seqs)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sample_user_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, user_group, max_seqs: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     30\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Amostra sequ√™ncias de um usu√°rio de forma estratificada.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     sequences = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_sequences_for_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sequences) <= max_seqs:\n\u001b[32m     34\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mCityAPretrainDataset._build_sequences_for_user\u001b[39m\u001b[34m(self, user_data)\u001b[39m\n\u001b[32m     62\u001b[39m     coords_seq = user_data.iloc[seq_start:seq_end][[\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]].values\n\u001b[32m     63\u001b[39m     current_info = user_data.iloc[seq_end - \u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     target_coords = \u001b[43muser_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtarget_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.values\n\u001b[32m     66\u001b[39m     sequences.append({\n\u001b[32m     67\u001b[39m         \u001b[33m'\u001b[39m\u001b[33muid\u001b[39m\u001b[33m'\u001b[39m: current_info[\u001b[33m'\u001b[39m\u001b[33muid\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     68\u001b[39m         \u001b[33m'\u001b[39m\u001b[33md\u001b[39m\u001b[33m'\u001b[39m: current_info[\u001b[33m'\u001b[39m\u001b[33md\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtarget_coords\u001b[39m\u001b[33m'\u001b[39m: target_coords.astype(np.float32)\n\u001b[32m     74\u001b[39m     })\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sequences\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/pandas/core/indexes/base.py:6192\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6190\u001b[39m keyarr = key\n\u001b[32m   6191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keyarr, Index):\n\u001b[32m-> \u001b[39m\u001b[32m6192\u001b[39m     keyarr = \u001b[43mcom\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray_tuplesafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6194\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._index_as_unique:\n\u001b[32m   6195\u001b[39m     indexer = \u001b[38;5;28mself\u001b[39m.get_indexer_for(keyarr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/orion_ct/lib/python3.12/site-packages/pandas/core/common.py:247\u001b[39m, in \u001b[36masarray_tuplesafe\u001b[39m\u001b[34m(values, dtype)\u001b[39m\n\u001b[32m    245\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np_version_gte1p24:\n\u001b[32m    246\u001b[39m             warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m, np.VisibleDeprecationWarning)\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m         result = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# Using try/except since it's more performant than checking is_list_like\u001b[39;00m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# over each element\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"construct_1d_object_array_from_listlike\"\u001b[39;00m\n\u001b[32m    252\u001b[39m     \u001b[38;5;66;03m# has incompatible type \"Iterable[Any]\"; expected \"Sized\"\u001b[39;00m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m construct_1d_object_array_from_listlike(values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# üöÄ EXECU√á√ÉO PRINCIPAL - ESTRAT√âGIA DE PR√â-TREINO\n",
    "# ===================================================\n",
    "\n",
    "print(\"üéØ Executando estrat√©gia de pr√©-treino sugerida...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Estrat√©gia: A (pr√©-treino) ‚Üí D (zero-shot) ‚Üí D (fine-tune opcional)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# # EXECUTA O PIPELINE PRINCIPAL\n",
    "# results = run_pretrain_pipeline(\n",
    "#     parquet_path=parquet_file,\n",
    "#     device=device,\n",
    "#     n_clusters=1024,        # Est√°vel e r√°pido\n",
    "#     pretrain_epochs=6,      # R√°pido para validar\n",
    "#     sequence_length=48,     # 1 dia de hist√≥rico\n",
    "#     batch_size=32,\n",
    "#     learning_rate=2e-3\n",
    "# )\n",
    "\n",
    "results = run_pretrain_pipeline(\n",
    "    parquet_path=parquet_file,\n",
    "    device=device,\n",
    "    n_clusters=64,         # ‚¨áÔ∏è Era 1024 ‚Üí 256 (4x menos centros)\n",
    "    pretrain_epochs=2,      # ‚¨áÔ∏è Era 6 ‚Üí 2 (3x menos √©pocas)  \n",
    "    sequence_length=6,     # ‚¨áÔ∏è Era 48 ‚Üí 12 (4x menos hist√≥rico)\n",
    "    batch_size=32,         \n",
    "    learning_rate=1e-3     \n",
    ")\n",
    "\n",
    "# Verifica se o pr√©-treino foi bem-sucedido\n",
    "if results['mse_zero_shot'] < 1000:  # Limite razo√°vel\n",
    "    print(\"\\n‚úÖ Pr√©-treino bem-sucedido! Modelo pronto para uso.\")\n",
    "    print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "    print(f\"   1. Modelo final salvo em: {results['checkpoint_path']}\")\n",
    "    print(\"   2. Para submiss√£o HuMob, use create_submission_data()\")\n",
    "    print(\"   3. Para rollout completo (15 dias), ajuste n_steps=48*15\")\n",
    "    print(\"   4. Lembre-se de discretizar coordenadas [0,199]\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pr√©-treino n√£o convergiu bem.\")\n",
    "    print(\"üí° Sugest√µes:\")\n",
    "    print(\"   - Aumentar n√∫mero de √©pocas\")\n",
    "    print(\"   - Ajustar learning rate\")\n",
    "    print(\"   - Verificar normaliza√ß√£o dos POIs\")\n",
    "    print(\"   - Revisar sequ√™ncia temporal do dataset\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ Pipeline executado com sucesso!\")\n",
    "print(\"üìÅ Arquivos gerados:\")\n",
    "print(\"   - centers_A_1024.npy (centros est√°veis)\")\n",
    "print(\"   - ckpt_A_warmup.pt (modelo pr√©-treinado)\")\n",
    "print(\"   - pretrain_results.png (gr√°ficos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c1e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68433bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

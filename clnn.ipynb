{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93226949",
   "metadata": {},
   "source": [
    "Imports Completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c05c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Configura√ß√£o inicial\n",
      "Device: cpu\n",
      "Arquivo: humob_all_cities_v2_normalized.parquet\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seus m√≥dulos locais\n",
    "from external_information import ExternalInformationFusionNormalized, ExternalInformationDense\n",
    "from partial_information import CoordLSTM\n",
    "\n",
    "# Configura√ß√µes\n",
    "parquet_file = \"humob_all_cities_v2_normalized.parquet\"\n",
    "n_users_by_city = {\"A\": 100_000, \"B\": 25_000, \"C\": 20_000, \"D\": 6_000}\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"üöÄ Configura√ß√£o inicial\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Arquivo: {parquet_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd29efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_int_clip(series: pd.Series, lo: int, hi: int) -> pd.Series:\n",
    "    \"\"\"Converte para inteiro e clampa em [lo, hi]. Conta quantas linhas foram ajustadas.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").fillna(lo).astype(np.int64)\n",
    "    before_min, before_max = int(s.min()), int(s.max())\n",
    "    s = s.clip(lo, hi)\n",
    "    after_min, after_max = int(s.min()), int(s.max())\n",
    "    fixed = (before_min < lo) or (before_max > hi)\n",
    "    return s, fixed, (before_min, before_max), (after_min, after_max)\n",
    "\n",
    "def sanitize_xy(df: pd.DataFrame, grid_size: int = 200) -> pd.DataFrame:\n",
    "    lo, hi = 0, grid_size - 1\n",
    "    df = df.copy()\n",
    "    df[\"x\"], fx, rngx_before, rngx_after = _coerce_int_clip(df[\"x\"], lo, hi)\n",
    "    df[\"y\"], fy, rngy_before, rngy_after = _coerce_int_clip(df[\"y\"], lo, hi)\n",
    "    if fx or fy:\n",
    "        print(f\"‚ö†Ô∏è clamp x {rngx_before}‚Üí{rngx_after} | y {rngy_before}‚Üí{rngy_after}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de00896",
   "metadata": {},
   "source": [
    " C√©lula 2: Centros Est√°veis (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81924d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stable_centers(\n",
    "    parquet_path: str,\n",
    "    cities: list[str] = [\"A\"],\n",
    "    day_threshold: int = 60,\n",
    "    n_clusters: int = 1024,\n",
    "    sample_size: int = 200_000,\n",
    "    save_path: str = \"centers_stable.npy\",\n",
    "    chunk_size: int = 50_000\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calcula centros est√°veis usando K-Means.\n",
    "    Mais robusto que HDBSCAN: sempre produz exatamente n_clusters centros.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se j√° existe, carrega\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"üìÇ Carregando centros existentes: {save_path}\")\n",
    "        centers = np.load(save_path)\n",
    "        return torch.from_numpy(centers.astype(np.float32))\n",
    "    \n",
    "    print(f\"üîÑ Calculando {n_clusters} centros para cidades {cities}...\")\n",
    "    \n",
    "    # 1) Coleta coordenadas\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    coords_list = []\n",
    "    \n",
    "    for batch in pf.iter_batches(batch_size=chunk_size):\n",
    "        tbl = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "        \n",
    "        # Filtra cidades e dias\n",
    "        city_mask = pc.is_in(tbl.column(\"city\"), pa.array(cities))\n",
    "        day_mask = pc.less(tbl.column(\"d\"), day_threshold)\n",
    "        mask = pc.and_(city_mask, day_mask)\n",
    "        \n",
    "        tbl = tbl.filter(mask)\n",
    "        if tbl.num_rows == 0:\n",
    "            continue\n",
    "            \n",
    "        xs = tbl.column(\"x\").to_numpy()\n",
    "        ys = tbl.column(\"y\").to_numpy()\n",
    "        coords_list.append(np.stack([xs, ys], axis=1))\n",
    "    \n",
    "    coords = np.vstack(coords_list)\n",
    "    print(f\"üìä Coletadas {len(coords):,} coordenadas\")\n",
    "    \n",
    "    # 2) Amostra se muito grande\n",
    "    if len(coords) > sample_size:\n",
    "        idx = np.random.choice(len(coords), sample_size, replace=False)\n",
    "        coords = coords[idx]\n",
    "        print(f\"üé≤ Amostradas {sample_size:,} coordenadas\")\n",
    "    \n",
    "    # 3) K-Means\n",
    "    print(\"‚öôÔ∏è Executando K-Means...\")\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters, \n",
    "        n_init='auto', \n",
    "        random_state=42,\n",
    "        max_iter=300\n",
    "    ).fit(coords)\n",
    "    \n",
    "    centers = kmeans.cluster_centers_.astype(np.float32)\n",
    "    centers = centers / 200.0  # Normaliza centros tamb√©m\n",
    "    \n",
    "    # 4) Salva para reutilizar\n",
    "    np.save(save_path, centers)\n",
    "    print(f\"üíæ Centros salvos em: {save_path}\")\n",
    "    print(f\"üìè Shape dos centros: {centers.shape}\")\n",
    "    \n",
    "    return torch.from_numpy(centers)\n",
    "\n",
    "\n",
    "def load_or_compute_centers(\n",
    "    parquet_path: str,\n",
    "    cities: list[str] = [\"A\"],\n",
    "    n_clusters: int = 1024,\n",
    "    save_path: str = \"centers_stable.npy\"\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Fun√ß√£o conveniente que carrega se existe, sen√£o calcula.\"\"\"\n",
    "    return compute_stable_centers(\n",
    "        parquet_path=parquet_path,\n",
    "        cities=cities,\n",
    "        n_clusters=n_clusters,\n",
    "        save_path=save_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a231eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityAPretrainDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Dataset otimizado para pr√©-treino em cidade A.\n",
    "    Faz amostragem estratificada por usu√°rio e divide train/val por dias.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        parquet_path: str,\n",
    "        mode: str = \"train\",  # \"train\" ou \"val\"\n",
    "        sequence_length: int = 48,\n",
    "        prediction_steps: int = 1,\n",
    "        chunk_size: int = 10_000,\n",
    "        max_sequences_per_user: int = 50,\n",
    "        train_days: tuple = (1, 55),\n",
    "        val_days: tuple = (56, 60)\n",
    "    ):\n",
    "        self.parquet_path = parquet_path\n",
    "        self.mode = mode\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_sequences_per_user = max_sequences_per_user\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            self.day_range = train_days\n",
    "        else:\n",
    "            self.day_range = val_days\n",
    "\n",
    "    def check_data_sanity(self, df):\n",
    "        \"\"\"Verifica se dados est√£o OK antes do treino - vers√£o robusta\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        # 1. Verifica coordenadas (mais flex√≠vel)\n",
    "        x_min, x_max = df['x'].min(), df['x'].max()\n",
    "        y_min, y_max = df['y'].min(), df['y'].max()\n",
    "        \n",
    "        if x_min < 0 or x_max > 200:  # üîÑ Mudou de < 200 para <= 200\n",
    "            issues.append(f\"x fora do range [0,200]: [{x_min}, {x_max}]\")\n",
    "        \n",
    "        if y_min < 0 or y_max > 200:  # üîÑ Mudou de < 200 para <= 200  \n",
    "            issues.append(f\"y fora do range [0,200]: [{y_min}, {y_max}]\")\n",
    "        \n",
    "        # 2. Verifica POIs (mais robusto)\n",
    "        poi_cols = [col for col in df.columns if 'POI' in col]\n",
    "        for col in poi_cols:\n",
    "            if col in df.columns:\n",
    "                try:\n",
    "                    sample_vals = df[col].iloc[:3]  # Menos amostras para speed\n",
    "                    for i, val in enumerate(sample_vals):\n",
    "                        if hasattr(val, '__len__') and not isinstance(val, str):\n",
    "                            arr = np.array(val, dtype=np.float32)\n",
    "                            if not np.isfinite(arr).all():\n",
    "                                issues.append(f\"POI {col}[{i}] cont√©m NaN/Inf\")\n",
    "                                break  # Para no primeiro erro\n",
    "                except Exception as e:\n",
    "                    issues.append(f\"Erro ao verificar POI {col}: {str(e)[:50]}...\")\n",
    "        \n",
    "        # 3. Log e decis√£o inteligente\n",
    "        if issues:\n",
    "            print(f\"‚ö†Ô∏è  Encontrados {len(issues)} problemas nos dados:\")\n",
    "            for issue in issues[:3]:  # Mostra s√≥ os 3 primeiros\n",
    "                print(f\"   ‚Ä¢ {issue}\")\n",
    "            if len(issues) > 3:\n",
    "                print(f\"   ‚Ä¢ ... e mais {len(issues)-3} problemas\")\n",
    "            \n",
    "            # üéØ Decis√£o inteligente: s√≥ falha em casos graves\n",
    "            grave_issues = [i for i in issues if 'fora do range' in i and ('> 220' in i or '< -20' in i)]\n",
    "            if grave_issues:\n",
    "                print(\"   ‚ùå Problemas graves detectados, pulando batch\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"   ‚ö° Problemas menores, continuando com corre√ß√µes autom√°ticas...\")\n",
    "                return True\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def auto_fix_data(self, df):\n",
    "        \"\"\"Corrige automaticamente problemas menores nos dados\"\"\"\n",
    "        # Clamp coordenadas para range v√°lido\n",
    "        df['x'] = df['x'].clip(0, 199)  # üîÑ For√ßa range [0,199]\n",
    "        df['y'] = df['y'].clip(0, 199)  # üîÑ For√ßa range [0,199] \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _sample_user_sequences(self, user_group, max_seqs: int):\n",
    "        \"\"\"Amostra sequ√™ncias de um usu√°rio de forma estratificada.\"\"\"\n",
    "        sequences = self._build_sequences_for_user(user_group)\n",
    "        \n",
    "        if len(sequences) <= max_seqs:\n",
    "            return sequences\n",
    "        \n",
    "        # Amostragem estratificada por hora do dia\n",
    "        sequences_by_hour = {}\n",
    "        for seq in sequences:\n",
    "            hour = seq['t'] // 2  # agrupa slots em horas (48 slots / 24h)\n",
    "            if hour not in sequences_by_hour:\n",
    "                sequences_by_hour[hour] = []\n",
    "            sequences_by_hour[hour].append(seq)\n",
    "        \n",
    "        # Amostra proporcionalmente de cada hora\n",
    "        sampled = []\n",
    "        for hour_seqs in sequences_by_hour.values():\n",
    "            n_sample = max(1, min(len(hour_seqs), max_seqs // len(sequences_by_hour)))\n",
    "            sampled.extend(np.random.choice(hour_seqs, n_sample, replace=False))\n",
    "        \n",
    "        return sampled[:max_seqs]\n",
    "    \n",
    "    def _build_sequences_for_user(self, user_data):\n",
    "        \"\"\"Constr√≥i sequ√™ncias temporais para um usu√°rio espec√≠fico.\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(len(user_data) - self.sequence_length - self.prediction_steps + 1):\n",
    "            seq_start = i\n",
    "            seq_end = i + self.sequence_length\n",
    "            target_start = seq_end\n",
    "            target_end = target_start + self.prediction_steps\n",
    "            \n",
    "            coords_seq = user_data.iloc[seq_start:seq_end][['x', 'y']].values / 200.0  # Normaliza\n",
    "            current_info = user_data.iloc[seq_end - 1]\n",
    "            target_coords = user_data.iloc[target_start:target_end][['x', 'y']].values / 200.0  # Normaliza\n",
    "            \n",
    "            sequences.append({\n",
    "                'uid': current_info['uid'],\n",
    "                'd': current_info['d'], \n",
    "                't': current_info['t'],\n",
    "                'city_idx': 0,  # A √© sempre 0\n",
    "                'poi': current_info['POI'],\n",
    "                'coords_seq': coords_seq.astype(np.float32),\n",
    "                'target_coords': target_coords.astype(np.float32)\n",
    "            })\n",
    "            \n",
    "        return sequences\n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        pf = pq.ParquetFile(self.parquet_path)\n",
    "        \n",
    "        for batch in pf.iter_batches(batch_size=self.chunk_size):\n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "\n",
    "            # Filtra cidade A e range de dias\n",
    "            city_mask = pc.equal(table.column(\"city\"), \"A\")\n",
    "            day_mask = pc.and_(\n",
    "                pc.greater_equal(table.column(\"d\"), self.day_range[0]),\n",
    "                pc.less_equal(table.column(\"d\"), self.day_range[1])\n",
    "            )\n",
    "            mask = pc.and_(city_mask, day_mask)\n",
    "            \n",
    "            table = table.filter(mask)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            # Converte e normaliza POIs\n",
    "            df = table.to_pandas()\n",
    "            if not self.check_data_sanity(df):\n",
    "                continue  # Pula apenas casos graves\n",
    "                \n",
    "            # üÜï Corre√ß√£o autom√°tica de problemas menores\n",
    "            df = self.auto_fix_data(df)\n",
    "\n",
    "            # poi_cols = [col for col in df.columns if 'POI' in col or col == 'POI']\n",
    "            # if poi_cols:\n",
    "            #     for col in poi_cols:\n",
    "            #         if col in df.columns:\n",
    "            #             def robust_poi_normalize(x):\n",
    "            #                 if x is None:\n",
    "            #                     return np.zeros(85, dtype=np.float32)\n",
    "                            \n",
    "            #                 # Converte e limpa AGRESSIVAMENTE\n",
    "            #                 arr = np.asarray(x, dtype=np.float32)\n",
    "            #                 arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            #                 arr = np.clip(arr, 0.0, None)\n",
    "                            \n",
    "            #                 # üÜï NORMALIZA√á√ÉO MAIS SEGURA:\n",
    "            #                 arr = np.log1p(arr)\n",
    "                            \n",
    "            #                 # üÜï EVITA VALORES GIGANTES:\n",
    "            #                 chmax = arr.max(axis=0, keepdims=True)\n",
    "            #                 chmax[chmax == 0] = 1.0\n",
    "            #                 arr = arr / chmax\n",
    "                            \n",
    "            #                 # üÜï VERIFICA√á√ÉO FINAL:\n",
    "            #                 if not np.isfinite(arr).all():\n",
    "            #                     print(f\"‚ö†Ô∏è  POI ainda cont√©m problemas ap√≥s limpeza!\")\n",
    "            #                     arr = np.zeros(85, dtype=np.float32)\n",
    "                            \n",
    "            #                 return arr\n",
    "                        \n",
    "            #             df[col] = df[col].apply(robust_poi_normalize)\n",
    "            \n",
    "            df = df.sort_values(['uid', 'd', 't'])\n",
    "            \n",
    "            # Processa cada usu√°rio com amostragem estratificada\n",
    "            for uid, user_group in df.groupby('uid'):\n",
    "                if len(user_group) < self.sequence_length + self.prediction_steps:\n",
    "                    continue\n",
    "                \n",
    "                sequences = self._sample_user_sequences(\n",
    "                    user_group, \n",
    "                    self.max_sequences_per_user\n",
    "                )\n",
    "                \n",
    "                for seq in sequences:\n",
    "                    yield (\n",
    "                        torch.tensor(seq['uid'], dtype=torch.long),\n",
    "                        torch.tensor(seq['d'], dtype=torch.long),\n",
    "                        torch.tensor(seq['t'], dtype=torch.long),\n",
    "                        torch.tensor(seq['city_idx'], dtype=torch.long),\n",
    "                        torch.from_numpy(seq['poi']),\n",
    "                        torch.from_numpy(seq['coords_seq']),\n",
    "                        torch.from_numpy(seq['target_coords'])\n",
    "                    )\n",
    "\n",
    "\n",
    "def create_pretrain_loaders(\n",
    "    parquet_path: str,\n",
    "    batch_size: int = 32,\n",
    "    sequence_length: int = 48,\n",
    "    max_sequences_per_user: int = 10,   #50\n",
    "    num_workers: int = 0    #2\n",
    "):\n",
    "    \"\"\"Cria loaders de treino e valida√ß√£o para cidade A.\"\"\"\n",
    "    \n",
    "    train_ds = CityAPretrainDataset(\n",
    "        parquet_path=parquet_path,\n",
    "        mode=\"train\",\n",
    "        sequence_length=sequence_length,\n",
    "        max_sequences_per_user=max_sequences_per_user\n",
    "    )\n",
    "    \n",
    "    val_ds = CityAPretrainDataset(\n",
    "        parquet_path=parquet_path,\n",
    "        mode=\"val\", \n",
    "        sequence_length=sequence_length,\n",
    "        max_sequences_per_user=max_sequences_per_user // 2\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=batch_size, \n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False  #‚¨ÖÔ∏è Alterado para False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=False  #‚¨ÖÔ∏è Alterado para False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7358b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetCityDDataset(IterableDataset):\n",
    "    \"\"\"Dataset para avalia√ß√£o em cidade D.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        parquet_path: str,\n",
    "        city_list: list[str],\n",
    "        chunk_size: int = 10_000,\n",
    "        sequence_length: int = 48,\n",
    "        prediction_steps: int = 1\n",
    "    ):\n",
    "        self.parquet_path = parquet_path\n",
    "        self.city_list = city_list\n",
    "        self.city_set = set(city_list)\n",
    "        self.city_to_idx = {c: i for i, c in enumerate(city_list)}\n",
    "        self.chunk_size = chunk_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "\n",
    "    def _build_sequences_for_user(self, user_data):\n",
    "        \"\"\"Constr√≥i sequ√™ncias temporais para um usu√°rio espec√≠fico.\"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(len(user_data) - self.sequence_length - self.prediction_steps + 1):\n",
    "            seq_start = i\n",
    "            seq_end = i + self.sequence_length\n",
    "            target_start = seq_end\n",
    "            target_end = target_start + self.prediction_steps\n",
    "            \n",
    "            coords_seq = user_data.iloc[seq_start:seq_end][['x_norm', 'y_norm']].values  # [0,1]\n",
    "            current_info = user_data.iloc[seq_end - 1]\n",
    "            target_coords = user_data.iloc[target_start:target_end][['x_norm', 'y_norm']].values  # [0,1]\n",
    "            \n",
    "            # sequences.append({\n",
    "            #     'uid': current_info['uid'],\n",
    "            #     'd': current_info['d'], \n",
    "            #     't': current_info['t'],\n",
    "            #     'city_idx': self.city_to_idx[current_info['city']],\n",
    "            #     'poi': current_info['POI'],\n",
    "            #     'coords_seq': coords_seq.astype(np.float32),\n",
    "            #     'target_coords': target_coords.astype(np.float32)\n",
    "            # })\n",
    "            sequences.append({\n",
    "                'uid': current_info['uid'],\n",
    "                'd_norm': current_info['d_norm'],  # üîÑ Tempo normalizado\n",
    "                't_sin': current_info['t_sin'],    # üîÑ Timeslot circular (sin)\n",
    "                't_cos': current_info['t_cos'],    # üîÑ Timeslot circular (cos)\n",
    "                'city_idx': current_info['city_encoded'],  # üîÑ Cidade encoded\n",
    "                'poi_norm': current_info['POI_norm'],      # üîÑ POI normalizado\n",
    "                'coords_seq': coords_seq.astype(np.float32),     # ‚úÖ J√° normalizado\n",
    "                'target_coords': target_coords.astype(np.float32) # ‚úÖ J√° normalizado\n",
    "            })\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        pf = pq.ParquetFile(self.parquet_path)\n",
    "        \n",
    "        for batch in pf.iter_batches(batch_size=self.chunk_size):\n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "\n",
    "            # Filtra cidades\n",
    "            mask = pc.is_in(table.column(\"city\"), pa.array(list(self.city_set)))\n",
    "            table = table.filter(mask)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            # Converte para pandas\n",
    "            df = table.to_pandas()\n",
    "            \n",
    "\n",
    "            # Normaliza POIs (VERS√ÉO CORRIGIDA)\n",
    "            poi_cols = [col for col in df.columns if 'POI' in col or col == 'POI']\n",
    "            if poi_cols:\n",
    "                for col in poi_cols:\n",
    "                    if col in df.columns:\n",
    "                        def robust_poi_normalize(x):\n",
    "                            if x is None:\n",
    "                                return np.zeros(85, dtype=np.float32)\n",
    "                            \n",
    "                            # Converte para array e limpa valores problem√°ticos\n",
    "                            arr = np.asarray(x, dtype=np.float32)\n",
    "                            arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)  # üÜï Limpa NaN/Inf\n",
    "                            arr = np.clip(arr, 0.0, None)  # üÜï Remove negativos\n",
    "                            \n",
    "                            # Log1p mais seguro\n",
    "                            return np.log1p(arr)\n",
    "                        \n",
    "                        df[col] = df[col].apply(robust_poi_normalize)\n",
    "            \n",
    "            df = df.sort_values(['uid', 'd', 't'])\n",
    "            \n",
    "            for uid, user_group in df.groupby('uid'):\n",
    "                if len(user_group) < self.sequence_length + self.prediction_steps:\n",
    "                    continue\n",
    "                    \n",
    "                sequences = self._build_sequences_for_user(user_group)\n",
    "                \n",
    "                for seq in sequences:\n",
    "                    yield (\n",
    "                        torch.tensor(seq['uid'], dtype=torch.long),\n",
    "                        torch.tensor(seq['d'], dtype=torch.long),\n",
    "                        torch.tensor(seq['t'], dtype=torch.long),\n",
    "                        torch.tensor(seq['city_idx'], dtype=torch.long),\n",
    "                        torch.from_numpy(seq['poi']),\n",
    "                        torch.from_numpy(seq['coords_seq']),\n",
    "                        torch.from_numpy(seq['target_coords'])\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3bbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Funde dois vetores de mesmo tamanho por uma soma ponderada aprend√≠vel.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 20, init_w_r: float = 0.5, init_w_e: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.w_r = nn.Parameter(torch.tensor(init_w_r, dtype=torch.float32))\n",
    "        self.w_e = nn.Parameter(torch.tensor(init_w_e, dtype=torch.float32))\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, static_red: torch.Tensor, dyn_emb: torch.Tensor) -> torch.Tensor:\n",
    "        assert static_red.shape == dyn_emb.shape and static_red.size(1) == self.dim\n",
    "        fused = self.w_r * static_red + self.w_e * dyn_emb\n",
    "        return fused\n",
    "\n",
    "\n",
    "class MLP500(nn.Module):\n",
    "    \"\"\"MLP simples com 1 hidden layer de 500 ReLUs.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, n_clusters: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, n_clusters)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class MLPSmall(nn.Module):\n",
    "    \"\"\"MLP menor e mais est√°vel\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, n_clusters: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.1)  # Regulariza√ß√£o\n",
    "        self.fc2 = nn.Linear(hidden_dim, n_clusters)\n",
    "        \n",
    "        # Inicializa√ß√£o Xavier (mais est√°vel)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class DestinationHead(nn.Module):\n",
    "    \"\"\"Combina MLP + softmax + weighted sum pelos cluster centers.\"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, cluster_centers: torch.Tensor):\n",
    "        super().__init__()\n",
    "        C, coord_dim = cluster_centers.shape\n",
    "        assert coord_dim == 2\n",
    "        # self.mlp500 = MLP500(in_dim, hidden_dim, C)\n",
    "        self.mlp = MLPSmall(in_dim, hidden_dim, C)  # ‚¨ÖÔ∏è Usa MLP menor\n",
    "        self.register_buffer(\"centers\", cluster_centers)\n",
    "\n",
    "    def forward(self, fused: torch.Tensor) -> torch.Tensor:\n",
    "        # logits = self.mlp500(fused)\n",
    "        logits = self.mlp(fused)\n",
    "        P = F.softmax(logits, dim=1)\n",
    "        coords = P @ self.centers\n",
    "        return coords\n",
    "\n",
    "\n",
    "def discretize_coordinates(coords_pred: torch.Tensor, grid_size: int = 200):\n",
    "    \"\"\"Converte coordenadas cont√≠nuas para grid discreto [0, grid_size-1].\"\"\"\n",
    "    coords_discrete = torch.round(coords_pred).long()\n",
    "    coords_discrete = torch.clamp(coords_discrete, 0, grid_size - 1)\n",
    "    return coords_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b997e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuMobModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo completo que combina todas as partes e faz rollout para m√∫ltiplos passos.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users: int,\n",
    "        n_days: int,\n",
    "        n_slots: int,\n",
    "        n_cities: int,\n",
    "        cluster_centers: torch.Tensor,\n",
    "        emb_dim: int = 4,      #10\n",
    "        poi_in_dim: int = 85,\n",
    "        poi_out_dim: int = 4,  #10\n",
    "        lstm_hidden: int = 4,  #10\n",
    "        fusion_dim: int = 8,  #20\n",
    "        sequence_length: int = 48,\n",
    "        prediction_steps: int = 1,\n",
    "        # disable_poi_debug=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Salva configura√ß√µes\n",
    "        # self.disable_poi_debug = disable_poi_debug\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "        \n",
    "        # Componentes\n",
    "        self.fusion = ExternalInformationFusionDTPC(\n",
    "            n_users=n_users,\n",
    "            n_days=n_days,\n",
    "            n_slots=n_slots,\n",
    "            n_cities=n_cities,\n",
    "            emb_dim=emb_dim,\n",
    "            poi_in_dim=poi_in_dim,\n",
    "            poi_out_dim=poi_out_dim,\n",
    "            # disable_poi_debug=disable_poi_debug\n",
    "        )\n",
    "        self.dense = ExternalInformationDense(\n",
    "            in_dim=self.fusion.out_dim, \n",
    "            out_dim=fusion_dim\n",
    "        )\n",
    "        self.lstm = CoordLSTM(\n",
    "            input_size=2, \n",
    "            hidden_size=lstm_hidden, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.weighted_fusion = WeightedFusion(dim=fusion_dim)\n",
    "        self.destination_head = DestinationHead(\n",
    "            in_dim=fusion_dim,\n",
    "            hidden_dim=500,\n",
    "            cluster_centers=cluster_centers\n",
    "        )\n",
    "        self._stable_init()  # Inicializa√ß√£o mais est√°vel\n",
    "        \n",
    "    def forward_single_step(self, uid, d, t, city, poi, coords_seq):\n",
    "        \"\"\"Faz uma predi√ß√£o para um √∫nico passo.\"\"\"\n",
    "        # Informa√ß√£o est√°tica (contexto)\n",
    "        static_emb = self.fusion(uid, d, t, city, poi)\n",
    "        static_red = self.dense(static_emb)\n",
    "        \n",
    "        # Informa√ß√£o din√¢mica (padr√£o de movimento)\n",
    "        dyn_emb = self.lstm(coords_seq)\n",
    "        \n",
    "        # Fus√£o inteligente\n",
    "        fused = self.weighted_fusion(static_red, dyn_emb)\n",
    "        \n",
    "        # Predi√ß√£o final\n",
    "        pred_coords = self.destination_head(fused)\n",
    "        \n",
    "        return pred_coords\n",
    "    \n",
    "    def _stable_init(self):\n",
    "        \"\"\"Inicializa√ß√£o mais est√°vel\"\"\"\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Embedding):\n",
    "                torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)  # Bem menor que padr√£o\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "    \n",
    "    def rollout_predictions(\n",
    "        self, \n",
    "        uid, d, t, city, poi, coords_seq, \n",
    "        n_steps: int,\n",
    "        use_predictions: bool = True\n",
    "    ):\n",
    "        \"\"\"Faz predi√ß√µes para m√∫ltiplos passos futuros.\"\"\"\n",
    "        predictions = []\n",
    "        current_seq = coords_seq.clone()\n",
    "        current_t = t.clone()\n",
    "        current_d = d.clone()\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Prediz pr√≥ximo passo\n",
    "            pred = self.forward_single_step(uid, current_d, current_t, city, poi, current_seq)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            if use_predictions and step < n_steps - 1:\n",
    "                # Atualiza sequ√™ncia: remove primeiro ponto, adiciona predi√ß√£o\n",
    "                new_point = pred.unsqueeze(1)  # (batch, 1, 2)\n",
    "                current_seq = torch.cat([current_seq[:, 1:, :], new_point], dim=1)\n",
    "            \n",
    "            # Incrementa tempo corretamente (wrap em 48 slots)\n",
    "            current_t = (current_t + 1) % 48\n",
    "            # Se voltou para slot 0, incrementa dia\n",
    "            mask_new_day = (current_t == 0)\n",
    "            current_d = current_d + mask_new_day.long()\n",
    "            \n",
    "        return torch.stack(predictions, dim=1)  # (batch, n_steps, 2)\n",
    "    \n",
    "    def forward(self, uid, d, t, city, poi, coords_seq, n_steps=1):\n",
    "        \"\"\"Forward principal - pode ser usado tanto para treino quanto infer√™ncia.\"\"\"\n",
    "        if n_steps == 1:\n",
    "            return self.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "        else:\n",
    "            return self.rollout_predictions(uid, d, t, city, poi, coords_seq, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb9adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_on_city_A(\n",
    "    parquet_path: str,\n",
    "    centers: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    n_epochs: int = 6,\n",
    "    learning_rate: float = 2e-3,\n",
    "    batch_size: int = 32,\n",
    "    sequence_length: int = 48,\n",
    "    save_path: str = \"ckpt_A_warmup.pt\"\n",
    "):\n",
    "    \"\"\"Pr√©-treina o modelo na cidade A para validar o pipeline.\"\"\"\n",
    "    print(\"üèãÔ∏è Iniciando pr√©-treino na cidade A...\")\n",
    "    \n",
    "    # 1. Cria loaders\n",
    "    train_loader, val_loader = create_pretrain_loaders(\n",
    "        parquet_path=parquet_path,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # 2. Instancia modelo\n",
    "    model = HuMobModel(\n",
    "        n_users=100_000,  # cidade A\n",
    "        n_days=75,\n",
    "        n_slots=48,\n",
    "        n_cities=4,\n",
    "        cluster_centers=centers,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_steps=1,  # Next-step apenas\n",
    "        disable_poi_debug=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # 3. Setup de treino (MSE apenas para come√ßar)\n",
    "    # 3. Setup de treino (hiperpar√¢metros mais seguros)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=1e-4)  # üîÑ AdamW + LR menor\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    def grad_norm(model):\n",
    "        \"\"\"Calcula norma total dos gradientes\"\"\"\n",
    "        total = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total += p.grad.detach().float().norm(2).item() ** 2\n",
    "        return (total ** 0.5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Par√¢metros trein√°veis: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # === TREINO ===\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        train_count = 0\n",
    "        \n",
    "        print(f\"\\nüîÑ √âpoca {epoch+1}/{n_epochs}\")\n",
    "        train_pbar = tqdm(train_loader, desc=f'Treino A')\n",
    "        \n",
    "        for batch in train_pbar:\n",
    "            uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "            \n",
    "            # üÜï Verifica√ß√£o de dados de entrada\n",
    "            if not torch.isfinite(coords_seq).all() or not torch.isfinite(target_coords).all():\n",
    "                print(\"‚ö†Ô∏è Dados de entrada cont√™m NaN/Inf, pulando batch\")\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            # Forward\n",
    "            pred = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "            target = target_coords.squeeze(1)\n",
    "            \n",
    "            # üÜï Verifica√ß√£o de predi√ß√£o\n",
    "            if not torch.isfinite(pred).all():\n",
    "                print(\"‚ö†Ô∏è Predi√ß√£o cont√©m NaN/Inf, pulando batch\")\n",
    "                continue\n",
    "            \n",
    "            # Loss\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            # üÜï Verifica√ß√£o de loss\n",
    "            if not torch.isfinite(loss):\n",
    "                print(f\"‚ö†Ô∏è Loss={loss.item()} n√£o finito, pulando batch\")\n",
    "                continue\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "\n",
    "            pre_clip_norm = grad_norm(model)  # antes do clip\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # üîÑ Era 0.3 ‚Üí 0.1 (mais agressivo)\n",
    "            post_clip_norm = grad_norm(model)  # depois do clip\n",
    "\n",
    "            # üÜï LOG DETALHADO:\n",
    "            if pre_clip_norm > 1000:\n",
    "                print(f\"‚ö†Ô∏è  GRADIENTE EXPLODINDO: pre-clip={pre_clip_norm:.1f} ‚Üí post-clip={post_clip_norm:.2f}\")\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Logs melhorados\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'PreClip': f'{pre_clip_norm:.1f}',  # üÜï \n",
    "                'PostClip': f'{post_clip_norm:.1f}',  # üÜï\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "        \n",
    "        # === VALIDA√á√ÉO ===\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        val_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc='Val A')\n",
    "            for batch in val_pbar:\n",
    "                uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "                \n",
    "                # pred = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "                target = target_coords.squeeze(1)\n",
    "                \n",
    "                loss = criterion(pred, target)\n",
    "                \n",
    "                val_loss_epoch += loss.item() * target.size(0)\n",
    "                val_count += target.size(0)\n",
    "                \n",
    "                val_pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # M√©dias\n",
    "        avg_train_loss = train_loss_epoch / max(train_count, 1)\n",
    "        avg_val_loss = val_loss_epoch / max(val_count, 1)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Treino: {avg_train_loss:.4f} | Val: {avg_val_loss:.4f}\")\n",
    "        print(f\"Fusion weights: w_r={model.weighted_fusion.w_r.item():.3f}, w_e={model.weighted_fusion.w_e.item():.3f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Salva melhor modelo\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"üíæ Novo melhor modelo! Loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            torch.save({\n",
    "                'state_dict': model.state_dict(),\n",
    "                'centers': centers.cpu().numpy(),\n",
    "                'config': {\n",
    "                    'sequence_length': sequence_length,\n",
    "                    'prediction_steps': 1,\n",
    "                    'n_clusters': centers.shape[0]\n",
    "                },\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'epoch': epoch\n",
    "            }, save_path)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Pr√©-treino conclu√≠do! Modelo salvo em: {save_path}\")\n",
    "    print(f\"Melhor loss de valida√ß√£o: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def evaluate_zero_shot_on_D(\n",
    "    parquet_path: str,\n",
    "    checkpoint_path: str,\n",
    "    device: torch.device,\n",
    "    n_samples: int = 5000,\n",
    "    sequence_length: int = 48\n",
    "):\n",
    "    \"\"\"Avalia o modelo pr√©-treinado em A na cidade D (zero-shot).\"\"\"\n",
    "    print(\"üéØ Avalia√ß√£o zero-shot em cidade D...\")\n",
    "    \n",
    "    # 1. Carrega checkpoint\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "    centers = torch.from_numpy(ckpt['centers']).to(device)\n",
    "    \n",
    "    # 2. Instancia modelo\n",
    "    model = HuMobModel(\n",
    "        n_users=6_000,  # cidade D\n",
    "        n_days=75,\n",
    "        n_slots=48,\n",
    "        n_cities=4,\n",
    "        cluster_centers=centers,\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_steps=1\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 3. Cria dataset de avalia√ß√£o em D\n",
    "    eval_ds = ParquetCityDDataset(\n",
    "        parquet_path=parquet_path,\n",
    "        city_list=[\"D\"],\n",
    "        sequence_length=sequence_length,\n",
    "        prediction_steps=1,\n",
    "        chunk_size=2500\n",
    "    )\n",
    "    \n",
    "    # 4. Avalia em amostra\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    coords_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        eval_loader = DataLoader(eval_ds, batch_size=32, num_workers=2)\n",
    "        pbar = tqdm(eval_loader, desc='Eval D')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            if total_samples >= n_samples:\n",
    "                break\n",
    "                \n",
    "            uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Filtra apenas dados ‚â§ dia 60 (observados)\n",
    "            mask = d <= 60\n",
    "            if not mask.any():\n",
    "                continue\n",
    "                \n",
    "            uid, d, t, city, poi = uid[mask], d[mask], t[mask], city[mask], poi[mask]\n",
    "            coords_seq, target_coords = coords_seq[mask], target_coords[mask]\n",
    "            \n",
    "            pred = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "            target = target_coords.squeeze(1)\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "            total_samples += target.size(0)\n",
    "            \n",
    "            # Calcula erro em c√©lulas (ap√≥s discretiza√ß√£o)\n",
    "            pred_discrete = discretize_coordinates(pred)\n",
    "            target_discrete = discretize_coordinates(target)\n",
    "            cell_error = torch.abs(pred_discrete - target_discrete).float().mean(dim=1)\n",
    "            coords_errors.extend(cell_error.cpu().tolist())\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'MSE': f'{loss.item():.4f}',\n",
    "                'Samples': total_samples\n",
    "            })\n",
    "    \n",
    "    avg_mse = total_loss / max(total_samples, 1)\n",
    "    avg_cell_error = np.mean(coords_errors)\n",
    "    \n",
    "    print(f\"\\nüìä Resultados zero-shot em D:\")\n",
    "    print(f\"  MSE: {avg_mse:.4f}\")\n",
    "    print(f\"  Erro m√©dio em c√©lulas: {avg_cell_error:.2f}\")\n",
    "    print(f\"  Amostras avaliadas: {total_samples:,}\")\n",
    "    \n",
    "    return avg_mse, avg_cell_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb2e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretrain_pipeline(\n",
    "    parquet_path: str = \"humob_all_cities_dpsk.parquet\",\n",
    "    device: torch.device = None,\n",
    "    n_clusters: int = 1024,\n",
    "    pretrain_epochs: int = 6,\n",
    "    sequence_length: int = 48,\n",
    "    batch_size: int = 32,\n",
    "    learning_rate: float = 2e-3\n",
    "):\n",
    "    \"\"\"Pipeline completo de pr√©-treino.\"\"\"\n",
    "    \n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(\"üöÄ Iniciando pipeline de pr√©-treino HuMob\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Clusters: {n_clusters}\")\n",
    "    print(f\"Epochs: {pretrain_epochs}\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 1. CENTROS EST√ÅVEIS\n",
    "    print(\"\\nüìç ETAPA 1: Calculando centros est√°veis...\")\n",
    "    centers = load_or_compute_centers(\n",
    "        parquet_path=parquet_path,\n",
    "        cities=[\"A\"],\n",
    "        n_clusters=n_clusters,\n",
    "        save_path=f\"centers_A_{n_clusters}.npy\"\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"‚úÖ Centros prontos: {centers.shape}\")\n",
    "    \n",
    "    # 2. PR√â-TREINO EM A\n",
    "    print(\"\\nüèãÔ∏è ETAPA 2: Pr√©-treino na cidade A...\")\n",
    "    model, train_losses, val_losses = pretrain_on_city_A(\n",
    "        parquet_path=parquet_path,\n",
    "        centers=centers,\n",
    "        device=device,\n",
    "        n_epochs=pretrain_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        sequence_length=sequence_length,\n",
    "        save_path=\"ckpt_A_warmup.pt\"\n",
    "    )\n",
    "    \n",
    "    # Plot das curvas\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Val')\n",
    "    plt.xlabel('√âpoca')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Pr√©-treino em A')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot([model.weighted_fusion.w_r.item()], [model.weighted_fusion.w_e.item()], 'ro', markersize=10)\n",
    "    plt.xlabel('w_r (est√°tico)')\n",
    "    plt.ylabel('w_e (din√¢mico)')\n",
    "    plt.title('Pesos da Fus√£o')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pretrain_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. AVALIA√á√ÉO ZERO-SHOT EM D\n",
    "    print(\"\\nüéØ ETAPA 3: Avalia√ß√£o zero-shot na cidade D...\")\n",
    "    mse_d, cell_error_d = evaluate_zero_shot_on_D(\n",
    "        parquet_path=parquet_path,\n",
    "        checkpoint_path=\"ckpt_A_warmup.pt\",\n",
    "        device=device,\n",
    "        n_samples=5000,\n",
    "        sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # 4. RELAT√ìRIO FINAL\n",
    "    print(\"\\nüìä RELAT√ìRIO FINAL\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"‚úÖ Pr√©-treino conclu√≠do em {pretrain_epochs} √©pocas\")\n",
    "    print(f\"üìà Loss final de treino: {train_losses[-1]:.4f}\")\n",
    "    print(f\"üìâ Loss final de valida√ß√£o: {val_losses[-1]:.4f}\")\n",
    "    print(f\"üéØ MSE zero-shot em D: {mse_d:.4f}\")\n",
    "    print(f\"üìç Erro m√©dio em c√©lulas: {cell_error_d:.2f}\")\n",
    "    print(f\"‚öñÔ∏è Pesos da fus√£o: w_r={model.weighted_fusion.w_r.item():.3f}, w_e={model.weighted_fusion.w_e.item():.3f}\")\n",
    "    print(f\"üíæ Checkpoint salvo: ckpt_A_warmup.pt\")\n",
    "    print(f\"üé® Gr√°ficos salvos: pretrain_results.png\")\n",
    "    \n",
    "    # Verifica se est√° tudo OK\n",
    "    print(\"\\nüîç VERIFICA√á√ïES:\")\n",
    "    converged = val_losses[-1] < val_losses[0] * 0.8\n",
    "    reasonable_mse = mse_d < 1000\n",
    "    reasonable_error = cell_error_d < 20\n",
    "    \n",
    "    print(f\"  Convergiu? {'‚úÖ' if converged else '‚ùå'} (loss diminuiu 20%+)\")\n",
    "    print(f\"  MSE razo√°vel? {'‚úÖ' if reasonable_mse else '‚ùå'} (< 1000)\")\n",
    "    print(f\"  Erro de c√©lulas OK? {'‚úÖ' if reasonable_error else '‚ùå'} (< 20 c√©lulas)\")\n",
    "    \n",
    "    all_good = converged and reasonable_mse and reasonable_error\n",
    "    \n",
    "    if all_good:\n",
    "        print(\"\\nüéâ PIPELINE EXECUTADO COM SUCESSO!\")\n",
    "        print(\"   Pronto para pr√≥ximas etapas:\")\n",
    "        print(\"   - Fine-tune em D (opcional)\")\n",
    "        print(\"   - Rollout para submiss√£o (dias 61-75)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è ALGUNS PROBLEMAS DETECTADOS\")\n",
    "        print(\"   Recomenda√ß√µes:\")\n",
    "        if not converged:\n",
    "            print(\"   - Aumentar n√∫mero de √©pocas ou ajustar LR\")\n",
    "        if not reasonable_mse:\n",
    "            print(\"   - Verificar normaliza√ß√£o dos dados\")\n",
    "        if not reasonable_error:\n",
    "            print(\"   - Revisar c√°lculo de centros ou head\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'centers': centers,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'mse_zero_shot': mse_d,\n",
    "        'cell_error': cell_error_d,\n",
    "        'checkpoint_path': 'ckpt_A_warmup.pt'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26036ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Executando estrat√©gia de pr√©-treino sugerida...\n",
      "Device: cpu\n",
      "Estrat√©gia: A (pr√©-treino) ‚Üí D (zero-shot) ‚Üí D (fine-tune opcional)\n",
      "============================================================\n",
      "üöÄ Iniciando pipeline de pr√©-treino HuMob\n",
      "Device: cpu\n",
      "Clusters: 64\n",
      "Epochs: 1\n",
      "Sequence length: 6\n",
      "==================================================\n",
      "\n",
      "üìç ETAPA 1: Calculando centros est√°veis...\n",
      "üìÇ Carregando centros existentes: centers_A_64.npy\n",
      "‚úÖ Centros prontos: torch.Size([64, 2])\n",
      "\n",
      "üèãÔ∏è ETAPA 2: Pr√©-treino na cidade A...\n",
      "üèãÔ∏è Iniciando pr√©-treino na cidade A...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HuMobModel.__init__() got an unexpected keyword argument 'disable_poi_debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# # EXECUTA O PIPELINE PRINCIPAL\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# results = run_pretrain_pipeline(\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     parquet_path=parquet_file,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#     learning_rate=2e-3\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m results = \u001b[43mrun_pretrain_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparquet_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# ‚¨áÔ∏è Era 1024 ‚Üí 256 (4x menos centros)\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# ‚¨áÔ∏è Era 6 ‚Üí 2 (3x menos √©pocas)  \u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# ‚¨áÔ∏è Era 48 ‚Üí 12 (4x menos hist√≥rico)\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m     \u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Verifica se o pr√©-treino foi bem-sucedido\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results[\u001b[33m'\u001b[39m\u001b[33mmse_zero_shot\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m1000\u001b[39m:  \u001b[38;5;66;03m# Limite razo√°vel\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mrun_pretrain_pipeline\u001b[39m\u001b[34m(parquet_path, device, n_clusters, pretrain_epochs, sequence_length, batch_size, learning_rate)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 2. PR√â-TREINO EM A\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müèãÔ∏è ETAPA 2: Pr√©-treino na cidade A...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m model, train_losses, val_losses = \u001b[43mpretrain_on_city_A\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparquet_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mckpt_A_warmup.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     44\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Plot das curvas\u001b[39;00m\n\u001b[32m     47\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mpretrain_on_city_A\u001b[39m\u001b[34m(parquet_path, centers, device, n_epochs, learning_rate, batch_size, sequence_length, save_path)\u001b[39m\n\u001b[32m     15\u001b[39m train_loader, val_loader = create_pretrain_loaders(\n\u001b[32m     16\u001b[39m     parquet_path=parquet_path,\n\u001b[32m     17\u001b[39m     batch_size=batch_size,\n\u001b[32m     18\u001b[39m     sequence_length=sequence_length\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 2. Instancia modelo\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m model = \u001b[43mHuMobModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_users\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# cidade A\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_days\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_slots\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m48\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_cities\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcluster_centers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Next-step apenas\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_poi_debug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 3. Setup de treino (MSE apenas para come√ßar)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# 3. Setup de treino (hiperpar√¢metros mais seguros)\u001b[39;00m\n\u001b[32m     35\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(\u001b[32m0.9\u001b[39m, \u001b[32m0.95\u001b[39m), weight_decay=\u001b[32m1e-4\u001b[39m)  \u001b[38;5;66;03m# üîÑ AdamW + LR menor\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: HuMobModel.__init__() got an unexpected keyword argument 'disable_poi_debug'"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# üöÄ EXECU√á√ÉO PRINCIPAL - ESTRAT√âGIA DE PR√â-TREINO\n",
    "# ===================================================\n",
    "\n",
    "print(\"üéØ Executando estrat√©gia de pr√©-treino sugerida...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Estrat√©gia: A (pr√©-treino) ‚Üí D (zero-shot) ‚Üí D (fine-tune opcional)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# # EXECUTA O PIPELINE PRINCIPAL\n",
    "# results = run_pretrain_pipeline(\n",
    "#     parquet_path=parquet_file,\n",
    "#     device=device,\n",
    "#     n_clusters=1024,        # Est√°vel e r√°pido\n",
    "#     pretrain_epochs=6,      # R√°pido para validar\n",
    "#     sequence_length=48,     # 1 dia de hist√≥rico\n",
    "#     batch_size=32,\n",
    "#     learning_rate=2e-3\n",
    "# )\n",
    "\n",
    "results = run_pretrain_pipeline(\n",
    "    parquet_path=parquet_file,\n",
    "    device=device,\n",
    "    n_clusters=64,         # ‚¨áÔ∏è Era 1024 ‚Üí 256 (4x menos centros)\n",
    "    pretrain_epochs=1,      # ‚¨áÔ∏è Era 6 ‚Üí 2 (3x menos √©pocas)  \n",
    "    sequence_length=6,     # ‚¨áÔ∏è Era 48 ‚Üí 12 (4x menos hist√≥rico)\n",
    "    batch_size=34,         \n",
    "    learning_rate=1e-3     \n",
    ")\n",
    "\n",
    "# Verifica se o pr√©-treino foi bem-sucedido\n",
    "if results['mse_zero_shot'] < 1000:  # Limite razo√°vel\n",
    "    print(\"\\n‚úÖ Pr√©-treino bem-sucedido! Modelo pronto para uso.\")\n",
    "    print(\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "    print(f\"   1. Modelo final salvo em: {results['checkpoint_path']}\")\n",
    "    print(\"   2. Para submiss√£o HuMob, use create_submission_data()\")\n",
    "    print(\"   3. Para rollout completo (15 dias), ajuste n_steps=48*15\")\n",
    "    print(\"   4. Lembre-se de discretizar coordenadas [0,199]\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Pr√©-treino n√£o convergiu bem.\")\n",
    "    print(\"üí° Sugest√µes:\")\n",
    "    print(\"   - Aumentar n√∫mero de √©pocas\")\n",
    "    print(\"   - Ajustar learning rate\")\n",
    "    print(\"   - Verificar normaliza√ß√£o dos POIs\")\n",
    "    print(\"   - Revisar sequ√™ncia temporal do dataset\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÅ Pipeline executado com sucesso!\")\n",
    "print(\"üìÅ Arquivos gerados:\")\n",
    "print(\"   - centers_A_1024.npy (centros est√°veis)\")\n",
    "print(\"   - ckpt_A_warmup.pt (modelo pr√©-treinado)\")\n",
    "print(\"   - pretrain_results.png (gr√°ficos)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

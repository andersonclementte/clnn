{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "import hdbscan\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seus módulos locais\n",
    "from external_information import ExternalInformationFusionDTPC, ExternalInformationDense\n",
    "from partial_information import CoordLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d424ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParquetCityDDataset(IterableDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        parquet_path: str,\n",
    "        city_list: list[str],\n",
    "        chunk_size: int = 10_000,\n",
    "        sequence_length: int = 24,  # NOVO: janela temporal\n",
    "        prediction_steps: int = 1    # NOVO: quantos passos predizer\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset que constrói sequências temporais para cada usuário.\n",
    "        \n",
    "        sequence_length: quantos timesteps passados usar (ex: 24 slots)\n",
    "        prediction_steps: quantos timesteps futuros predizer (ex: 1)\n",
    "        \"\"\"\n",
    "        self.parquet_path = parquet_path\n",
    "        self.city_list = city_list\n",
    "        self.city_set = set(city_list)\n",
    "        self.city_to_idx = {c: i for i, c in enumerate(city_list)}\n",
    "        self.chunk_size = chunk_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.prediction_steps = prediction_steps\n",
    "\n",
    "    def _build_sequences_for_user(self, user_data):\n",
    "        \"\"\"\n",
    "        Constrói sequências temporais para um usuário específico.\n",
    "        user_data: DataFrame com colunas [uid, d, t, city, POI, x, y] ordenado por (d,t)\n",
    "        \"\"\"\n",
    "        sequences = []\n",
    "        \n",
    "        for i in range(len(user_data) - self.sequence_length - self.prediction_steps + 1):\n",
    "            # Janela de entrada (histórico)\n",
    "            seq_start = i\n",
    "            seq_end = i + self.sequence_length\n",
    "            \n",
    "            # Próximos passos (alvos)\n",
    "            target_start = seq_end\n",
    "            target_end = target_start + self.prediction_steps\n",
    "            \n",
    "            # Extrai sequência de coordenadas (input para LSTM)\n",
    "            coords_seq = user_data.iloc[seq_start:seq_end][['x', 'y']].values\n",
    "            \n",
    "            # Extrai informações do último ponto da sequência (contexto atual)\n",
    "            current_info = user_data.iloc[seq_end - 1]\n",
    "            \n",
    "            # Extrai coordenadas alvo\n",
    "            target_coords = user_data.iloc[target_start:target_end][['x', 'y']].values\n",
    "            \n",
    "            sequences.append({\n",
    "                'uid': current_info['uid'],\n",
    "                'd': current_info['d'], \n",
    "                't': current_info['t'],\n",
    "                'city_idx': self.city_to_idx[current_info['city']],\n",
    "                'poi': current_info['POI'],\n",
    "                'coords_seq': coords_seq.astype(np.float32),  # (sequence_length, 2)\n",
    "                'target_coords': target_coords.astype(np.float32)  # (prediction_steps, 2)\n",
    "            })\n",
    "            \n",
    "        return sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        pf = pq.ParquetFile(self.parquet_path)\n",
    "        \n",
    "        for batch in pf.iter_batches(batch_size=self.chunk_size):\n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "\n",
    "            # Filtra cidades\n",
    "            mask = pc.is_in(table.column(\"city\"), pa.array(list(self.city_set)))\n",
    "            table = table.filter(mask)\n",
    "            if table.num_rows == 0:\n",
    "                continue\n",
    "\n",
    "            # Converte para pandas para facilitar agrupamento\n",
    "            df = table.to_pandas()\n",
    "            \n",
    "            # Normaliza POIs (CORREÇÃO IMPORTANTE)\n",
    "            poi_cols = [col for col in df.columns if 'POI' in col or col == 'POI']\n",
    "            if poi_cols:\n",
    "                df[poi_cols] = np.log1p(df[poi_cols])  # log(1+x) para estabilizar\n",
    "            \n",
    "            # Agrupa por usuário e ordena por tempo\n",
    "            df = df.sort_values(['uid', 'd', 't'])\n",
    "            \n",
    "            for uid, user_group in df.groupby('uid'):\n",
    "                # Só processa usuários com dados suficientes\n",
    "                if len(user_group) < self.sequence_length + self.prediction_steps:\n",
    "                    continue\n",
    "                    \n",
    "                sequences = self._build_sequences_for_user(user_group)\n",
    "                \n",
    "                for seq in sequences:\n",
    "                    yield (\n",
    "                        torch.tensor(seq['uid'], dtype=torch.long),\n",
    "                        torch.tensor(seq['d'], dtype=torch.long),\n",
    "                        torch.tensor(seq['t'], dtype=torch.long),\n",
    "                        torch.tensor(seq['city_idx'], dtype=torch.long),\n",
    "                        torch.from_numpy(seq['poi']),\n",
    "                        torch.from_numpy(seq['coords_seq']),     # (seq_len, 2)\n",
    "                        torch.from_numpy(seq['target_coords'])   # (pred_steps, 2)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ca3150",
   "metadata": {},
   "source": [
    "## Fusão de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac01273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Funde dois vetores de mesmo tamanho (B, dim) por uma soma ponderada:\n",
    "        output = w_r * static_red + w_e * dyn_emb\n",
    "    onde w_r e w_e são parâmetros escalar aprendíveis.\n",
    "    A saída tem a mesma dimensão (dim) dos vetores de entrada.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 20, init_w_r: float = 0.5, init_w_e: float = 0.5):\n",
    "        super().__init__()\n",
    "        # pesos escalares aprendíveis\n",
    "        self.w_r = nn.Parameter(torch.tensor(init_w_r, dtype=torch.float32))\n",
    "        self.w_e = nn.Parameter(torch.tensor(init_w_e, dtype=torch.float32))\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, static_red: torch.Tensor, dyn_emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        static_red: Tensor[B, dim] – vetor reduzido estático\n",
    "        dyn_emb:     Tensor[B, dim] – vetor reduzido dinâmico (LSTM)\n",
    "        retorna:     Tensor[B, dim] – fusão ponderada\n",
    "        \"\"\"\n",
    "        # checa que as dimensões batem\n",
    "        assert static_red.shape == dyn_emb.shape and static_red.size(1) == self.dim\n",
    "        # soma ponderada\n",
    "        fused = self.w_r * static_red + self.w_e * dyn_emb\n",
    "        return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0637b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users_by_city = {\"A\":100_000, \"B\":25_000, \"C\":20_000, \"D\":6_000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f328717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP500(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP simples com 1 hidden layer de 500 ReLUs.\n",
    "     - in_dim → 500 → C logits\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, hidden_dim: int, n_clusters: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, n_clusters)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, in_dim)\n",
    "        x = F.relu(self.fc1(x))  # (B, hidden_dim)\n",
    "        return self.fc2(x)       # (B, n_clusters)\n",
    "\n",
    "\n",
    "class DestinationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Combina um MLP500 + softmax + weighted sum pelos cluster centers.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim: int,           # deve ser 20\n",
    "                 hidden_dim: int,       # 500\n",
    "                 cluster_centers: torch.Tensor  # (C,2)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        C, coord_dim = cluster_centers.shape\n",
    "        assert coord_dim == 2\n",
    "        self.mlp500 = MLP500(in_dim, hidden_dim, C)\n",
    "        # armazenamos centros como buffer (não aprensíveis)\n",
    "        self.register_buffer(\"centers\", cluster_centers)\n",
    "\n",
    "    def forward(self, fused: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        fused: (B, 20) → retorna coords (B,2)\n",
    "        \"\"\"\n",
    "        logits = self.mlp500(fused)        # (B, C)\n",
    "        P      = F.softmax(logits, dim=1)  # (B, C)\n",
    "        # média ponderada: P @ centers → (B,2)\n",
    "        coords = P @ self.centers\n",
    "        return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2567e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan  \n",
    "\n",
    "def compute_hdbscan_centers(\n",
    "    parquet_path: str,\n",
    "    city_letter: str = \"D\",\n",
    "    day_threshold: int = 60,\n",
    "    chunk_size: int = 50_000,\n",
    "    min_cluster_size: int = 100\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    1) Lê em chunks o Parquet,\n",
    "    2) filtra city==city_letter e d<day_threshold,\n",
    "    3) acumula destinos (x,y),\n",
    "    4) roda HDBSCAN para descobrir clusters,\n",
    "    5) retorna tensor K×2 com os centros (média dos pontos em cada cluster).\n",
    "    \"\"\"\n",
    "    # 1) coleta todos os destinos de treino da cidade\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    coords_list = []\n",
    "    for batch in pf.iter_batches(batch_size=chunk_size):\n",
    "        tbl = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "        mask = pc.and_(\n",
    "            pc.equal(tbl.column(\"city\"), city_letter),\n",
    "            pc.less(tbl.column(\"d\"), day_threshold)\n",
    "        )\n",
    "        tbl = tbl.filter(mask)\n",
    "        if tbl.num_rows == 0:\n",
    "            continue\n",
    "        xs = tbl.column(\"x\").to_numpy()\n",
    "        ys = tbl.column(\"y\").to_numpy()\n",
    "        coords_list.append(np.stack([xs, ys], axis=1))\n",
    "    coords = np.vstack(coords_list)  # shape (N,2)\n",
    "\n",
    "    # 2) (opcional) amostra para acelerar\n",
    "    if len(coords) > 200_000:\n",
    "        idx = np.random.choice(len(coords), 200_000, replace=False)\n",
    "        sample = coords[idx]\n",
    "    else:\n",
    "        sample = coords\n",
    "\n",
    "    # 3) roda HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        metric=\"euclidean\",\n",
    "        cluster_selection_method=\"eom\"\n",
    "    )\n",
    "    labels = clusterer.fit_predict(sample)  # shape (M,)\n",
    "\n",
    "    # 4) calcula centros como média de cada cluster\n",
    "    unique_labels = [lab for lab in np.unique(labels) if lab >= 0]\n",
    "    centers = []\n",
    "    for lab in unique_labels:\n",
    "        pts = sample[labels == lab]\n",
    "        centers.append(pts.mean(axis=0))\n",
    "    centers = np.vstack(centers)  # shape (K,2)\n",
    "\n",
    "    return torch.from_numpy(centers.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291bc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuMobModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo completo que combina todas as partes e faz rollout para múltiplos passos.\n",
    "    É como um \"diretor de orquestra\" que coordena todos os músicos (módulos).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_users: int,\n",
    "        n_days: int,\n",
    "        n_slots: int,\n",
    "        n_cities: int,\n",
    "        cluster_centers: torch.Tensor,\n",
    "        emb_dim: int = 10,\n",
    "        poi_in_dim: int = 85,\n",
    "        poi_out_dim: int = 10,\n",
    "        lstm_hidden: int = 10,\n",
    "        fusion_dim: int = 20\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Componentes que você já tem\n",
    "        self.fusion = ExternalInformationFusionDTPC(\n",
    "            n_users=n_users,\n",
    "            n_days=n_days,\n",
    "            n_slots=n_slots,\n",
    "            n_cities=n_cities,\n",
    "            emb_dim=emb_dim,\n",
    "            poi_in_dim=poi_in_dim,\n",
    "            poi_out_dim=poi_out_dim\n",
    "        )\n",
    "        self.dense = ExternalInformationDense(\n",
    "            in_dim=self.fusion.out_dim, \n",
    "            out_dim=fusion_dim\n",
    "        )\n",
    "        self.lstm = CoordLSTM(\n",
    "            input_size=2, \n",
    "            hidden_size=lstm_hidden, \n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.weighted_fusion = WeightedFusion(dim=fusion_dim)\n",
    "        self.destination_head = DestinationHead(\n",
    "            in_dim=fusion_dim,\n",
    "            hidden_dim=500,\n",
    "            cluster_centers=cluster_centers\n",
    "        )\n",
    "        \n",
    "    def forward_single_step(self, uid, d, t, city, poi, coords_seq):\n",
    "        \"\"\"\n",
    "        Faz uma predição para um único passo.\n",
    "        É como fazer uma \"foto\" da situação atual e prever o próximo movimento.\n",
    "        \"\"\"\n",
    "        # Informação estática (contexto)\n",
    "        static_emb = self.fusion(uid, d, t, city, poi)\n",
    "        static_red = self.dense(static_emb)\n",
    "        \n",
    "        # Informação dinâmica (padrão de movimento)\n",
    "        dyn_emb = self.lstm(coords_seq)\n",
    "        \n",
    "        # Fusão inteligente\n",
    "        fused = self.weighted_fusion(static_red, dyn_emb)\n",
    "        \n",
    "        # Predição final\n",
    "        pred_coords = self.destination_head(fused)\n",
    "        \n",
    "        return pred_coords\n",
    "    \n",
    "    def rollout_predictions(\n",
    "        self, \n",
    "        uid, d, t, city, poi, coords_seq, \n",
    "        n_steps: int,\n",
    "        use_predictions: bool = True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Faz predições para múltiplos passos futuros.\n",
    "        É como um \"GPS\" que planeja toda a rota, não só o próximo movimento.\n",
    "        \n",
    "        use_predictions: se True, usa predições anteriores como input (autoregressivo)\n",
    "                        se False, sempre usa a sequência original (teacher forcing)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        current_seq = coords_seq.clone()\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Prediz próximo passo\n",
    "            pred = self.forward_single_step(uid, d, t, city, poi, current_seq)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            if use_predictions and step < n_steps - 1:\n",
    "                # Atualiza sequência: remove primeiro ponto, adiciona predição\n",
    "                # É como \"deslizar uma janela\" no tempo\n",
    "                new_point = pred.unsqueeze(1)  # (batch, 1, 2)\n",
    "                current_seq = torch.cat([current_seq[:, 1:, :], new_point], dim=1)\n",
    "            \n",
    "            # Incrementa tempo (simplificado - você pode ajustar conforme sua lógica)\n",
    "            t = t + 1\n",
    "            \n",
    "        return torch.stack(predictions, dim=1)  # (batch, n_steps, 2)\n",
    "    \n",
    "    def forward(self, uid, d, t, city, poi, coords_seq, n_steps=1):\n",
    "        \"\"\"Forward principal - pode ser usado tanto para treino quanto inferência.\"\"\"\n",
    "        if n_steps == 1:\n",
    "            return self.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "        else:\n",
    "            return self.rollout_predictions(uid, d, t, city, poi, coords_seq, n_steps)\n",
    "\n",
    "\n",
    "def discretize_coordinates(coords_pred: torch.Tensor, grid_size: int = 200):\n",
    "    \"\"\"\n",
    "    Converte coordenadas contínuas para grid discreto [0, grid_size-1].\n",
    "    É como \"encaixar\" as predições nas células da grade.\n",
    "    \"\"\"\n",
    "    # Arredonda e garante que está no intervalo correto\n",
    "    coords_discrete = torch.round(coords_pred).long()\n",
    "    coords_discrete = torch.clamp(coords_discrete, 0, grid_size - 1)\n",
    "    return coords_discrete\n",
    "\n",
    "\n",
    "def create_submission_data(model, dataloader, device, n_prediction_days=15, slots_per_day=48):\n",
    "    \"\"\"\n",
    "    Gera dados para submissão no formato exigido pelo HuMob.\n",
    "    É como \"traduzir\" as predições do modelo para o formato que os juízes esperam.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    submission_rows = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            uid, d, t, city, poi, coords_seq, _ = [b.to(device) for b in batch]\n",
    "            \n",
    "            # Total de passos a prever\n",
    "            total_steps = n_prediction_days * slots_per_day\n",
    "            \n",
    "            # Faz rollout\n",
    "            predictions = model.rollout_predictions(\n",
    "                uid, d, t, city, poi, coords_seq, \n",
    "                n_steps=total_steps,\n",
    "                use_predictions=True\n",
    "            )\n",
    "            \n",
    "            # Discretiza coordenadas\n",
    "            predictions_discrete = discretize_coordinates(predictions)\n",
    "            \n",
    "            # Converte para formato de submissão\n",
    "            batch_size = uid.size(0)\n",
    "            for b in range(batch_size):\n",
    "                user_id = uid[b].item()\n",
    "                start_day = d[b].item()\n",
    "                \n",
    "                for step in range(total_steps):\n",
    "                    day = start_day + (step // slots_per_day)\n",
    "                    slot = step % slots_per_day\n",
    "                    x = predictions_discrete[b, step, 0].item()\n",
    "                    y = predictions_discrete[b, step, 1].item()\n",
    "                    \n",
    "                    submission_rows.append({\n",
    "                        'uid': user_id,\n",
    "                        'day': day,\n",
    "                        'slot': slot,\n",
    "                        'x': x,\n",
    "                        'y': y\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(submission_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HuMobLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Loss híbrida que combina classificação (qual cluster) + regressão (coordenadas exatas).\n",
    "    É como ter dois \"professores\": um que ensina \"em que região\" e outro \"exatamente onde\".\n",
    "    \"\"\"\n",
    "    def __init__(self, cluster_centers: torch.Tensor, alpha: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.register_buffer('cluster_centers', cluster_centers)\n",
    "        self.alpha = alpha  # peso para o loss de coordenadas\n",
    "        self.ce_loss = CrossEntropyLoss()\n",
    "        self.mse_loss = MSELoss()\n",
    "        \n",
    "    def forward(self, predicted_coords, target_coords, cluster_logits=None):\n",
    "        \"\"\"\n",
    "        predicted_coords: (batch, 2) ou (batch, steps, 2)\n",
    "        target_coords: (batch, 2) ou (batch, steps, 2) \n",
    "        cluster_logits: (batch, n_clusters) ou (batch, steps, n_clusters)\n",
    "        \"\"\"\n",
    "        # Achata se for sequência\n",
    "        if predicted_coords.dim() == 3:\n",
    "            predicted_coords = predicted_coords.view(-1, 2)\n",
    "            target_coords = target_coords.view(-1, 2)\n",
    "            if cluster_logits is not None:\n",
    "                cluster_logits = cluster_logits.view(-1, cluster_logits.size(-1))\n",
    "        \n",
    "        # Loss de coordenadas (regressão)\n",
    "        coord_loss = self.mse_loss(predicted_coords, target_coords)\n",
    "        \n",
    "        total_loss = self.alpha * coord_loss\n",
    "        \n",
    "        # Loss de cluster (classificação) - opcional\n",
    "        if cluster_logits is not None:\n",
    "            # Encontra cluster mais próximo para cada target\n",
    "            distances = torch.cdist(target_coords, self.cluster_centers)  # (batch, n_clusters)\n",
    "            target_clusters = distances.argmin(dim=1)  # (batch,)\n",
    "            \n",
    "            cluster_loss = self.ce_loss(cluster_logits, target_clusters)\n",
    "            total_loss += (1 - self.alpha) * cluster_loss\n",
    "            \n",
    "            return total_loss, coord_loss, cluster_loss\n",
    "        \n",
    "        return total_loss, coord_loss, torch.tensor(0.0)\n",
    "\n",
    "\n",
    "def train_humob_model(\n",
    "    model: HuMobModel,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    n_epochs: int = 100,\n",
    "    learning_rate: float = 1e-3,\n",
    "    patience: int = 10,\n",
    "    teacher_forcing_ratio: float = 0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Treina o modelo HuMob com early stopping e teacher forcing.\n",
    "    \n",
    "    teacher_forcing_ratio: probabilidade de usar ground truth vs predições durante treino\n",
    "                          É como decidir quando \"dar a resposta\" vs deixar o modelo \"chutar\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup do treinamento\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    criterion = HuMobLoss(model.destination_head.centers)\n",
    "    \n",
    "    # Métricas para acompanhar\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(f\"Treinando modelo por {n_epochs} épocas...\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Parâmetros treináveis: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # === TREINAMENTO ===\n",
    "        model.train()\n",
    "        train_loss_epoch = 0\n",
    "        train_coord_loss_epoch = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f'Época {epoch+1}/{n_epochs} [Treino]')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Decide se usa teacher forcing (durante treino sequencial)\n",
    "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            \n",
    "            if target_coords.size(1) == 1:  # Single step prediction\n",
    "                pred_coords = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "                target = target_coords.squeeze(1)  # Remove dimensão do step\n",
    "            else:  # Multi-step prediction\n",
    "                pred_coords = model.rollout_predictions(\n",
    "                    uid, d, t, city, poi, coords_seq,\n",
    "                    n_steps=target_coords.size(1),\n",
    "                    use_predictions=not use_teacher_forcing\n",
    "                )\n",
    "                target = target_coords\n",
    "            \n",
    "            # Calcula loss\n",
    "            loss, coord_loss, cluster_loss = criterion(pred_coords, target)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumula métricas\n",
    "            train_loss_epoch += loss.item()\n",
    "            train_coord_loss_epoch += coord_loss.item()\n",
    "            \n",
    "            # Atualiza progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Coord': f'{coord_loss.item():.4f}',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "            })\n",
    "        \n",
    "        # === VALIDAÇÃO ===\n",
    "        model.eval()\n",
    "        val_loss_epoch = 0\n",
    "        val_coord_loss_epoch = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Época {epoch+1}/{n_epochs} [Val]')\n",
    "            \n",
    "            for batch in val_pbar:\n",
    "                uid, d, t, city, poi, coords_seq, target_coords = [b.to(device) for b in batch]\n",
    "                \n",
    "                if target_coords.size(1) == 1:\n",
    "                    pred_coords = model.forward_single_step(uid, d, t, city, poi, coords_seq)\n",
    "                    target = target_coords.squeeze(1)\n",
    "                else:\n",
    "                    pred_coords = model.rollout_predictions(\n",
    "                        uid, d, t, city, poi, coords_seq,\n",
    "                        n_steps=target_coords.size(1),\n",
    "                        use_predictions=True  # Sempre usa predições na validação\n",
    "                    )\n",
    "                    target = target_coords\n",
    "                \n",
    "                loss, coord_loss, _ = criterion(pred_coords, target)\n",
    "                \n",
    "                val_loss_epoch += loss.item()\n",
    "                val_coord_loss_epoch += coord_loss.item()\n",
    "                \n",
    "                val_pbar.set_postfix({'Val Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # Médias da época\n",
    "        avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "        avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "        avg_train_coord = train_coord_loss_epoch / len(train_loader)\n",
    "        avg_val_coord = val_coord_loss_epoch / len(val_loader)\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Salva melhor modelo\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': avg_val_loss,\n",
    "                'train_loss': avg_train_loss\n",
    "            }, 'best_humob_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Print estatísticas da época\n",
    "        print(f'\\nÉpoca {epoch+1}:')\n",
    "        print(f'  Train Loss: {avg_train_loss:.4f} (Coord: {avg_train_coord:.4f})')\n",
    "        print(f'  Val Loss:   {avg_val_loss:.4f} (Coord: {avg_val_coord:.4f})')\n",
    "        print(f'  LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print(f'  Fusion weights: w_r={model.weighted_fusion.w_r.item():.3f}, w_e={model.weighted_fusion.w_e.item():.3f}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping! Sem melhoria por {patience} épocas.')\n",
    "            break\n",
    "    \n",
    "    # Plot das curvas de treinamento\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(val_losses, label='Validation')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Curvas de Treinamento')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot([model.weighted_fusion.w_r.item()], [model.weighted_fusion.w_e.item()], 'ro', markersize=10)\n",
    "    plt.xlabel('w_r (peso estático)')\n",
    "    plt.ylabel('w_e (peso dinâmico)')\n",
    "    plt.title('Pesos Finais da Fusão')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "# Função para usar no seu notebook\n",
    "def setup_and_train():\n",
    "    \"\"\"\n",
    "    Função principal que você pode chamar no seu notebook.\n",
    "    Substitui a parte que está faltando.\n",
    "    \"\"\"\n",
    "    # 1. Calcula centros dos clusters (seu código já faz isso)\n",
    "    cluster_centers = compute_hdbscan_centers(\n",
    "        parquet_file,\n",
    "        city_letter=\"D\",\n",
    "        day_threshold=60,\n",
    "        chunk_size=50_000,\n",
    "        min_cluster_size=200\n",
    "    ).to(device)\n",
    "    \n",
    "    # 2. Cria dataset corrigido\n",
    "    train_ds = ParquetCityDDataset(\n",
    "        parquet_file, \n",
    "        city_list=[\"D\"], \n",
    "        chunk_size=2500,\n",
    "        sequence_length=24,  # 24 slots de história\n",
    "        prediction_steps=1   # Prediz 1 passo à frente\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, num_workers=2, shuffle=False)\n",
    "    \n",
    "    # 3. Cria modelo completo\n",
    "    model = HuMobModel(\n",
    "        n_users=n_users_D,\n",
    "        n_days=75,\n",
    "        n_slots=48,\n",
    "        n_cities=4,\n",
    "        cluster_centers=cluster_centers,\n",
    "        emb_dim=10,\n",
    "        poi_in_dim=85,\n",
    "        poi_out_dim=10,\n",
    "        lstm_hidden=10,\n",
    "        fusion_dim=20\n",
    "    ).to(device)\n",
    "    \n",
    "    # 4. Treina (você pode dividir em train/val se quiser)\n",
    "    train_losses, val_losses = train_humob_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=train_loader,  # Por enquanto usa mesmo dataset\n",
    "        device=device,\n",
    "        n_epochs=50,\n",
    "        learning_rate=1e-3,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

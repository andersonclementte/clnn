{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aae19df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Iniciando normaliza√ß√£o do dataset HuMob\n",
      "üìÇ Input: humob_all_cities_dpsk.parquet\n",
      "üìÅ Output: humob_all_cities_normalized.parquet\n",
      "============================================================\n",
      "üìä Calculando estat√≠sticas dos dados...\n",
      "üìÅ Processando 162,785,736 linhas em 3256 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando stats:   0%|          | 13/3256 [01:18<5:24:18,  6.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 427\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Executa normaliza√ß√£o\u001b[39;00m\n\u001b[32m    426\u001b[39m normalizer = HuMobNormalizer(INPUT_FILE, OUTPUT_FILE)\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Verifica resultado\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 362\u001b[39m, in \u001b[36mHuMobNormalizer.run\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# Passo 1: Calcular estat√≠sticas (TODAS, n√£o s√≥ debug)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_statistics_robust\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# Passo 2: Processar e salvar\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;28mself\u001b[39m.process_and_save(chunk_size)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mHuMobNormalizer.compute_statistics_robust\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m poi_list:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         float_val = \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(float_val) \u001b[38;5;129;01mor\u001b[39;00m float_val < \u001b[32m0\u001b[39m:\n\u001b[32m    140\u001b[39m             float_val = \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üéØ Script de Normaliza√ß√£o do Dataset HuMob - VERS√ÉO CORRIGIDA\n",
    "===========================================================\n",
    "\n",
    "Normaliza os dados do HuMob e salva um dataset j√° pr√©-processado.\n",
    "Isso resolve problemas de explos√£o de gradientes ao colocar todas as \n",
    "features na mesma escala.\n",
    "\n",
    "Uso:\n",
    "    python normalize_humob_data_fixed.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HuMobNormalizer:\n",
    "    \"\"\"\n",
    "    Normaliza dataset HuMob com estrat√©gias espec√≠ficas para cada tipo de feature.\n",
    "    \n",
    "    üéØ Analogia: Como calibrar uma receita onde cada ingrediente tem escala diferente\n",
    "    - Coordenadas (x,y): Como quilos de farinha ‚Üí normalizar para [0,1]\n",
    "    - Tempo (d,t): Como colheres de sal ‚Üí normalizar ciclicamente  \n",
    "    - POIs: Como gotas de corante ‚Üí log-transform + normalize\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path: str, output_path: str):\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        \n",
    "        # Scalers que ser√£o salvos para uso posterior\n",
    "        self.coords_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.poi_stats = {}  # Para log1p + normaliza√ß√£o manual\n",
    "        \n",
    "        # Metadados para salvar junto\n",
    "        self.normalization_info = {\n",
    "            \"version\": \"1.0\",\n",
    "            \"strategy\": {\n",
    "                \"coordinates\": \"MinMaxScaler [0,1]\",\n",
    "                \"days\": \"Linear [0,1]\", \n",
    "                \"timeslots\": \"Circular sin/cos\",\n",
    "                \"pois\": \"log1p + per-column normalization\",\n",
    "                \"cities\": \"Label encoding A=0, B=1, C=2, D=3\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def compute_statistics_robust(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Calcula estat√≠sticas com tratamento robusto de POIs problem√°ticos.\n",
    "        PROCESSA TODOS OS CHUNKS, n√£o s√≥ os primeiros 5!\n",
    "        \"\"\"\n",
    "        print(\"üìä Calculando estat√≠sticas dos dados...\")\n",
    "        \n",
    "        # Acumuladores para coordenadas\n",
    "        all_coords = []\n",
    "        \n",
    "        # Acumuladores para POIs (por coluna) - COM LIMPEZA\n",
    "        poi_sums = np.zeros(85)\n",
    "        poi_counts = 0\n",
    "        poi_max_vals = np.zeros(85)\n",
    "        \n",
    "        # Ranges de tempo\n",
    "        max_day = 0\n",
    "        \n",
    "        # Contadores para debug\n",
    "        problematic_chunks = 0\n",
    "        total_problematic_pois = 0\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        total_batches = (pf.metadata.num_rows + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        print(f\"üìÅ Processando {pf.metadata.num_rows:,} linhas em {total_batches} chunks...\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                                      total=total_batches, desc=\"Calculando stats\")):\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # 1. COORDENADAS (como sempre)\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            coords = np.column_stack([x_vals, y_vals])\n",
    "            all_coords.append(coords)\n",
    "            \n",
    "            # 2. DIAS (como sempre)\n",
    "            d_vals = table.column(\"d\").to_numpy()\n",
    "            max_day = max(max_day, d_vals.max())\n",
    "            \n",
    "            # 3. POIs COM LIMPEZA ROBUSTA\n",
    "            try:\n",
    "                poi_lists = table.column(\"POI\").to_pylist()\n",
    "                \n",
    "                # Limpa POIs problem√°ticos\n",
    "                cleaned_poi_lists = []\n",
    "                chunk_problems = 0\n",
    "                \n",
    "                for poi_list in poi_lists:\n",
    "                    if poi_list is None:\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif not isinstance(poi_list, (list, tuple)):\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif len(poi_list) != 85:\n",
    "                        # Ajusta tamanho\n",
    "                        if len(poi_list) < 85:\n",
    "                            poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "                        else:\n",
    "                            poi_fixed = list(poi_list)[:85]\n",
    "                        \n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_fixed:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                        chunk_problems += 1\n",
    "                    else:\n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_list:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                \n",
    "                # Converte para array e processa\n",
    "                poi_array = np.array(cleaned_poi_lists, dtype=np.float32)\n",
    "                \n",
    "                # Limpa qualquer NaN/Inf restante\n",
    "                poi_array = np.nan_to_num(poi_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # Acumula estat√≠sticas\n",
    "                poi_sums += poi_array.sum(axis=0)\n",
    "                poi_counts += poi_array.shape[0]\n",
    "                poi_max_vals = np.maximum(poi_max_vals, poi_array.max(axis=0))\n",
    "                \n",
    "                # Conta problemas\n",
    "                if chunk_problems > 0:\n",
    "                    problematic_chunks += 1\n",
    "                    total_problematic_pois += chunk_problems\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro no chunk {i}: {str(e)}\")\n",
    "                problematic_chunks += 1\n",
    "                # Usa zeros para este chunk problem√°tico\n",
    "                poi_array = np.zeros((chunk_size, 85), dtype=np.float32)\n",
    "                poi_sums += poi_array.sum(axis=0)\n",
    "                poi_counts += min(chunk_size, len(x_vals))  # Usa tamanho real do chunk\n",
    "        \n",
    "        # Calcula scalers das coordenadas\n",
    "        all_coords = np.vstack(all_coords)\n",
    "        self.coords_scaler.fit(all_coords)\n",
    "        \n",
    "        # Salva estat√≠sticas POI\n",
    "        self.poi_stats = {\n",
    "            'max_vals': poi_max_vals.tolist(),\n",
    "            'mean_vals': (poi_sums / max(poi_counts, 1)).tolist()  # Evita divis√£o por zero\n",
    "        }\n",
    "        \n",
    "        # Salva info de tempo\n",
    "        self.max_day = max_day\n",
    "        \n",
    "        print(f\"‚úÖ Estat√≠sticas calculadas:\")\n",
    "        print(f\"   üìç Coordenadas: x[{all_coords[:,0].min():.0f}, {all_coords[:,0].max():.0f}], y[{all_coords[:,1].min():.0f}, {all_coords[:,1].max():.0f}] ‚Üí [0,1]\")\n",
    "        print(f\"   üìÖ Dias: [0, {max_day}]\")\n",
    "        print(f\"   üè¢ POIs: max_sum={poi_max_vals.max():.0f}, mean_sum={poi_sums.max()/max(poi_counts,1):.2f}\")\n",
    "        \n",
    "        if problematic_chunks > 0 or total_problematic_pois > 0:\n",
    "            print(f\"   üîß Limpeza POI: {problematic_chunks} chunks com problemas, {total_problematic_pois:,} POIs corrigidos\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ POIs: Nenhum problema encontrado!\")\n",
    "    \n",
    "    def normalize_coordinates(self, x_vals, y_vals):\n",
    "        \"\"\"Normaliza coordenadas para [0, 1]\"\"\"\n",
    "        coords = np.column_stack([x_vals, y_vals])\n",
    "        coords_norm = self.coords_scaler.transform(coords)\n",
    "        return coords_norm[:, 0], coords_norm[:, 1]\n",
    "    \n",
    "    def normalize_time(self, d_vals, t_vals):\n",
    "        \"\"\"\n",
    "        Normaliza tempo:\n",
    "        - Dias: linear [0,1]  \n",
    "        - Timeslots: circular (sin/cos)\n",
    "        \"\"\"\n",
    "        # Dias: normaliza√ß√£o linear\n",
    "        d_norm = d_vals.astype(np.float32) / self.max_day\n",
    "        \n",
    "        # Timeslots: normaliza√ß√£o circular (importante para hor√°rios!)\n",
    "        t_sin = np.sin(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        t_cos = np.cos(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        \n",
    "        return d_norm, t_sin, t_cos\n",
    "    \n",
    "    def normalize_pois_robust(self, poi_lists):\n",
    "        \"\"\"\n",
    "        Normaliza POIs com estrat√©gia robusta - VERS√ÉO MELHORADA\n",
    "        \"\"\"\n",
    "        # Limpeza preventiva\n",
    "        valid_poi_lists = []\n",
    "        \n",
    "        for poi_list in poi_lists:\n",
    "            # Trata casos problem√°ticos\n",
    "            if poi_list is None or not isinstance(poi_list, (list, tuple)):\n",
    "                valid_poi_lists.append([0.0] * 85)\n",
    "                continue\n",
    "            \n",
    "            # Converte e ajusta tamanho\n",
    "            if len(poi_list) < 85:\n",
    "                poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "            elif len(poi_list) > 85:\n",
    "                poi_fixed = list(poi_list)[:85]\n",
    "            else:\n",
    "                poi_fixed = list(poi_list)\n",
    "            \n",
    "            # Converte para float e limpa\n",
    "            poi_cleaned = []\n",
    "            for val in poi_fixed:\n",
    "                try:\n",
    "                    float_val = float(val) if val is not None else 0.0\n",
    "                    if not np.isfinite(float_val):\n",
    "                        float_val = 0.0\n",
    "                    poi_cleaned.append(max(0.0, float_val))  # Remove negativos tamb√©m\n",
    "                except (ValueError, TypeError):\n",
    "                    poi_cleaned.append(0.0)\n",
    "            \n",
    "            valid_poi_lists.append(poi_cleaned)\n",
    "        \n",
    "        # Converte para array\n",
    "        poi_array = np.array(valid_poi_lists, dtype=np.float32)\n",
    "        \n",
    "        # Aplica log1p e normaliza√ß√£o como antes\n",
    "        poi_log = np.log1p(poi_array)\n",
    "        \n",
    "        max_vals = np.array(self.poi_stats['max_vals'])\n",
    "        max_vals_log = np.log1p(max_vals)\n",
    "        max_vals_log[max_vals_log == 0] = 1.0\n",
    "        \n",
    "        poi_normalized = poi_log / max_vals_log[np.newaxis, :]\n",
    "        poi_normalized = np.clip(poi_normalized, 0.0, 1.0)\n",
    "        \n",
    "        return poi_normalized.tolist()\n",
    "    \n",
    "    def normalize_cities(self, city_vals):\n",
    "        \"\"\"Converte cidades para encoding num√©rico\"\"\"\n",
    "        city_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "        return np.array([city_mapping[c] for c in city_vals], dtype=np.int8)\n",
    "    \n",
    "    def process_and_save(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Processa e salva dataset normalizado chunk por chunk.\n",
    "        \"\"\"\n",
    "        print(\"üîÑ Normalizando e salvando dados...\")\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        total_batches = (pf.metadata.num_rows + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        writer = None\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for batch in tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                         total=total_batches, desc=\"Normalizando\"):\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # Extrai dados originais\n",
    "            uid_vals = table.column(\"uid\").to_numpy()\n",
    "            d_vals = table.column(\"d\").to_numpy() \n",
    "            t_vals = table.column(\"t\").to_numpy()\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            city_vals = table.column(\"city\").to_pylist()\n",
    "            poi_lists = table.column(\"POI\").to_pylist()\n",
    "            \n",
    "            # Aplica normaliza√ß√µes\n",
    "            x_norm, y_norm = self.normalize_coordinates(x_vals, y_vals)\n",
    "            d_norm, t_sin, t_cos = self.normalize_time(d_vals, t_vals)\n",
    "            poi_norm = self.normalize_pois_robust(poi_lists)\n",
    "            city_encoded = self.normalize_cities(city_vals)\n",
    "            \n",
    "            # Cria novo schema\n",
    "            if writer is None:\n",
    "                new_schema = pa.schema([\n",
    "                    pa.field(\"uid\", pa.int64()),\n",
    "                    pa.field(\"d_orig\", pa.uint8()),  # Original para refer√™ncia  \n",
    "                    pa.field(\"t_orig\", pa.uint8()),  # Original para refer√™ncia\n",
    "                    pa.field(\"d_norm\", pa.float32()),  # Normalizado [0,1]\n",
    "                    pa.field(\"t_sin\", pa.float32()),   # Circular sin\n",
    "                    pa.field(\"t_cos\", pa.float32()),   # Circular cos  \n",
    "                    pa.field(\"x_norm\", pa.float32()),  # Normalizado [0,1]\n",
    "                    pa.field(\"y_norm\", pa.float32()),  # Normalizado [0,1]\n",
    "                    pa.field(\"city\", pa.string()),     # Original\n",
    "                    pa.field(\"city_encoded\", pa.int8()),  # Encoded\n",
    "                    pa.field(\"POI_norm\", pa.list_(pa.float32()))  # Normalizado\n",
    "                ])\n",
    "                \n",
    "                # Adiciona metadados\n",
    "                metadata = {\n",
    "                    b\"normalization_info\": json.dumps(self.normalization_info).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_min\": json.dumps(self.coords_scaler.data_min_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_scale\": json.dumps(self.coords_scaler.scale_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"poi_stats\": json.dumps(self.poi_stats).encode(\"utf-8\"),\n",
    "                    b\"max_day\": str(self.max_day).encode(\"utf-8\")\n",
    "                }\n",
    "                new_schema = new_schema.with_metadata(metadata)\n",
    "                \n",
    "                writer = pq.ParquetWriter(self.output_path, new_schema, compression='snappy')\n",
    "            \n",
    "            # Cria nova tabela\n",
    "            new_table = pa.table({\n",
    "                \"uid\": uid_vals,\n",
    "                \"d_orig\": d_vals,\n",
    "                \"t_orig\": t_vals, \n",
    "                \"d_norm\": d_norm,\n",
    "                \"t_sin\": t_sin,\n",
    "                \"t_cos\": t_cos,\n",
    "                \"x_norm\": x_norm,\n",
    "                \"y_norm\": y_norm,\n",
    "                \"city\": city_vals,\n",
    "                \"city_encoded\": city_encoded,\n",
    "                \"POI_norm\": poi_norm\n",
    "            }, schema=new_schema)\n",
    "            \n",
    "            writer.write_table(new_table)\n",
    "            processed_rows += len(uid_vals)\n",
    "        \n",
    "        if writer:\n",
    "            writer.close()\n",
    "            \n",
    "        print(f\"‚úÖ Dataset normalizado salvo em: {self.output_path}\")\n",
    "        print(f\"üìä Linhas processadas: {processed_rows:,}\")\n",
    "        print(f\"üíæ Tamanho do arquivo: {self.output_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "    \n",
    "    def run(self, chunk_size: int = 50_000):\n",
    "        \"\"\"Executa normaliza√ß√£o completa\"\"\"\n",
    "        print(\"üéØ Iniciando normaliza√ß√£o do dataset HuMob\")\n",
    "        print(f\"üìÇ Input: {self.input_path}\")\n",
    "        print(f\"üìÅ Output: {self.output_path}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Passo 1: Calcular estat√≠sticas (TODAS, n√£o s√≥ debug)\n",
    "        self.compute_statistics_robust(chunk_size)\n",
    "        \n",
    "        # Passo 2: Processar e salvar\n",
    "        self.process_and_save(chunk_size)\n",
    "        \n",
    "        print(\"\\nüéâ Normaliza√ß√£o conclu√≠da com sucesso!\")\n",
    "        print(\"\\nüìã Pr√≥ximos passos:\")\n",
    "        print(\"   1. Teste o novo dataset com check_normalized_data()\")\n",
    "        print(\"   2. Atualize seu clnn.ipynb para usar dados normalizados\")\n",
    "        print(\"   3. Remova normaliza√ß√µes inline do c√≥digo de treino\")\n",
    "\n",
    "\n",
    "def check_normalized_data(file_path: str, n_samples: int = 1000):\n",
    "    \"\"\"\n",
    "    Verifica se os dados foram normalizados corretamente.\n",
    "    \"\"\"\n",
    "    print(f\"üîç Verificando dados normalizados em: {file_path}\")\n",
    "    \n",
    "    # L√™ amostra\n",
    "    df = pq.read_table(file_path).slice(0, n_samples).to_pandas()\n",
    "    \n",
    "    print(f\"\\nüìä Estat√≠sticas da amostra ({len(df):,} linhas):\")\n",
    "    print(f\"   üìç Coordenadas:\")\n",
    "    print(f\"      x_norm: [{df['x_norm'].min():.3f}, {df['x_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      y_norm: [{df['y_norm'].min():.3f}, {df['y_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    \n",
    "    print(f\"   üìÖ Tempo:\")\n",
    "    print(f\"      d_norm: [{df['d_norm'].min():.3f}, {df['d_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      t_sin: [{df['t_sin'].min():.3f}, {df['t_sin'].max():.3f}] (deve ser [-1,1])\")\n",
    "    print(f\"      t_cos: [{df['t_cos'].min():.3f}, {df['t_cos'].max():.3f}] (deve ser [-1,1])\")\n",
    "    \n",
    "    print(f\"   üè¢ POIs:\")\n",
    "    poi_sample = np.array(df['POI_norm'].iloc[0])\n",
    "    print(f\"      Exemplo: [{poi_sample.min():.3f}, {poi_sample.max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      Dimens√£o: {len(poi_sample)} (deve ser 85)\")\n",
    "    \n",
    "    print(f\"   üèôÔ∏è Cidades:\")\n",
    "    print(f\"      Encoded: {sorted(df['city_encoded'].unique())} (deve ser [0,1,2,3])\")\n",
    "    print(f\"      Original: {sorted(df['city'].unique())}\")\n",
    "    \n",
    "    # Verifica metadados\n",
    "    pf = pq.ParquetFile(file_path)\n",
    "    metadata = pf.schema_arrow.metadata\n",
    "    if b\"normalization_info\" in metadata:\n",
    "        norm_info = json.loads(metadata[b\"normalization_info\"])\n",
    "        print(f\"\\n‚úÖ Metadados de normaliza√ß√£o encontrados:\")\n",
    "        for key, value in norm_info[\"strategy\"].items():\n",
    "            print(f\"      {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Metadados de normaliza√ß√£o n√£o encontrados!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configura√ß√µes\n",
    "    INPUT_FILE = \"humob_all_cities_dpsk.parquet\"  # Seu arquivo original\n",
    "    OUTPUT_FILE = \"humob_all_cities_normalized.parquet\"  # Arquivo normalizado\n",
    "    \n",
    "    # Verifica se arquivo de entrada existe\n",
    "    if not Path(INPUT_FILE).exists():\n",
    "        print(f\"‚ùå Arquivo n√£o encontrado: {INPUT_FILE}\")\n",
    "        print(\"üìù Certifique-se de que o arquivo foi gerado pelo data_loader.ipynb\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Executa normaliza√ß√£o\n",
    "    normalizer = HuMobNormalizer(INPUT_FILE, OUTPUT_FILE)\n",
    "    normalizer.run(chunk_size=50_000)\n",
    "    \n",
    "    # Verifica resultado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    check_normalized_data(OUTPUT_FILE, n_samples=5000)\n",
    "    \n",
    "    print(f\"\\nüéØ PRONTO! Agora voc√™ pode:\")\n",
    "    print(f\"   1. Usar '{OUTPUT_FILE}' no seu clnn.ipynb\")\n",
    "    print(f\"   2. Remover todas as normaliza√ß√µes inline do c√≥digo\")\n",
    "    print(f\"   3. Usar diretamente as colunas *_norm nas features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

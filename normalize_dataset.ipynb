{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aae19df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Iniciando normalização do dataset HuMob\n",
      "📂 Input: humob_all_cities_dpsk.parquet\n",
      "📁 Output: humob_all_cities_normalized.parquet\n",
      "============================================================\n",
      "📊 Calculando estatísticas dos dados...\n",
      "📁 Processando 162,785,736 linhas em 3256 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculando stats:   0%|          | 13/3256 [01:18<5:24:18,  6.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 427\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Executa normalização\u001b[39;00m\n\u001b[32m    426\u001b[39m normalizer = HuMobNormalizer(INPUT_FILE, OUTPUT_FILE)\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[43mnormalizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Verifica resultado\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 362\u001b[39m, in \u001b[36mHuMobNormalizer.run\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# Passo 1: Calcular estatísticas (TODAS, não só debug)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_statistics_robust\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# Passo 2: Processar e salvar\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;28mself\u001b[39m.process_and_save(chunk_size)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mHuMobNormalizer.compute_statistics_robust\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m poi_list:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         float_val = \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(float_val) \u001b[38;5;129;01mor\u001b[39;00m float_val < \u001b[32m0\u001b[39m:\n\u001b[32m    140\u001b[39m             float_val = \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "🎯 Script de Normalização do Dataset HuMob - VERSÃO CORRIGIDA\n",
    "===========================================================\n",
    "\n",
    "Normaliza os dados do HuMob e salva um dataset já pré-processado.\n",
    "Isso resolve problemas de explosão de gradientes ao colocar todas as \n",
    "features na mesma escala.\n",
    "\n",
    "Uso:\n",
    "    python normalize_humob_data_fixed.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HuMobNormalizer:\n",
    "    \"\"\"\n",
    "    Normaliza dataset HuMob com estratégias específicas para cada tipo de feature.\n",
    "    \n",
    "    🎯 Analogia: Como calibrar uma receita onde cada ingrediente tem escala diferente\n",
    "    - Coordenadas (x,y): Como quilos de farinha → normalizar para [0,1]\n",
    "    - Tempo (d,t): Como colheres de sal → normalizar ciclicamente  \n",
    "    - POIs: Como gotas de corante → log-transform + normalize\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path: str, output_path: str):\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        \n",
    "        # Scalers que serão salvos para uso posterior\n",
    "        self.coords_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.poi_stats = {}  # Para log1p + normalização manual\n",
    "        \n",
    "        # Metadados para salvar junto\n",
    "        self.normalization_info = {\n",
    "            \"version\": \"1.0\",\n",
    "            \"strategy\": {\n",
    "                \"coordinates\": \"MinMaxScaler [0,1]\",\n",
    "                \"days\": \"Linear [0,1]\", \n",
    "                \"timeslots\": \"Circular sin/cos\",\n",
    "                \"pois\": \"log1p + per-column normalization\",\n",
    "                \"cities\": \"Label encoding A=0, B=1, C=2, D=3\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def compute_statistics_robust(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Calcula estatísticas com tratamento robusto de POIs problemáticos.\n",
    "        PROCESSA TODOS OS CHUNKS, não só os primeiros 5!\n",
    "        \"\"\"\n",
    "        print(\"📊 Calculando estatísticas dos dados...\")\n",
    "        \n",
    "        # Acumuladores para coordenadas\n",
    "        all_coords = []\n",
    "        \n",
    "        # Acumuladores para POIs (por coluna) - COM LIMPEZA\n",
    "        poi_sums = np.zeros(85)\n",
    "        poi_counts = 0\n",
    "        poi_max_vals = np.zeros(85)\n",
    "        \n",
    "        # Ranges de tempo\n",
    "        max_day = 0\n",
    "        \n",
    "        # Contadores para debug\n",
    "        problematic_chunks = 0\n",
    "        total_problematic_pois = 0\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        total_batches = (pf.metadata.num_rows + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        print(f\"📁 Processando {pf.metadata.num_rows:,} linhas em {total_batches} chunks...\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                                      total=total_batches, desc=\"Calculando stats\")):\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # 1. COORDENADAS (como sempre)\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            coords = np.column_stack([x_vals, y_vals])\n",
    "            all_coords.append(coords)\n",
    "            \n",
    "            # 2. DIAS (como sempre)\n",
    "            d_vals = table.column(\"d\").to_numpy()\n",
    "            max_day = max(max_day, d_vals.max())\n",
    "            \n",
    "            # 3. POIs COM LIMPEZA ROBUSTA\n",
    "            try:\n",
    "                poi_lists = table.column(\"POI\").to_pylist()\n",
    "                \n",
    "                # Limpa POIs problemáticos\n",
    "                cleaned_poi_lists = []\n",
    "                chunk_problems = 0\n",
    "                \n",
    "                for poi_list in poi_lists:\n",
    "                    if poi_list is None:\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif not isinstance(poi_list, (list, tuple)):\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif len(poi_list) != 85:\n",
    "                        # Ajusta tamanho\n",
    "                        if len(poi_list) < 85:\n",
    "                            poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "                        else:\n",
    "                            poi_fixed = list(poi_list)[:85]\n",
    "                        \n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_fixed:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                        chunk_problems += 1\n",
    "                    else:\n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_list:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                \n",
    "                # Converte para array e processa\n",
    "                poi_array = np.array(cleaned_poi_lists, dtype=np.float32)\n",
    "                \n",
    "                # Limpa qualquer NaN/Inf restante\n",
    "                poi_array = np.nan_to_num(poi_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # Acumula estatísticas\n",
    "                poi_sums += poi_array.sum(axis=0)\n",
    "                poi_counts += poi_array.shape[0]\n",
    "                poi_max_vals = np.maximum(poi_max_vals, poi_array.max(axis=0))\n",
    "                \n",
    "                # Conta problemas\n",
    "                if chunk_problems > 0:\n",
    "                    problematic_chunks += 1\n",
    "                    total_problematic_pois += chunk_problems\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erro no chunk {i}: {str(e)}\")\n",
    "                problematic_chunks += 1\n",
    "                # Usa zeros para este chunk problemático\n",
    "                poi_array = np.zeros((chunk_size, 85), dtype=np.float32)\n",
    "                poi_sums += poi_array.sum(axis=0)\n",
    "                poi_counts += min(chunk_size, len(x_vals))  # Usa tamanho real do chunk\n",
    "        \n",
    "        # Calcula scalers das coordenadas\n",
    "        all_coords = np.vstack(all_coords)\n",
    "        self.coords_scaler.fit(all_coords)\n",
    "        \n",
    "        # Salva estatísticas POI\n",
    "        self.poi_stats = {\n",
    "            'max_vals': poi_max_vals.tolist(),\n",
    "            'mean_vals': (poi_sums / max(poi_counts, 1)).tolist()  # Evita divisão por zero\n",
    "        }\n",
    "        \n",
    "        # Salva info de tempo\n",
    "        self.max_day = max_day\n",
    "        \n",
    "        print(f\"✅ Estatísticas calculadas:\")\n",
    "        print(f\"   📍 Coordenadas: x[{all_coords[:,0].min():.0f}, {all_coords[:,0].max():.0f}], y[{all_coords[:,1].min():.0f}, {all_coords[:,1].max():.0f}] → [0,1]\")\n",
    "        print(f\"   📅 Dias: [0, {max_day}]\")\n",
    "        print(f\"   🏢 POIs: max_sum={poi_max_vals.max():.0f}, mean_sum={poi_sums.max()/max(poi_counts,1):.2f}\")\n",
    "        \n",
    "        if problematic_chunks > 0 or total_problematic_pois > 0:\n",
    "            print(f\"   🔧 Limpeza POI: {problematic_chunks} chunks com problemas, {total_problematic_pois:,} POIs corrigidos\")\n",
    "        else:\n",
    "            print(f\"   ✅ POIs: Nenhum problema encontrado!\")\n",
    "    \n",
    "    def normalize_coordinates(self, x_vals, y_vals):\n",
    "        \"\"\"Normaliza coordenadas para [0, 1]\"\"\"\n",
    "        coords = np.column_stack([x_vals, y_vals])\n",
    "        coords_norm = self.coords_scaler.transform(coords)\n",
    "        return coords_norm[:, 0], coords_norm[:, 1]\n",
    "    \n",
    "    def normalize_time(self, d_vals, t_vals):\n",
    "        \"\"\"\n",
    "        Normaliza tempo:\n",
    "        - Dias: linear [0,1]  \n",
    "        - Timeslots: circular (sin/cos)\n",
    "        \"\"\"\n",
    "        # Dias: normalização linear\n",
    "        d_norm = d_vals.astype(np.float32) / self.max_day\n",
    "        \n",
    "        # Timeslots: normalização circular (importante para horários!)\n",
    "        t_sin = np.sin(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        t_cos = np.cos(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        \n",
    "        return d_norm, t_sin, t_cos\n",
    "    \n",
    "    def normalize_pois_robust(self, poi_lists):\n",
    "        \"\"\"\n",
    "        Normaliza POIs com estratégia robusta - VERSÃO MELHORADA\n",
    "        \"\"\"\n",
    "        # Limpeza preventiva\n",
    "        valid_poi_lists = []\n",
    "        \n",
    "        for poi_list in poi_lists:\n",
    "            # Trata casos problemáticos\n",
    "            if poi_list is None or not isinstance(poi_list, (list, tuple)):\n",
    "                valid_poi_lists.append([0.0] * 85)\n",
    "                continue\n",
    "            \n",
    "            # Converte e ajusta tamanho\n",
    "            if len(poi_list) < 85:\n",
    "                poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "            elif len(poi_list) > 85:\n",
    "                poi_fixed = list(poi_list)[:85]\n",
    "            else:\n",
    "                poi_fixed = list(poi_list)\n",
    "            \n",
    "            # Converte para float e limpa\n",
    "            poi_cleaned = []\n",
    "            for val in poi_fixed:\n",
    "                try:\n",
    "                    float_val = float(val) if val is not None else 0.0\n",
    "                    if not np.isfinite(float_val):\n",
    "                        float_val = 0.0\n",
    "                    poi_cleaned.append(max(0.0, float_val))  # Remove negativos também\n",
    "                except (ValueError, TypeError):\n",
    "                    poi_cleaned.append(0.0)\n",
    "            \n",
    "            valid_poi_lists.append(poi_cleaned)\n",
    "        \n",
    "        # Converte para array\n",
    "        poi_array = np.array(valid_poi_lists, dtype=np.float32)\n",
    "        \n",
    "        # Aplica log1p e normalização como antes\n",
    "        poi_log = np.log1p(poi_array)\n",
    "        \n",
    "        max_vals = np.array(self.poi_stats['max_vals'])\n",
    "        max_vals_log = np.log1p(max_vals)\n",
    "        max_vals_log[max_vals_log == 0] = 1.0\n",
    "        \n",
    "        poi_normalized = poi_log / max_vals_log[np.newaxis, :]\n",
    "        poi_normalized = np.clip(poi_normalized, 0.0, 1.0)\n",
    "        \n",
    "        return poi_normalized.tolist()\n",
    "    \n",
    "    def normalize_cities(self, city_vals):\n",
    "        \"\"\"Converte cidades para encoding numérico\"\"\"\n",
    "        city_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "        return np.array([city_mapping[c] for c in city_vals], dtype=np.int8)\n",
    "    \n",
    "    def process_and_save(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Processa e salva dataset normalizado chunk por chunk.\n",
    "        \"\"\"\n",
    "        print(\"🔄 Normalizando e salvando dados...\")\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        total_batches = (pf.metadata.num_rows + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        writer = None\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for batch in tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                         total=total_batches, desc=\"Normalizando\"):\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # Extrai dados originais\n",
    "            uid_vals = table.column(\"uid\").to_numpy()\n",
    "            d_vals = table.column(\"d\").to_numpy() \n",
    "            t_vals = table.column(\"t\").to_numpy()\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            city_vals = table.column(\"city\").to_pylist()\n",
    "            poi_lists = table.column(\"POI\").to_pylist()\n",
    "            \n",
    "            # Aplica normalizações\n",
    "            x_norm, y_norm = self.normalize_coordinates(x_vals, y_vals)\n",
    "            d_norm, t_sin, t_cos = self.normalize_time(d_vals, t_vals)\n",
    "            poi_norm = self.normalize_pois_robust(poi_lists)\n",
    "            city_encoded = self.normalize_cities(city_vals)\n",
    "            \n",
    "            # Cria novo schema\n",
    "            if writer is None:\n",
    "                new_schema = pa.schema([\n",
    "                    pa.field(\"uid\", pa.int64()),\n",
    "                    pa.field(\"d_orig\", pa.uint8()),  # Original para referência  \n",
    "                    pa.field(\"t_orig\", pa.uint8()),  # Original para referência\n",
    "                    pa.field(\"d_norm\", pa.float32()),  # Normalizado [0,1]\n",
    "                    pa.field(\"t_sin\", pa.float32()),   # Circular sin\n",
    "                    pa.field(\"t_cos\", pa.float32()),   # Circular cos  \n",
    "                    pa.field(\"x_norm\", pa.float32()),  # Normalizado [0,1]\n",
    "                    pa.field(\"y_norm\", pa.float32()),  # Normalizado [0,1]\n",
    "                    pa.field(\"city\", pa.string()),     # Original\n",
    "                    pa.field(\"city_encoded\", pa.int8()),  # Encoded\n",
    "                    pa.field(\"POI_norm\", pa.list_(pa.float32()))  # Normalizado\n",
    "                ])\n",
    "                \n",
    "                # Adiciona metadados\n",
    "                metadata = {\n",
    "                    b\"normalization_info\": json.dumps(self.normalization_info).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_min\": json.dumps(self.coords_scaler.data_min_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_scale\": json.dumps(self.coords_scaler.scale_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"poi_stats\": json.dumps(self.poi_stats).encode(\"utf-8\"),\n",
    "                    b\"max_day\": str(self.max_day).encode(\"utf-8\")\n",
    "                }\n",
    "                new_schema = new_schema.with_metadata(metadata)\n",
    "                \n",
    "                writer = pq.ParquetWriter(self.output_path, new_schema, compression='snappy')\n",
    "            \n",
    "            # Cria nova tabela\n",
    "            new_table = pa.table({\n",
    "                \"uid\": uid_vals,\n",
    "                \"d_orig\": d_vals,\n",
    "                \"t_orig\": t_vals, \n",
    "                \"d_norm\": d_norm,\n",
    "                \"t_sin\": t_sin,\n",
    "                \"t_cos\": t_cos,\n",
    "                \"x_norm\": x_norm,\n",
    "                \"y_norm\": y_norm,\n",
    "                \"city\": city_vals,\n",
    "                \"city_encoded\": city_encoded,\n",
    "                \"POI_norm\": poi_norm\n",
    "            }, schema=new_schema)\n",
    "            \n",
    "            writer.write_table(new_table)\n",
    "            processed_rows += len(uid_vals)\n",
    "        \n",
    "        if writer:\n",
    "            writer.close()\n",
    "            \n",
    "        print(f\"✅ Dataset normalizado salvo em: {self.output_path}\")\n",
    "        print(f\"📊 Linhas processadas: {processed_rows:,}\")\n",
    "        print(f\"💾 Tamanho do arquivo: {self.output_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "    \n",
    "    def run(self, chunk_size: int = 50_000):\n",
    "        \"\"\"Executa normalização completa\"\"\"\n",
    "        print(\"🎯 Iniciando normalização do dataset HuMob\")\n",
    "        print(f\"📂 Input: {self.input_path}\")\n",
    "        print(f\"📁 Output: {self.output_path}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Passo 1: Calcular estatísticas (TODAS, não só debug)\n",
    "        self.compute_statistics_robust(chunk_size)\n",
    "        \n",
    "        # Passo 2: Processar e salvar\n",
    "        self.process_and_save(chunk_size)\n",
    "        \n",
    "        print(\"\\n🎉 Normalização concluída com sucesso!\")\n",
    "        print(\"\\n📋 Próximos passos:\")\n",
    "        print(\"   1. Teste o novo dataset com check_normalized_data()\")\n",
    "        print(\"   2. Atualize seu clnn.ipynb para usar dados normalizados\")\n",
    "        print(\"   3. Remova normalizações inline do código de treino\")\n",
    "\n",
    "\n",
    "def check_normalized_data(file_path: str, n_samples: int = 1000):\n",
    "    \"\"\"\n",
    "    Verifica se os dados foram normalizados corretamente.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Verificando dados normalizados em: {file_path}\")\n",
    "    \n",
    "    # Lê amostra\n",
    "    df = pq.read_table(file_path).slice(0, n_samples).to_pandas()\n",
    "    \n",
    "    print(f\"\\n📊 Estatísticas da amostra ({len(df):,} linhas):\")\n",
    "    print(f\"   📍 Coordenadas:\")\n",
    "    print(f\"      x_norm: [{df['x_norm'].min():.3f}, {df['x_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      y_norm: [{df['y_norm'].min():.3f}, {df['y_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    \n",
    "    print(f\"   📅 Tempo:\")\n",
    "    print(f\"      d_norm: [{df['d_norm'].min():.3f}, {df['d_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      t_sin: [{df['t_sin'].min():.3f}, {df['t_sin'].max():.3f}] (deve ser [-1,1])\")\n",
    "    print(f\"      t_cos: [{df['t_cos'].min():.3f}, {df['t_cos'].max():.3f}] (deve ser [-1,1])\")\n",
    "    \n",
    "    print(f\"   🏢 POIs:\")\n",
    "    poi_sample = np.array(df['POI_norm'].iloc[0])\n",
    "    print(f\"      Exemplo: [{poi_sample.min():.3f}, {poi_sample.max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      Dimensão: {len(poi_sample)} (deve ser 85)\")\n",
    "    \n",
    "    print(f\"   🏙️ Cidades:\")\n",
    "    print(f\"      Encoded: {sorted(df['city_encoded'].unique())} (deve ser [0,1,2,3])\")\n",
    "    print(f\"      Original: {sorted(df['city'].unique())}\")\n",
    "    \n",
    "    # Verifica metadados\n",
    "    pf = pq.ParquetFile(file_path)\n",
    "    metadata = pf.schema_arrow.metadata\n",
    "    if b\"normalization_info\" in metadata:\n",
    "        norm_info = json.loads(metadata[b\"normalization_info\"])\n",
    "        print(f\"\\n✅ Metadados de normalização encontrados:\")\n",
    "        for key, value in norm_info[\"strategy\"].items():\n",
    "            print(f\"      {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Metadados de normalização não encontrados!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurações\n",
    "    INPUT_FILE = \"humob_all_cities_dpsk.parquet\"  # Seu arquivo original\n",
    "    OUTPUT_FILE = \"humob_all_cities_normalized.parquet\"  # Arquivo normalizado\n",
    "    \n",
    "    # Verifica se arquivo de entrada existe\n",
    "    if not Path(INPUT_FILE).exists():\n",
    "        print(f\"❌ Arquivo não encontrado: {INPUT_FILE}\")\n",
    "        print(\"📝 Certifique-se de que o arquivo foi gerado pelo data_loader.ipynb\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Executa normalização\n",
    "    normalizer = HuMobNormalizer(INPUT_FILE, OUTPUT_FILE)\n",
    "    normalizer.run(chunk_size=50_000)\n",
    "    \n",
    "    # Verifica resultado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    check_normalized_data(OUTPUT_FILE, n_samples=5000)\n",
    "    \n",
    "    print(f\"\\n🎯 PRONTO! Agora você pode:\")\n",
    "    print(f\"   1. Usar '{OUTPUT_FILE}' no seu clnn.ipynb\")\n",
    "    print(f\"   2. Remover todas as normalizações inline do código\")\n",
    "    print(f\"   3. Usar diretamente as colunas *_norm nas features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d71cdae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

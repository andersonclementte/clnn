{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed72780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Iniciando TESTE de normalização do dataset HuMob\n",
      "📂 Input: humob_all_cities_dpsk.parquet\n",
      "📁 Output: humob_sample_normalized.parquet\n",
      "🔬 Amostra: 20 chunks (~1,000,000 linhas)\n",
      "============================================================\n",
      "🧪 Calculando estatísticas da AMOSTRA (primeiros 20 chunks)...\n",
      "📁 Dataset completo: 162,785,736 linhas\n",
      "🧪 Processando amostra: ~1,000,000 linhas (20 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUG Chunk 0:\n",
      "   Row 0: OK, range=[0.0, 19.0]\n",
      "   Row 1: OK, range=[0.0, 19.0]\n",
      "   Row 2: OK, range=[0.0, 6.0]\n",
      "   Row 3: OK, range=[0.0, 6.0]\n",
      "   Row 4: OK, range=[0.0, 8.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra:   5%|▌         | 1/20 [00:05<01:49,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUG Chunk 1:\n",
      "   Row 0: OK, range=[0.0, 3.0]\n",
      "   Row 1: OK, range=[0.0, 3.0]\n",
      "   Row 2: OK, range=[0.0, 4.0]\n",
      "   Row 3: OK, range=[0.0, 8.0]\n",
      "   Row 4: OK, range=[0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra:  10%|█         | 2/20 [00:11<01:43,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 DEBUG Chunk 2:\n",
      "   Row 0: OK, range=[0.0, 8.0]\n",
      "   Row 1: OK, range=[0.0, 14.0]\n",
      "   Row 2: OK, range=[0.0, 14.0]\n",
      "   Row 3: OK, range=[0.0, 3.0]\n",
      "   Row 4: OK, range=[0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra: 100%|██████████| 20/20 [01:53<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Estatísticas da AMOSTRA calculadas:\n",
      "   📍 Coordenadas: x[1, 200], y[1, 200] → [0,1]\n",
      "   📅 Dias: [0, 74]\n",
      "   🏢 POIs: max_sum=236, mean_sum=6.07\n",
      "   📊 Linhas processadas: 1,000,000\n",
      "   ✅ POIs: Nenhum problema encontrado!\n",
      "🔄 Normalizando e salvando AMOSTRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizando amostra: 100%|██████████| 20/20 [02:07<00:00,  6.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ AMOSTRA normalizada salva em: humob_sample_normalized.parquet\n",
      "📊 Linhas processadas: 1,000,000\n",
      "💾 Tamanho do arquivo: 27.7 MB\n",
      "\n",
      "🧪 TESTE concluído com sucesso!\n",
      "\n",
      "📋 Próximos passos:\n",
      "   1. Verifique se o arquivo de teste está correto\n",
      "   2. Se OK, execute a versão completa\n",
      "   3. Se problemas, ajuste parâmetros e teste novamente\n",
      "\n",
      "============================================================\n",
      "🔍 Verificando AMOSTRA normalizada em: humob_sample_normalized.parquet\n",
      "\n",
      "📊 Estatísticas da amostra (5,000 linhas):\n",
      "   📍 Coordenadas:\n",
      "      x_norm: [0.226, 0.824] (deve ser [0,1])\n",
      "      y_norm: [0.040, 0.719] (deve ser [0,1])\n",
      "   📅 Tempo:\n",
      "      d_norm: [0.000, 1.000] (deve ser [0,1])\n",
      "      t_sin: [-1.000, 1.000] (deve ser [-1,1])\n",
      "      t_cos: [-1.000, 1.000] (deve ser [-1,1])\n",
      "   🏢 POIs:\n",
      "      Exemplo: [0.000, 0.756] (deve ser [0,1])\n",
      "      Dimensão: 85 (deve ser 85)\n",
      "   🧪 Valores NaN: ✅ Nenhum\n",
      "   🏙️ Cidades:\n",
      "      Encoded: [np.int8(0)] (deve ser [0,1,2,3])\n",
      "      Original: ['A']\n",
      "\n",
      "🧪 TESTE COMPLETO!\n",
      "   📄 Arquivo de teste: humob_sample_normalized.parquet\n",
      "   📊 ~1,000,000 linhas processadas\n",
      "\n",
      "✅ Se tudo estiver OK, execute a versão completa:\n",
      "   python normalize_humob_data_fixed.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "🧪 Teste de Normalização - VERSÃO AMOSTRA\n",
    "========================================\n",
    "\n",
    "Testa a normalização em uma pequena amostra do dataset HuMob.\n",
    "Perfeito para debugar e validar antes de processar os 162M de linhas!\n",
    "\n",
    "Uso:\n",
    "    python normalize_sample.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HuMobNormalizerSample:\n",
    "    \"\"\"\n",
    "    Versão de teste que normaliza apenas uma amostra do dataset.\n",
    "    \n",
    "    🧪 Use isso para:\n",
    "    - Validar se o processo funciona\n",
    "    - Debugar problemas de POIs\n",
    "    - Ajustar hiperparâmetros\n",
    "    - Testar mudanças rapidamente\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path: str, output_path: str, max_chunks: int = 10):\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.max_chunks = max_chunks  # 🧪 LIMITA QUANTOS CHUNKS PROCESSAR\n",
    "        \n",
    "        # Scalers que serão salvos para uso posterior\n",
    "        self.coords_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.poi_stats = {}  # Para log1p + normalização manual\n",
    "        \n",
    "        # Metadados para salvar junto\n",
    "        self.normalization_info = {\n",
    "            \"version\": \"1.0-SAMPLE\",\n",
    "            \"strategy\": {\n",
    "                \"coordinates\": \"MinMaxScaler [0,1]\",\n",
    "                \"days\": \"Linear [0,1]\", \n",
    "                \"timeslots\": \"Circular sin/cos\",\n",
    "                \"pois\": \"log1p + per-column normalization\",\n",
    "                \"cities\": \"Label encoding A=0, B=1, C=2, D=3\"\n",
    "            },\n",
    "            \"sample_info\": {\n",
    "                \"max_chunks\": max_chunks,\n",
    "                \"chunk_size\": 50000,\n",
    "                \"estimated_rows\": max_chunks * 50000\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def compute_statistics_sample(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Calcula estatísticas apenas dos primeiros chunks (AMOSTRA).\n",
    "        \"\"\"\n",
    "        print(f\"🧪 Calculando estatísticas da AMOSTRA (primeiros {self.max_chunks} chunks)...\")\n",
    "        \n",
    "        # Acumuladores para coordenadas\n",
    "        all_coords = []\n",
    "        \n",
    "        # Acumuladores para POIs (por coluna) - COM LIMPEZA\n",
    "        poi_sums = np.zeros(85)\n",
    "        poi_counts = 0\n",
    "        poi_max_vals = np.zeros(85)\n",
    "        \n",
    "        # Ranges de tempo\n",
    "        max_day = 0\n",
    "        \n",
    "        # Contadores para debug\n",
    "        problematic_chunks = 0\n",
    "        total_problematic_pois = 0\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        print(f\"📁 Dataset completo: {pf.metadata.num_rows:,} linhas\")\n",
    "        print(f\"🧪 Processando amostra: ~{self.max_chunks * chunk_size:,} linhas ({self.max_chunks} chunks)\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                                      total=self.max_chunks, desc=\"Stats da amostra\")):\n",
    "            \n",
    "            # 🧪 PARA APÓS max_chunks\n",
    "            if i >= self.max_chunks:\n",
    "                break\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # 1. COORDENADAS (como sempre)\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            coords = np.column_stack([x_vals, y_vals])\n",
    "            all_coords.append(coords)\n",
    "            \n",
    "            # 2. DIAS (como sempre)\n",
    "            d_vals = table.column(\"d\").to_numpy()\n",
    "            max_day = max(max_day, d_vals.max())\n",
    "            \n",
    "            # 3. POIs COM DEBUG DETALHADO\n",
    "            try:\n",
    "                poi_lists = table.column(\"POI\").to_pylist()\n",
    "                \n",
    "                # 🔍 DEBUG DETALHADO NOS PRIMEIROS CHUNKS\n",
    "                if i < 3:  # Debug detalhado nos 3 primeiros\n",
    "                    print(f\"\\n🔍 DEBUG Chunk {i}:\")\n",
    "                    sample_pois = poi_lists[:5]  # Primeiros 5 POIs do chunk\n",
    "                    for j, poi in enumerate(sample_pois):\n",
    "                        if poi is None:\n",
    "                            print(f\"   Row {j}: None\")\n",
    "                        elif not isinstance(poi, (list, tuple)):\n",
    "                            print(f\"   Row {j}: type={type(poi)}\")\n",
    "                        elif len(poi) != 85:\n",
    "                            print(f\"   Row {j}: len={len(poi)} (expected 85)\")\n",
    "                        else:\n",
    "                            poi_arr = np.array(poi, dtype=np.float32)\n",
    "                            if not np.isfinite(poi_arr).all():\n",
    "                                print(f\"   Row {j}: contains NaN/Inf\")\n",
    "                            else:\n",
    "                                print(f\"   Row {j}: OK, range=[{poi_arr.min():.1f}, {poi_arr.max():.1f}]\")\n",
    "                \n",
    "                # Limpa POIs problemáticos\n",
    "                cleaned_poi_lists = []\n",
    "                chunk_problems = 0\n",
    "                \n",
    "                for poi_list in poi_lists:\n",
    "                    if poi_list is None:\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif not isinstance(poi_list, (list, tuple)):\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif len(poi_list) != 85:\n",
    "                        # Ajusta tamanho\n",
    "                        if len(poi_list) < 85:\n",
    "                            poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "                        else:\n",
    "                            poi_fixed = list(poi_list)[:85]\n",
    "                        \n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_fixed:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                        chunk_problems += 1\n",
    "                    else:\n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_list:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                \n",
    "                # Converte para array e processa\n",
    "                poi_array = np.array(cleaned_poi_lists, dtype=np.float32)\n",
    "                \n",
    "                # Limpa qualquer NaN/Inf restante\n",
    "                poi_array = np.nan_to_num(poi_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # Acumula estatísticas\n",
    "                poi_sums += poi_array.sum(axis=0)\n",
    "                poi_counts += poi_array.shape[0]\n",
    "                poi_max_vals = np.maximum(poi_max_vals, poi_array.max(axis=0))\n",
    "                \n",
    "                # Conta problemas\n",
    "                if chunk_problems > 0:\n",
    "                    problematic_chunks += 1\n",
    "                    total_problematic_pois += chunk_problems\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erro no chunk {i}: {str(e)}\")\n",
    "                problematic_chunks += 1\n",
    "        \n",
    "        # Calcula scalers das coordenadas\n",
    "        all_coords = np.vstack(all_coords)\n",
    "        self.coords_scaler.fit(all_coords)\n",
    "        \n",
    "        # Salva estatísticas POI\n",
    "        self.poi_stats = {\n",
    "            'max_vals': poi_max_vals.tolist(),\n",
    "            'mean_vals': (poi_sums / max(poi_counts, 1)).tolist()  # Evita divisão por zero\n",
    "        }\n",
    "        \n",
    "        # Salva info de tempo\n",
    "        self.max_day = max_day\n",
    "        \n",
    "        print(f\"\\n✅ Estatísticas da AMOSTRA calculadas:\")\n",
    "        print(f\"   📍 Coordenadas: x[{all_coords[:,0].min():.0f}, {all_coords[:,0].max():.0f}], y[{all_coords[:,1].min():.0f}, {all_coords[:,1].max():.0f}] → [0,1]\")\n",
    "        print(f\"   📅 Dias: [0, {max_day}]\")\n",
    "        print(f\"   🏢 POIs: max_sum={poi_max_vals.max():.0f}, mean_sum={poi_sums.max()/max(poi_counts,1):.2f}\")\n",
    "        print(f\"   📊 Linhas processadas: {poi_counts:,}\")\n",
    "        \n",
    "        if problematic_chunks > 0 or total_problematic_pois > 0:\n",
    "            print(f\"   🔧 Limpeza POI: {problematic_chunks} chunks com problemas, {total_problematic_pois:,} POIs corrigidos\")\n",
    "        else:\n",
    "            print(f\"   ✅ POIs: Nenhum problema encontrado!\")\n",
    "    \n",
    "    def normalize_coordinates(self, x_vals, y_vals):\n",
    "        \"\"\"Normaliza coordenadas para [0, 1]\"\"\"\n",
    "        coords = np.column_stack([x_vals, y_vals])\n",
    "        coords_norm = self.coords_scaler.transform(coords)\n",
    "        return coords_norm[:, 0], coords_norm[:, 1]\n",
    "    \n",
    "    def normalize_time(self, d_vals, t_vals):\n",
    "        \"\"\"\n",
    "        Normaliza tempo:\n",
    "        - Dias: linear [0,1]  \n",
    "        - Timeslots: circular (sin/cos)\n",
    "        \"\"\"\n",
    "        # Dias: normalização linear\n",
    "        d_norm = d_vals.astype(np.float32) / self.max_day\n",
    "        \n",
    "        # Timeslots: normalização circular (importante para horários!)\n",
    "        t_sin = np.sin(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        t_cos = np.cos(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        \n",
    "        return d_norm, t_sin, t_cos\n",
    "    \n",
    "    def normalize_pois_robust(self, poi_lists):\n",
    "        \"\"\"Normaliza POIs com estratégia robusta\"\"\"\n",
    "        valid_poi_lists = []\n",
    "        \n",
    "        for poi_list in poi_lists:\n",
    "            if poi_list is None or not isinstance(poi_list, (list, tuple)):\n",
    "                valid_poi_lists.append([0.0] * 85)\n",
    "                continue\n",
    "            \n",
    "            # Converte e ajusta tamanho\n",
    "            if len(poi_list) < 85:\n",
    "                poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "            elif len(poi_list) > 85:\n",
    "                poi_fixed = list(poi_list)[:85]\n",
    "            else:\n",
    "                poi_fixed = list(poi_list)\n",
    "            \n",
    "            # Converte para float e limpa\n",
    "            poi_cleaned = []\n",
    "            for val in poi_fixed:\n",
    "                try:\n",
    "                    float_val = float(val) if val is not None else 0.0\n",
    "                    if not np.isfinite(float_val):\n",
    "                        float_val = 0.0\n",
    "                    poi_cleaned.append(max(0.0, float_val))\n",
    "                except (ValueError, TypeError):\n",
    "                    poi_cleaned.append(0.0)\n",
    "            \n",
    "            valid_poi_lists.append(poi_cleaned)\n",
    "        \n",
    "        # Converte para array\n",
    "        poi_array = np.array(valid_poi_lists, dtype=np.float32)\n",
    "        \n",
    "        # Aplica log1p e normalização\n",
    "        poi_log = np.log1p(poi_array)\n",
    "        \n",
    "        max_vals = np.array(self.poi_stats['max_vals'])\n",
    "        max_vals_log = np.log1p(max_vals)\n",
    "        max_vals_log[max_vals_log == 0] = 1.0\n",
    "        \n",
    "        poi_normalized = poi_log / max_vals_log[np.newaxis, :]\n",
    "        poi_normalized = np.clip(poi_normalized, 0.0, 1.0)\n",
    "        \n",
    "        return poi_normalized.tolist()\n",
    "    \n",
    "    def normalize_cities(self, city_vals):\n",
    "        \"\"\"Converte cidades para encoding numérico\"\"\"\n",
    "        city_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "        return np.array([city_mapping[c] for c in city_vals], dtype=np.int8)\n",
    "    \n",
    "    def process_and_save_sample(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Processa e salva apenas a AMOSTRA.\n",
    "        \"\"\"\n",
    "        print(f\"🔄 Normalizando e salvando AMOSTRA...\")\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        \n",
    "        writer = None\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                                      total=self.max_chunks, desc=\"Normalizando amostra\")):\n",
    "            \n",
    "            # 🧪 PARA APÓS max_chunks\n",
    "            if i >= self.max_chunks:\n",
    "                break\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # Extrai dados originais\n",
    "            uid_vals = table.column(\"uid\").to_numpy()\n",
    "            d_vals = table.column(\"d\").to_numpy() \n",
    "            t_vals = table.column(\"t\").to_numpy()\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            city_vals = table.column(\"city\").to_pylist()\n",
    "            poi_lists = table.column(\"POI\").to_pylist()\n",
    "            \n",
    "            # Aplica normalizações\n",
    "            x_norm, y_norm = self.normalize_coordinates(x_vals, y_vals)\n",
    "            d_norm, t_sin, t_cos = self.normalize_time(d_vals, t_vals)\n",
    "            poi_norm = self.normalize_pois_robust(poi_lists)\n",
    "            city_encoded = self.normalize_cities(city_vals)\n",
    "            \n",
    "            # Cria novo schema\n",
    "            if writer is None:\n",
    "                new_schema = pa.schema([\n",
    "                    pa.field(\"uid\", pa.int64()),\n",
    "                    pa.field(\"d_orig\", pa.uint8()),\n",
    "                    pa.field(\"t_orig\", pa.uint8()),\n",
    "                    pa.field(\"d_norm\", pa.float32()),\n",
    "                    pa.field(\"t_sin\", pa.float32()),\n",
    "                    pa.field(\"t_cos\", pa.float32()),\n",
    "                    pa.field(\"x_norm\", pa.float32()),\n",
    "                    pa.field(\"y_norm\", pa.float32()),\n",
    "                    pa.field(\"city\", pa.string()),\n",
    "                    pa.field(\"city_encoded\", pa.int8()),\n",
    "                    pa.field(\"POI_norm\", pa.list_(pa.float32()))\n",
    "                ])\n",
    "                \n",
    "                # Adiciona metadados\n",
    "                metadata = {\n",
    "                    b\"normalization_info\": json.dumps(self.normalization_info).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_min\": json.dumps(self.coords_scaler.data_min_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_scale\": json.dumps(self.coords_scaler.scale_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"poi_stats\": json.dumps(self.poi_stats).encode(\"utf-8\"),\n",
    "                    b\"max_day\": str(self.max_day).encode(\"utf-8\")\n",
    "                }\n",
    "                new_schema = new_schema.with_metadata(metadata)\n",
    "                \n",
    "                writer = pq.ParquetWriter(self.output_path, new_schema, compression='snappy')\n",
    "            \n",
    "            # Cria nova tabela\n",
    "            new_table = pa.table({\n",
    "                \"uid\": uid_vals,\n",
    "                \"d_orig\": d_vals,\n",
    "                \"t_orig\": t_vals, \n",
    "                \"d_norm\": d_norm,\n",
    "                \"t_sin\": t_sin,\n",
    "                \"t_cos\": t_cos,\n",
    "                \"x_norm\": x_norm,\n",
    "                \"y_norm\": y_norm,\n",
    "                \"city\": city_vals,\n",
    "                \"city_encoded\": city_encoded,\n",
    "                \"POI_norm\": poi_norm\n",
    "            }, schema=new_schema)\n",
    "            \n",
    "            writer.write_table(new_table)\n",
    "            processed_rows += len(uid_vals)\n",
    "        \n",
    "        if writer:\n",
    "            writer.close()\n",
    "            \n",
    "        print(f\"✅ AMOSTRA normalizada salva em: {self.output_path}\")\n",
    "        print(f\"📊 Linhas processadas: {processed_rows:,}\")\n",
    "        print(f\"💾 Tamanho do arquivo: {self.output_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "    \n",
    "    def run(self, chunk_size: int = 50_000):\n",
    "        \"\"\"Executa normalização da AMOSTRA\"\"\"\n",
    "        print(\"🧪 Iniciando TESTE de normalização do dataset HuMob\")\n",
    "        print(f\"📂 Input: {self.input_path}\")\n",
    "        print(f\"📁 Output: {self.output_path}\")\n",
    "        print(f\"🔬 Amostra: {self.max_chunks} chunks (~{self.max_chunks * chunk_size:,} linhas)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Passo 1: Calcular estatísticas da amostra\n",
    "        self.compute_statistics_sample(chunk_size)\n",
    "        \n",
    "        # Passo 2: Processar e salvar amostra\n",
    "        self.process_and_save_sample(chunk_size)\n",
    "        \n",
    "        print(\"\\n🧪 TESTE concluído com sucesso!\")\n",
    "        print(\"\\n📋 Próximos passos:\")\n",
    "        print(\"   1. Verifique se o arquivo de teste está correto\")\n",
    "        print(\"   2. Se OK, execute a versão completa\")\n",
    "        print(\"   3. Se problemas, ajuste parâmetros e teste novamente\")\n",
    "\n",
    "\n",
    "def check_normalized_sample(file_path: str, n_samples: int = 1000):\n",
    "    \"\"\"Verifica se a amostra foi normalizada corretamente.\"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"❌ Arquivo não encontrado: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🔍 Verificando AMOSTRA normalizada em: {file_path}\")\n",
    "    \n",
    "    # Lê amostra\n",
    "    df = pq.read_table(file_path).slice(0, n_samples).to_pandas()\n",
    "    \n",
    "    print(f\"\\n📊 Estatísticas da amostra ({len(df):,} linhas):\")\n",
    "    print(f\"   📍 Coordenadas:\")\n",
    "    print(f\"      x_norm: [{df['x_norm'].min():.3f}, {df['x_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      y_norm: [{df['y_norm'].min():.3f}, {df['y_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    \n",
    "    print(f\"   📅 Tempo:\")\n",
    "    print(f\"      d_norm: [{df['d_norm'].min():.3f}, {df['d_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      t_sin: [{df['t_sin'].min():.3f}, {df['t_sin'].max():.3f}] (deve ser [-1,1])\")\n",
    "    print(f\"      t_cos: [{df['t_cos'].min():.3f}, {df['t_cos'].max():.3f}] (deve ser [-1,1])\")\n",
    "    \n",
    "    print(f\"   🏢 POIs:\")\n",
    "    poi_sample = np.array(df['POI_norm'].iloc[0])\n",
    "    print(f\"      Exemplo: [{poi_sample.min():.3f}, {poi_sample.max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      Dimensão: {len(poi_sample)} (deve ser 85)\")\n",
    "    \n",
    "    # Verifica NaNs\n",
    "    has_nan = df.isnull().any().any()\n",
    "    print(f\"   🧪 Valores NaN: {'❌ Encontrados!' if has_nan else '✅ Nenhum'}\")\n",
    "    \n",
    "    print(f\"   🏙️ Cidades:\")\n",
    "    print(f\"      Encoded: {sorted(df['city_encoded'].unique())} (deve ser [0,1,2,3])\")\n",
    "    print(f\"      Original: {sorted(df['city'].unique())}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurações para TESTE\n",
    "    INPUT_FILE = \"humob_all_cities_dpsk.parquet\"\n",
    "    OUTPUT_FILE = \"humob_sample_normalized.parquet\"  # 🧪 Arquivo de teste\n",
    "    MAX_CHUNKS = 20  # 🧪 Apenas 20 chunks = ~1M linhas (2 minutos)\n",
    "    \n",
    "    # Verifica se arquivo de entrada existe\n",
    "    if not Path(INPUT_FILE).exists():\n",
    "        print(f\"❌ Arquivo não encontrado: {INPUT_FILE}\")\n",
    "        print(\"📝 Certifique-se de que o arquivo foi gerado pelo data_loader.ipynb\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Executa normalização de TESTE\n",
    "    normalizer = HuMobNormalizerSample(INPUT_FILE, OUTPUT_FILE, max_chunks=MAX_CHUNKS)\n",
    "    normalizer.run(chunk_size=50_000)\n",
    "    \n",
    "    # Verifica resultado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    check_normalized_sample(OUTPUT_FILE, n_samples=5000)\n",
    "    \n",
    "    print(f\"\\n🧪 TESTE COMPLETO!\")\n",
    "    print(f\"   📄 Arquivo de teste: {OUTPUT_FILE}\")\n",
    "    print(f\"   📊 ~{MAX_CHUNKS * 50000:,} linhas processadas\")\n",
    "    print(f\"\\n✅ Se tudo estiver OK, execute a versão completa:\")\n",
    "    print(f\"   python normalize_humob_data_fixed.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

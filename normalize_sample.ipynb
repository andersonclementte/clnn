{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed72780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Iniciando TESTE de normaliza√ß√£o do dataset HuMob\n",
      "üìÇ Input: humob_all_cities_dpsk.parquet\n",
      "üìÅ Output: humob_sample_normalized.parquet\n",
      "üî¨ Amostra: 20 chunks (~1,000,000 linhas)\n",
      "============================================================\n",
      "üß™ Calculando estat√≠sticas da AMOSTRA (primeiros 20 chunks)...\n",
      "üìÅ Dataset completo: 162,785,736 linhas\n",
      "üß™ Processando amostra: ~1,000,000 linhas (20 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG Chunk 0:\n",
      "   Row 0: OK, range=[0.0, 19.0]\n",
      "   Row 1: OK, range=[0.0, 19.0]\n",
      "   Row 2: OK, range=[0.0, 6.0]\n",
      "   Row 3: OK, range=[0.0, 6.0]\n",
      "   Row 4: OK, range=[0.0, 8.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra:   5%|‚ñå         | 1/20 [00:05<01:49,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG Chunk 1:\n",
      "   Row 0: OK, range=[0.0, 3.0]\n",
      "   Row 1: OK, range=[0.0, 3.0]\n",
      "   Row 2: OK, range=[0.0, 4.0]\n",
      "   Row 3: OK, range=[0.0, 8.0]\n",
      "   Row 4: OK, range=[0.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra:  10%|‚ñà         | 2/20 [00:11<01:43,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG Chunk 2:\n",
      "   Row 0: OK, range=[0.0, 8.0]\n",
      "   Row 1: OK, range=[0.0, 14.0]\n",
      "   Row 2: OK, range=[0.0, 14.0]\n",
      "   Row 3: OK, range=[0.0, 3.0]\n",
      "   Row 4: OK, range=[0.0, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stats da amostra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [01:53<00:00,  5.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Estat√≠sticas da AMOSTRA calculadas:\n",
      "   üìç Coordenadas: x[1, 200], y[1, 200] ‚Üí [0,1]\n",
      "   üìÖ Dias: [0, 74]\n",
      "   üè¢ POIs: max_sum=236, mean_sum=6.07\n",
      "   üìä Linhas processadas: 1,000,000\n",
      "   ‚úÖ POIs: Nenhum problema encontrado!\n",
      "üîÑ Normalizando e salvando AMOSTRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizando amostra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [02:07<00:00,  6.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AMOSTRA normalizada salva em: humob_sample_normalized.parquet\n",
      "üìä Linhas processadas: 1,000,000\n",
      "üíæ Tamanho do arquivo: 27.7 MB\n",
      "\n",
      "üß™ TESTE conclu√≠do com sucesso!\n",
      "\n",
      "üìã Pr√≥ximos passos:\n",
      "   1. Verifique se o arquivo de teste est√° correto\n",
      "   2. Se OK, execute a vers√£o completa\n",
      "   3. Se problemas, ajuste par√¢metros e teste novamente\n",
      "\n",
      "============================================================\n",
      "üîç Verificando AMOSTRA normalizada em: humob_sample_normalized.parquet\n",
      "\n",
      "üìä Estat√≠sticas da amostra (5,000 linhas):\n",
      "   üìç Coordenadas:\n",
      "      x_norm: [0.226, 0.824] (deve ser [0,1])\n",
      "      y_norm: [0.040, 0.719] (deve ser [0,1])\n",
      "   üìÖ Tempo:\n",
      "      d_norm: [0.000, 1.000] (deve ser [0,1])\n",
      "      t_sin: [-1.000, 1.000] (deve ser [-1,1])\n",
      "      t_cos: [-1.000, 1.000] (deve ser [-1,1])\n",
      "   üè¢ POIs:\n",
      "      Exemplo: [0.000, 0.756] (deve ser [0,1])\n",
      "      Dimens√£o: 85 (deve ser 85)\n",
      "   üß™ Valores NaN: ‚úÖ Nenhum\n",
      "   üèôÔ∏è Cidades:\n",
      "      Encoded: [np.int8(0)] (deve ser [0,1,2,3])\n",
      "      Original: ['A']\n",
      "\n",
      "üß™ TESTE COMPLETO!\n",
      "   üìÑ Arquivo de teste: humob_sample_normalized.parquet\n",
      "   üìä ~1,000,000 linhas processadas\n",
      "\n",
      "‚úÖ Se tudo estiver OK, execute a vers√£o completa:\n",
      "   python normalize_humob_data_fixed.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "üß™ Teste de Normaliza√ß√£o - VERS√ÉO AMOSTRA\n",
    "========================================\n",
    "\n",
    "Testa a normaliza√ß√£o em uma pequena amostra do dataset HuMob.\n",
    "Perfeito para debugar e validar antes de processar os 162M de linhas!\n",
    "\n",
    "Uso:\n",
    "    python normalize_sample.py\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class HuMobNormalizerSample:\n",
    "    \"\"\"\n",
    "    Vers√£o de teste que normaliza apenas uma amostra do dataset.\n",
    "    \n",
    "    üß™ Use isso para:\n",
    "    - Validar se o processo funciona\n",
    "    - Debugar problemas de POIs\n",
    "    - Ajustar hiperpar√¢metros\n",
    "    - Testar mudan√ßas rapidamente\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_path: str, output_path: str, max_chunks: int = 10):\n",
    "        self.input_path = Path(input_path)\n",
    "        self.output_path = Path(output_path)\n",
    "        self.max_chunks = max_chunks  # üß™ LIMITA QUANTOS CHUNKS PROCESSAR\n",
    "        \n",
    "        # Scalers que ser√£o salvos para uso posterior\n",
    "        self.coords_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        self.poi_stats = {}  # Para log1p + normaliza√ß√£o manual\n",
    "        \n",
    "        # Metadados para salvar junto\n",
    "        self.normalization_info = {\n",
    "            \"version\": \"1.0-SAMPLE\",\n",
    "            \"strategy\": {\n",
    "                \"coordinates\": \"MinMaxScaler [0,1]\",\n",
    "                \"days\": \"Linear [0,1]\", \n",
    "                \"timeslots\": \"Circular sin/cos\",\n",
    "                \"pois\": \"log1p + per-column normalization\",\n",
    "                \"cities\": \"Label encoding A=0, B=1, C=2, D=3\"\n",
    "            },\n",
    "            \"sample_info\": {\n",
    "                \"max_chunks\": max_chunks,\n",
    "                \"chunk_size\": 50000,\n",
    "                \"estimated_rows\": max_chunks * 50000\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def compute_statistics_sample(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Calcula estat√≠sticas apenas dos primeiros chunks (AMOSTRA).\n",
    "        \"\"\"\n",
    "        print(f\"üß™ Calculando estat√≠sticas da AMOSTRA (primeiros {self.max_chunks} chunks)...\")\n",
    "        \n",
    "        # Acumuladores para coordenadas\n",
    "        all_coords = []\n",
    "        \n",
    "        # Acumuladores para POIs (por coluna) - COM LIMPEZA\n",
    "        poi_sums = np.zeros(85)\n",
    "        poi_counts = 0\n",
    "        poi_max_vals = np.zeros(85)\n",
    "        \n",
    "        # Ranges de tempo\n",
    "        max_day = 0\n",
    "        \n",
    "        # Contadores para debug\n",
    "        problematic_chunks = 0\n",
    "        total_problematic_pois = 0\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        print(f\"üìÅ Dataset completo: {pf.metadata.num_rows:,} linhas\")\n",
    "        print(f\"üß™ Processando amostra: ~{self.max_chunks * chunk_size:,} linhas ({self.max_chunks} chunks)\")\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                                      total=self.max_chunks, desc=\"Stats da amostra\")):\n",
    "            \n",
    "            # üß™ PARA AP√ìS max_chunks\n",
    "            if i >= self.max_chunks:\n",
    "                break\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # 1. COORDENADAS (como sempre)\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            coords = np.column_stack([x_vals, y_vals])\n",
    "            all_coords.append(coords)\n",
    "            \n",
    "            # 2. DIAS (como sempre)\n",
    "            d_vals = table.column(\"d\").to_numpy()\n",
    "            max_day = max(max_day, d_vals.max())\n",
    "            \n",
    "            # 3. POIs COM DEBUG DETALHADO\n",
    "            try:\n",
    "                poi_lists = table.column(\"POI\").to_pylist()\n",
    "                \n",
    "                # üîç DEBUG DETALHADO NOS PRIMEIROS CHUNKS\n",
    "                if i < 3:  # Debug detalhado nos 3 primeiros\n",
    "                    print(f\"\\nüîç DEBUG Chunk {i}:\")\n",
    "                    sample_pois = poi_lists[:5]  # Primeiros 5 POIs do chunk\n",
    "                    for j, poi in enumerate(sample_pois):\n",
    "                        if poi is None:\n",
    "                            print(f\"   Row {j}: None\")\n",
    "                        elif not isinstance(poi, (list, tuple)):\n",
    "                            print(f\"   Row {j}: type={type(poi)}\")\n",
    "                        elif len(poi) != 85:\n",
    "                            print(f\"   Row {j}: len={len(poi)} (expected 85)\")\n",
    "                        else:\n",
    "                            poi_arr = np.array(poi, dtype=np.float32)\n",
    "                            if not np.isfinite(poi_arr).all():\n",
    "                                print(f\"   Row {j}: contains NaN/Inf\")\n",
    "                            else:\n",
    "                                print(f\"   Row {j}: OK, range=[{poi_arr.min():.1f}, {poi_arr.max():.1f}]\")\n",
    "                \n",
    "                # Limpa POIs problem√°ticos\n",
    "                cleaned_poi_lists = []\n",
    "                chunk_problems = 0\n",
    "                \n",
    "                for poi_list in poi_lists:\n",
    "                    if poi_list is None:\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif not isinstance(poi_list, (list, tuple)):\n",
    "                        cleaned_poi_lists.append([0.0] * 85)\n",
    "                        chunk_problems += 1\n",
    "                    elif len(poi_list) != 85:\n",
    "                        # Ajusta tamanho\n",
    "                        if len(poi_list) < 85:\n",
    "                            poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "                        else:\n",
    "                            poi_fixed = list(poi_list)[:85]\n",
    "                        \n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_fixed:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                        chunk_problems += 1\n",
    "                    else:\n",
    "                        # Converte valores para float seguro\n",
    "                        poi_cleaned = []\n",
    "                        for val in poi_list:\n",
    "                            try:\n",
    "                                float_val = float(val) if val is not None else 0.0\n",
    "                                if not np.isfinite(float_val) or float_val < 0:\n",
    "                                    float_val = 0.0\n",
    "                                poi_cleaned.append(float_val)\n",
    "                            except (ValueError, TypeError):\n",
    "                                poi_cleaned.append(0.0)\n",
    "                        \n",
    "                        cleaned_poi_lists.append(poi_cleaned)\n",
    "                \n",
    "                # Converte para array e processa\n",
    "                poi_array = np.array(cleaned_poi_lists, dtype=np.float32)\n",
    "                \n",
    "                # Limpa qualquer NaN/Inf restante\n",
    "                poi_array = np.nan_to_num(poi_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                \n",
    "                # Acumula estat√≠sticas\n",
    "                poi_sums += poi_array.sum(axis=0)\n",
    "                poi_counts += poi_array.shape[0]\n",
    "                poi_max_vals = np.maximum(poi_max_vals, poi_array.max(axis=0))\n",
    "                \n",
    "                # Conta problemas\n",
    "                if chunk_problems > 0:\n",
    "                    problematic_chunks += 1\n",
    "                    total_problematic_pois += chunk_problems\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro no chunk {i}: {str(e)}\")\n",
    "                problematic_chunks += 1\n",
    "        \n",
    "        # Calcula scalers das coordenadas\n",
    "        all_coords = np.vstack(all_coords)\n",
    "        self.coords_scaler.fit(all_coords)\n",
    "        \n",
    "        # Salva estat√≠sticas POI\n",
    "        self.poi_stats = {\n",
    "            'max_vals': poi_max_vals.tolist(),\n",
    "            'mean_vals': (poi_sums / max(poi_counts, 1)).tolist()  # Evita divis√£o por zero\n",
    "        }\n",
    "        \n",
    "        # Salva info de tempo\n",
    "        self.max_day = max_day\n",
    "        \n",
    "        print(f\"\\n‚úÖ Estat√≠sticas da AMOSTRA calculadas:\")\n",
    "        print(f\"   üìç Coordenadas: x[{all_coords[:,0].min():.0f}, {all_coords[:,0].max():.0f}], y[{all_coords[:,1].min():.0f}, {all_coords[:,1].max():.0f}] ‚Üí [0,1]\")\n",
    "        print(f\"   üìÖ Dias: [0, {max_day}]\")\n",
    "        print(f\"   üè¢ POIs: max_sum={poi_max_vals.max():.0f}, mean_sum={poi_sums.max()/max(poi_counts,1):.2f}\")\n",
    "        print(f\"   üìä Linhas processadas: {poi_counts:,}\")\n",
    "        \n",
    "        if problematic_chunks > 0 or total_problematic_pois > 0:\n",
    "            print(f\"   üîß Limpeza POI: {problematic_chunks} chunks com problemas, {total_problematic_pois:,} POIs corrigidos\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ POIs: Nenhum problema encontrado!\")\n",
    "    \n",
    "    def normalize_coordinates(self, x_vals, y_vals):\n",
    "        \"\"\"Normaliza coordenadas para [0, 1]\"\"\"\n",
    "        coords = np.column_stack([x_vals, y_vals])\n",
    "        coords_norm = self.coords_scaler.transform(coords)\n",
    "        return coords_norm[:, 0], coords_norm[:, 1]\n",
    "    \n",
    "    def normalize_time(self, d_vals, t_vals):\n",
    "        \"\"\"\n",
    "        Normaliza tempo:\n",
    "        - Dias: linear [0,1]  \n",
    "        - Timeslots: circular (sin/cos)\n",
    "        \"\"\"\n",
    "        # Dias: normaliza√ß√£o linear\n",
    "        d_norm = d_vals.astype(np.float32) / self.max_day\n",
    "        \n",
    "        # Timeslots: normaliza√ß√£o circular (importante para hor√°rios!)\n",
    "        t_sin = np.sin(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        t_cos = np.cos(2 * np.pi * t_vals / 48).astype(np.float32)\n",
    "        \n",
    "        return d_norm, t_sin, t_cos\n",
    "    \n",
    "    def normalize_pois_robust(self, poi_lists):\n",
    "        \"\"\"Normaliza POIs com estrat√©gia robusta\"\"\"\n",
    "        valid_poi_lists = []\n",
    "        \n",
    "        for poi_list in poi_lists:\n",
    "            if poi_list is None or not isinstance(poi_list, (list, tuple)):\n",
    "                valid_poi_lists.append([0.0] * 85)\n",
    "                continue\n",
    "            \n",
    "            # Converte e ajusta tamanho\n",
    "            if len(poi_list) < 85:\n",
    "                poi_fixed = list(poi_list) + [0.0] * (85 - len(poi_list))\n",
    "            elif len(poi_list) > 85:\n",
    "                poi_fixed = list(poi_list)[:85]\n",
    "            else:\n",
    "                poi_fixed = list(poi_list)\n",
    "            \n",
    "            # Converte para float e limpa\n",
    "            poi_cleaned = []\n",
    "            for val in poi_fixed:\n",
    "                try:\n",
    "                    float_val = float(val) if val is not None else 0.0\n",
    "                    if not np.isfinite(float_val):\n",
    "                        float_val = 0.0\n",
    "                    poi_cleaned.append(max(0.0, float_val))\n",
    "                except (ValueError, TypeError):\n",
    "                    poi_cleaned.append(0.0)\n",
    "            \n",
    "            valid_poi_lists.append(poi_cleaned)\n",
    "        \n",
    "        # Converte para array\n",
    "        poi_array = np.array(valid_poi_lists, dtype=np.float32)\n",
    "        \n",
    "        # Aplica log1p e normaliza√ß√£o\n",
    "        poi_log = np.log1p(poi_array)\n",
    "        \n",
    "        max_vals = np.array(self.poi_stats['max_vals'])\n",
    "        max_vals_log = np.log1p(max_vals)\n",
    "        max_vals_log[max_vals_log == 0] = 1.0\n",
    "        \n",
    "        poi_normalized = poi_log / max_vals_log[np.newaxis, :]\n",
    "        poi_normalized = np.clip(poi_normalized, 0.0, 1.0)\n",
    "        \n",
    "        return poi_normalized.tolist()\n",
    "    \n",
    "    def normalize_cities(self, city_vals):\n",
    "        \"\"\"Converte cidades para encoding num√©rico\"\"\"\n",
    "        city_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "        return np.array([city_mapping[c] for c in city_vals], dtype=np.int8)\n",
    "    \n",
    "    def process_and_save_sample(self, chunk_size: int = 50_000):\n",
    "        \"\"\"\n",
    "        Processa e salva apenas a AMOSTRA.\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Normalizando e salvando AMOSTRA...\")\n",
    "        \n",
    "        pf = pq.ParquetFile(self.input_path)\n",
    "        \n",
    "        writer = None\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(pf.iter_batches(batch_size=chunk_size), \n",
    "                                      total=self.max_chunks, desc=\"Normalizando amostra\")):\n",
    "            \n",
    "            # üß™ PARA AP√ìS max_chunks\n",
    "            if i >= self.max_chunks:\n",
    "                break\n",
    "            \n",
    "            table = pa.Table.from_batches([batch], schema=pf.schema_arrow)\n",
    "            \n",
    "            # Extrai dados originais\n",
    "            uid_vals = table.column(\"uid\").to_numpy()\n",
    "            d_vals = table.column(\"d\").to_numpy() \n",
    "            t_vals = table.column(\"t\").to_numpy()\n",
    "            x_vals = table.column(\"x\").to_numpy()\n",
    "            y_vals = table.column(\"y\").to_numpy()\n",
    "            city_vals = table.column(\"city\").to_pylist()\n",
    "            poi_lists = table.column(\"POI\").to_pylist()\n",
    "            \n",
    "            # Aplica normaliza√ß√µes\n",
    "            x_norm, y_norm = self.normalize_coordinates(x_vals, y_vals)\n",
    "            d_norm, t_sin, t_cos = self.normalize_time(d_vals, t_vals)\n",
    "            poi_norm = self.normalize_pois_robust(poi_lists)\n",
    "            city_encoded = self.normalize_cities(city_vals)\n",
    "            \n",
    "            # Cria novo schema\n",
    "            if writer is None:\n",
    "                new_schema = pa.schema([\n",
    "                    pa.field(\"uid\", pa.int64()),\n",
    "                    pa.field(\"d_orig\", pa.uint8()),\n",
    "                    pa.field(\"t_orig\", pa.uint8()),\n",
    "                    pa.field(\"d_norm\", pa.float32()),\n",
    "                    pa.field(\"t_sin\", pa.float32()),\n",
    "                    pa.field(\"t_cos\", pa.float32()),\n",
    "                    pa.field(\"x_norm\", pa.float32()),\n",
    "                    pa.field(\"y_norm\", pa.float32()),\n",
    "                    pa.field(\"city\", pa.string()),\n",
    "                    pa.field(\"city_encoded\", pa.int8()),\n",
    "                    pa.field(\"POI_norm\", pa.list_(pa.float32()))\n",
    "                ])\n",
    "                \n",
    "                # Adiciona metadados\n",
    "                metadata = {\n",
    "                    b\"normalization_info\": json.dumps(self.normalization_info).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_min\": json.dumps(self.coords_scaler.data_min_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"coords_scaler_scale\": json.dumps(self.coords_scaler.scale_.tolist()).encode(\"utf-8\"),\n",
    "                    b\"poi_stats\": json.dumps(self.poi_stats).encode(\"utf-8\"),\n",
    "                    b\"max_day\": str(self.max_day).encode(\"utf-8\")\n",
    "                }\n",
    "                new_schema = new_schema.with_metadata(metadata)\n",
    "                \n",
    "                writer = pq.ParquetWriter(self.output_path, new_schema, compression='snappy')\n",
    "            \n",
    "            # Cria nova tabela\n",
    "            new_table = pa.table({\n",
    "                \"uid\": uid_vals,\n",
    "                \"d_orig\": d_vals,\n",
    "                \"t_orig\": t_vals, \n",
    "                \"d_norm\": d_norm,\n",
    "                \"t_sin\": t_sin,\n",
    "                \"t_cos\": t_cos,\n",
    "                \"x_norm\": x_norm,\n",
    "                \"y_norm\": y_norm,\n",
    "                \"city\": city_vals,\n",
    "                \"city_encoded\": city_encoded,\n",
    "                \"POI_norm\": poi_norm\n",
    "            }, schema=new_schema)\n",
    "            \n",
    "            writer.write_table(new_table)\n",
    "            processed_rows += len(uid_vals)\n",
    "        \n",
    "        if writer:\n",
    "            writer.close()\n",
    "            \n",
    "        print(f\"‚úÖ AMOSTRA normalizada salva em: {self.output_path}\")\n",
    "        print(f\"üìä Linhas processadas: {processed_rows:,}\")\n",
    "        print(f\"üíæ Tamanho do arquivo: {self.output_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "    \n",
    "    def run(self, chunk_size: int = 50_000):\n",
    "        \"\"\"Executa normaliza√ß√£o da AMOSTRA\"\"\"\n",
    "        print(\"üß™ Iniciando TESTE de normaliza√ß√£o do dataset HuMob\")\n",
    "        print(f\"üìÇ Input: {self.input_path}\")\n",
    "        print(f\"üìÅ Output: {self.output_path}\")\n",
    "        print(f\"üî¨ Amostra: {self.max_chunks} chunks (~{self.max_chunks * chunk_size:,} linhas)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Passo 1: Calcular estat√≠sticas da amostra\n",
    "        self.compute_statistics_sample(chunk_size)\n",
    "        \n",
    "        # Passo 2: Processar e salvar amostra\n",
    "        self.process_and_save_sample(chunk_size)\n",
    "        \n",
    "        print(\"\\nüß™ TESTE conclu√≠do com sucesso!\")\n",
    "        print(\"\\nüìã Pr√≥ximos passos:\")\n",
    "        print(\"   1. Verifique se o arquivo de teste est√° correto\")\n",
    "        print(\"   2. Se OK, execute a vers√£o completa\")\n",
    "        print(\"   3. Se problemas, ajuste par√¢metros e teste novamente\")\n",
    "\n",
    "\n",
    "def check_normalized_sample(file_path: str, n_samples: int = 1000):\n",
    "    \"\"\"Verifica se a amostra foi normalizada corretamente.\"\"\"\n",
    "    if not Path(file_path).exists():\n",
    "        print(f\"‚ùå Arquivo n√£o encontrado: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Verificando AMOSTRA normalizada em: {file_path}\")\n",
    "    \n",
    "    # L√™ amostra\n",
    "    df = pq.read_table(file_path).slice(0, n_samples).to_pandas()\n",
    "    \n",
    "    print(f\"\\nüìä Estat√≠sticas da amostra ({len(df):,} linhas):\")\n",
    "    print(f\"   üìç Coordenadas:\")\n",
    "    print(f\"      x_norm: [{df['x_norm'].min():.3f}, {df['x_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      y_norm: [{df['y_norm'].min():.3f}, {df['y_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    \n",
    "    print(f\"   üìÖ Tempo:\")\n",
    "    print(f\"      d_norm: [{df['d_norm'].min():.3f}, {df['d_norm'].max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      t_sin: [{df['t_sin'].min():.3f}, {df['t_sin'].max():.3f}] (deve ser [-1,1])\")\n",
    "    print(f\"      t_cos: [{df['t_cos'].min():.3f}, {df['t_cos'].max():.3f}] (deve ser [-1,1])\")\n",
    "    \n",
    "    print(f\"   üè¢ POIs:\")\n",
    "    poi_sample = np.array(df['POI_norm'].iloc[0])\n",
    "    print(f\"      Exemplo: [{poi_sample.min():.3f}, {poi_sample.max():.3f}] (deve ser [0,1])\")\n",
    "    print(f\"      Dimens√£o: {len(poi_sample)} (deve ser 85)\")\n",
    "    \n",
    "    # Verifica NaNs\n",
    "    has_nan = df.isnull().any().any()\n",
    "    print(f\"   üß™ Valores NaN: {'‚ùå Encontrados!' if has_nan else '‚úÖ Nenhum'}\")\n",
    "    \n",
    "    print(f\"   üèôÔ∏è Cidades:\")\n",
    "    print(f\"      Encoded: {sorted(df['city_encoded'].unique())} (deve ser [0,1,2,3])\")\n",
    "    print(f\"      Original: {sorted(df['city'].unique())}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configura√ß√µes para TESTE\n",
    "    INPUT_FILE = \"humob_all_cities_dpsk.parquet\"\n",
    "    OUTPUT_FILE = \"humob_sample_normalized.parquet\"  # üß™ Arquivo de teste\n",
    "    MAX_CHUNKS = 20  # üß™ Apenas 20 chunks = ~1M linhas (2 minutos)\n",
    "    \n",
    "    # Verifica se arquivo de entrada existe\n",
    "    if not Path(INPUT_FILE).exists():\n",
    "        print(f\"‚ùå Arquivo n√£o encontrado: {INPUT_FILE}\")\n",
    "        print(\"üìù Certifique-se de que o arquivo foi gerado pelo data_loader.ipynb\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Executa normaliza√ß√£o de TESTE\n",
    "    normalizer = HuMobNormalizerSample(INPUT_FILE, OUTPUT_FILE, max_chunks=MAX_CHUNKS)\n",
    "    normalizer.run(chunk_size=50_000)\n",
    "    \n",
    "    # Verifica resultado\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    check_normalized_sample(OUTPUT_FILE, n_samples=5000)\n",
    "    \n",
    "    print(f\"\\nüß™ TESTE COMPLETO!\")\n",
    "    print(f\"   üìÑ Arquivo de teste: {OUTPUT_FILE}\")\n",
    "    print(f\"   üìä ~{MAX_CHUNKS * 50000:,} linhas processadas\")\n",
    "    print(f\"\\n‚úÖ Se tudo estiver OK, execute a vers√£o completa:\")\n",
    "    print(f\"   python normalize_humob_data_fixed.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orion_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
